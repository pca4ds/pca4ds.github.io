# How Does PCA Work? {#mechanics}

At its heart, performing a Principal Component Analysis involves taking a data table that contains the information about a certain phenomenon, in order to transform such data into a set of visual representations in some optimal sense. During the transformation process part of the information is "lost". However, PCA seeks to minimize this loss of information. There is a tradeoff between the amount of information that is lost, in exchange of gaining understanding and insight. We go from a raw data table to a set of graphical representations that should be easier to understand. In order to be able to _read_ the results and graphics obtained from a PCA, we need to discuss the mechanics of this technique and its underlying rationale.



## Principal Components

Let's consider the cloud of row-points, also known as the cloud of individuals. As we've mentioned, we are interested in decomposing the inertia (i.e. the spread) of this cloud in terms of a series of orthogonal directions.

The first step consists of looking for the most basic type of subspace, namely, a line. Geometrically, a line can be defined by a vector $mathbf{u}$ of unit norm. Based on the discusion from the previous chapter, we will attempt to define $\mathbf{u}$ in such a way that the projected points on this direction have maximum inertia (see figure \@ref(fig:fig-2-1)). In other words, $\mathbf{u}$ will be defined such that the distances between pairs of projected points are as close as possible to the original distances.


```{r fig-2-1, echo = FALSE, out.width = '75%', fig.cap='Projection of a row-point on the direction defined by a unit vector'}
knitr::include_graphics("images/figure-2-1.png")
```


The projection (or coordinate) of a row-point on the direction defined by $\mathbf{u}$ is given by:

\begin{equation}
\psi_i = \sum_{j=1}^{p} (x_{ij} - \bar{x}_j) u_j
(\#eq:2-1)
\end{equation}


The inertia (or variance) of all the projected points on $\mathbf{u}$ is then:

\begin{equation}
\sum_{i=1}^{n} = p_i \hspace{1mm} \psi_{i}^{2} = \lambda
(\#eq:2-2)
\end{equation}

The goal is to look for a line $\mathbf{u}$ that maximizes the value $\lambda$.

Let $\mathbf{X}$ be the mean-centered data matrix. Obtaining $\mathbf{u}$ implies diagonalizing the cross-product matrix $\mathbf{X^\mathsf{T} X}$. This matrix is the correlation matrix in a normalized PCA, whereas in a non-normalized PCA this matrix becomes teh covariance matrix.

It turns out that the unit vector $\mathbf{u}$ is the eigenvector associated to the largest eigenvalue from diagonalizing $\mathbf{X^\mathsf{T} X}$.

Analogously, the orthogonal direction to $\mathbf{u}$ that maximizes the projected inertia in this new direction corresponds to the eigenvector associated to the second largest eigenvalue from diagonalizing $\mathbf{X^\mathsf{T} X}$. This maximized inertia is equal to the second eigenvalue, so on and so forth.

The eigenvalues provide the projected inertias on each of the desired directions. Moreover, the sum of the eigenvalues is the sum of the inertias on the orthogonal directions, and this sum is equal to the global inertia of the cloud of points.

| Eigenvalues | Eigenvectors   |
|:-----------:|:--------------:|
| $\lambda_1$ | $\mathbf{u_1}$ |
| $\lambda_2$ | $\mathbf{u_2}$ |
|   $\dots$   |   $\dots$      |
| $\lambda_p$ | $\mathbf{u_p}$ |


\begin{equation}
I_T = \lambda_1 + \lambda_2 + \dots + \lambda_p = \begin{cases}
  0 & \text{in normalized PCA} \\
  \sum_{j=1}^{p} var(j) & \text{in non-normalized PCA}
\end{cases}
(\#eq:2-3)
\end{equation}

The eigenvectors give the directions of maximum inertia y we call them factorial axes.

On these directions we project the individuals, obtaining what is called the __principal components__ (see formula \@ref(eq:2-3)). As we can tell, each component is obtained as a linear combination of the original variables:

$$
\boldsymbol{\psi}_{\alpha} = u_1 \mathbf{x_1} + \dots + u_p \mathbf{x_p}
$$

Likewise, each component has a variance equal to its associated eigenvalue:

$$
var(\boldsymbol{\psi}_{\alpha}) = \lambda_\alpha
$$

In summary, a Principal Component Analysis can be seen as a technique in which we go from $p$ original variables $\mathbf{x_j}$, each having an importance given by its variance, into $p$ new variables $\boldsymbol{\psi}_{\alpha}$. These new variables are linear combination of the original variables, and have an importance given by their variance which turns out to be their eigenvalues (see figure \@ref(fig:fig-2-2)).

```{r fig-2-2, echo = FALSE, out.width = '75%', fig.cap='Change of basis and dimension reduction'}
knitr::include_graphics("images/figure-2-2.png")
```



### Interpreting the Inertia Proportions

In our working examples with the data about the cities, we obtain the following 12 eigenvalues:

```{r table-2-1, echo = FALSE}
# table of eigenvalues
eigs <- read.csv('data/eigenvalues.csv', row.names = 1)

knitr::kable(
  eigs,
  booktabs = TRUE,
  caption = 'Distribution of eigenvalues.'
)
```

Notice that we obtain a first principal component that stands out from the rest. 

The column `eigenvalue` provides the explained inertia for each direction. The sum of all of the inertias corresponds to the global inertia of the cloud of cities. Observe that this global inertia is equal to 12, which is the number of variables. Recall that this is property from a normalized PCA.

The column `percentage`, in tuerns, expresses the porpotion of the explained inertia by each axis. As we can tell from the table, the first direction explains about 85% of the global inertia, which is contained in a 12-dimensional space. Because of the very large value of this principal component, one could be tempted to neglect the rest of the components. However, we'll see that such an attitude is not excempt of risks. This does not imply that the rest of the components are useless or uninteresting. Quite the opposite, they may help reveal systematic patterns of variation in the data.

The last column of table \@ref(tab:table-2-1) provides the cumulative percentage of inertia. With the first three factorial axes we summarize about 95% of the inertia (or spread) of the cloud of points.



### How many axes to retain?

From the previous results, we've seen that with the first principal components, we get to recover or capture most of the spread in the cloud of points. A natural question arises: How many axes should we keep?

This is actually not an easy question, and the truth is that there is no definitive answer. In order to attempt answering this question, we have to consider another inquiry: What will the axes be used for? Let's see some examples.

__Example 1.__ One possibility involves using the axes to obtain a simple graphic representation of the data. In this case, the conventional number of axes to retain is 2, which are used to graph a scatter diagram: say we call these axes $F_1$ and $F_2$. With a third axis, we could even try to get a three-dimensional representation ($F_1$, $F_2$, and $F_3$). Beyond three dimensions, we can't get any visual representations.

Optionally, we could try to look at partial displays of the $p$-dimensional space. For instance, we can get a scatterplot with $F_2$ and $F_3$, and then another scatterplot with $F_1$ and $F_4$. Keep in mind that all these partial views require a considerable "intelectual" effort. Why? Because of the fact that in any of these partial configurations, the distances between points come from compressed spaces in which some directions have dissapeared. If the goal is to simply obtain a two-dimensional visualization, it usually suffices with looking at the first factorial plane ($F_1$ and $F_2$). To look "beyond" this plane, we will use outputs from clustering methods.

__Example 2.__ If the purpose is to keep the factorial axes as an intermediate stage of a clustering procedure, then this changes things drastically. In this situation, we want to retain several axes (so that we get to keep as much of the spread of the original variables). Usually we would discard those directions associated to the smallest eigenvalues. The reason to do this is because such directions typically reflect random fluctuations---"noise"---and not really a signal in the data.

__Example 3.__ If the goal is to use the factorial axes as explanatory variables in a regression model or in a classification model, we will try to keep a reduced number of axes, although not necessarily the first ones. It is certainly possible to find discriminant directions among axes that are not in the first positions.

As you can tell, deciding on the number of axes to retain is not that simple. This is a decision that is also linked to the stability of results.

We recommend not to blindly trust in automatic rules of thumb for deciding the number of directions to be kept. Our experience tells us that it is possible to find stable factorial axes with relatively small eigenvalue.

_Note:_ To decrease the percentage of inertia of each axis, one can add new uncorrelated variables to the data table (i.e. white noise). Doing so should not have an effect on the first factorial axes, which should still be able to capture most of the summarized "information".



### Coordinates of row-points

The table below contains the results about the cities with respect to the first three factorial axes:

```{r echo = FALSE, comment = ''}
load('data/table-2-2.RData')
cat('Table 2.2: Resutls of the individuals\n')
pca_inds
```

The column `wgt` indicates the weight of each city, which in this case is a uniform weight of 100 / 51 = 1.96.

The column `disto` provides the squared distance of each city to the center of gravity. This column allows us to find which cities are the _typical_ cities (i.e. the closest ones to the center of gravity), such as Helsinki. Likewise, it allows us to identify the _unusual_ or _unique_ cities (i.e. those that have a large distance to the center of gravity) like Zurich or Tokyo. In general, the distance to the center of gravity is a criterion of the _singularity_ of each city.

The third and fourth columns correspond to the coordinates obtained from projecting the cities onto the first two factorial axes. The representation on the first factorial plane is obtained with these coordinateas ($F_1$ and $F_2$), given in figure \@ref(fig:fig-1-9).

It is important to mention that the orientation of a factorial axis is arbitrary: the important trait is the direction. We could change the orientation of an axis by changing the sign of the coordinates on this axis. Graphically, this means that all symmetries are possible. The user has to make the decision of which orientation is the most convenient.



### Interpretation Tools

#### Contributions {-}

The next two columns, `contr1` and `contr2`, provide the _contributions_ (in percentages) of the cities to the explained inertia by each axis. The inertia of an axis is obtained via formula \@ref(eq:2-2). Thus, we can measure the part of an axis' inertia that is due to a given row-point by means of the quotient:

\begin{equation}
CTR(i, \alpha) = \frac{p_i \psi^{2}_{i \alpha}}{\lambda_{\alpha}} \times 100
(\#eq:2-4)
\end{equation}


The above quotient is the contribution of a point $i$ to the construction of axis $\alpha$.

We can use the contributions to identify the cities that contribute the most to the construction of the factorial axes.

If all cities had the same contribution, this would have a value of about 2% (100/51). Consequently, all those cities with contributions greater than 2% can be considered to have influence above the average.

_When is a contribution considered to be "high"?_

The answer to this question is not straightforward. A contribution will be considered "high" when, compared to the rest of contributions, it has an unusual large value.

For example, the city that contributes the most to the second axis is Abu Dhabi (48%). Almost half of the inertia of this axis is due to this city alone. Abu Dhabi is clearly very influent in the construction of this axis. We can actually ask about the stability of this axis, meaning, how much the result would change if Abu Dhabi were to be eliminated?

The next figure \@ref(fig:fig-2-3) shows the cities with a size proportional to their contributions on the first factorial plane (sum of the contributions of the first two axes).

```{r fig-2-3, echo = FALSE, out.width = '80%', fig.cap='Contributions of the cities in the first factorial plane'}
knitr::include_graphics("images/figure-2-3.png")
```

All the active points play a role in the construction of an axis. We can check that the sum of all the contributions in an axis add up to 100.

\begin{equation}
\sum_{i=1}^{n} CTR(i, \alpha) = 100
(\#eq:2-5)
\end{equation}



#### Squared Cosines {-}

The last columns of the table of results, `cosqr1` and `cosqr2`, contain the values of the __squared cosines__. These are used to assess the quality of the obtained factorial configuration when compared to the original configuration of the row-points.

Because the obtained representations are an approximation of the real distances between points, it is expected that some distances between pairs of points will be better represented whereas other distances will not reliably reflect the real distance between two points.

The goal is to have a good idea of how close is a point with respect to the factorial plane. If two points are close to the factorial plane, then the projected distance will be a good approximation to the actual distance in the original space. However, if at least one point is further away from the projection plane, then the real distance can be very different from that represented in the factorial plane.

This proximity to the factorial plane is measured with the squared cosine of each point to the factorial axes. 

```{r fig-2-4, echo = FALSE, out.width = '60%', fig.cap='The squared cosine as a measure of proximity'}
knitr::include_graphics("images/figure-2-4.png")
```

The figure \@ref(fig:fig-2-4) illustrates the definition given in equation \@ref(eq:2-6)

\begin{equation}
COS^2(i, \alpha) = \frac{\psi^{2}_{i \alpha}}{d^2(i, G)}
(\#eq:2-6)
\end{equation}


A squared cosine of 1means that the city is on the factorial axis (i.e. the angle $\omega$ is zero), whereas a squared cosine of 0 indicates that the city is on an orthogonal direction to the axis.

Notice that the sum of the squared cosines over all $p$ factorial axes is equal to 1. This has to do with fact that all axes are needed in order to have the exact position of a point in the entire space.

\begin{equation}
\sum_{\alpha = 1}^{p} COS^2(i, \alpha) = 1
(\#eq:2-7)
\end{equation}

Interestingly, the sum of the squared cosines of a given point over the first axes provides, in percentage, the "quality" of representation of the point in the subspace defined by these axes.

_What value of a squared cosine indicates that a point is "well represented" on a factorial plane?_

Similar to the contributions, the answer to the above question is not straightforward. A squared cosine (or the sum on the first two axes of the factorial plane) has to be compared with the rest of the squared cosines in order to determine if it is large or small.

In our working example, the cities are in general well represented on the first factorial plane. The sum of the squared cosines over the first two axes is close to 1. However, cities such as Dublin, Madrid, Sidney or Milan, which are close to the center, are not well represented. In contrast, Mumbai or Caracas are perfectly represented. Figure \@ref(fig:fig-2-5) shows the cities with a size proportional to their squared cosine on the first factorial plane.

```{r fig-2-5, echo = FALSE, out.width = '80%', fig.cap='Squared cosines of the cities on the first factorial plane'}
knitr::include_graphics("images/figure-2-5.png")
```

The cities that are deficiently represented in this plane are the "average" cities. We can only interpret the proximity between cities if they are well represented on the factorial plane.



## Projections of Variables

Just like row-points can be represented on a low-dimensional factorial space that preserves as much as possible the original distances, we can do the same with the column-points (i.e. the variables).

Mathematically, working with the column-points implies diagonalizing the cross-product matrix $\mathbf{X X^\mathsf{T}}$.

```{r fig-2-6, echo = FALSE, out.width = '75%', fig.cap='Matrices to be diagonalized depending on the type of points'}
knitr::include_graphics("images/figure-2-6.png")
```

Analogously to the row-points, we can obtain the decomposition of the inertia depending on the directions defined by the eigenvalues of the matrix $\mathbf{X X^\mathsf{T}}$. The projected inertia on each direction is equal to its associated eigenvalue.

```{r fig-2-7, echo = FALSE, out.width = '70%', fig.cap='Cloud of variables and factorial axes in the space of individuals'}
knitr::include_graphics("images/figure-2-7.png")
```

The line of maximum inertia is given by the eigenvector $\mathbf{v}$ (defining the direction $F_1$), associated to the largest eigenvalue. The plane of maximum inertia is formed by adding the line that defines the direction $F_2$. This second direction corresponds to the eigenvector associated to the second largest eigenvalue, and so on.

The representation of the variables on an axis is obtained by projecting the variable-points onto a unit vector $\mathbf{v}$ that defines the direction of the axis.

Let $\varphi_{j \alpha}$ be the coordinate of the $j$ variable on the axis $\alpha$

\begin{equation}
\varphi_{j \alpha} = \sum_{i=1}^{n} \frac{x_{ij} - \bar{x}_j}{s_j} \hspace{1mm} v_{i \alpha}
(\#eq:2-8)
\end{equation}

where $\bar{x}_j$ is the mean of the $j$-th variable.

```{r fig-2-8, echo = FALSE, out.width = '85%', fig.cap='Projection of the variables on the first factorial plane'}
knitr::include_graphics("images/figure-2-8.png")
```

The inertia on an axis is given by the sum of the inertias of each variable-point projected onto the axis. In PCA there is not an explicit  weight for the variable-points. However, each variable can play a role more or less important by changing the unit of measurement, which in turn will increase (or decrease) its variance in a non-normalized PCA.

\begin{equation}
\sum_{j=1}^{p} \varphi^{2}_{j \alpha} = \lambda_{\alpha}
(\#eq:2-9)
\end{equation}

Notice that the inertia of the variable-points that are projected onto an axis is the same as the inertia of the row-points projected on the axis of same rank.

Among the factorial axes of both clouds of points, the factorial axes in one space are related to the factorial axes in the other space. Thiese relationships allow us to obtain the directions of one space taking into account the directions of the other space. Such relationships are known as _transition relationships_.

By using the transition relationships, we just need to perform a PCA on one of the spaces (e.g. the rows), and then use these results to derive the results of the other space (e.g. the columns), without having to diagonalize two different cross-products.

In general, we perform the analysis on the cross-product with the smaller dimensions. Usually, this involves working with the matrix $\mathbf{X^\mathsf{T} X}$, assuming that the data matrix has more rows than columns: $n > p$. In this scenario, we obtain the projection of the row-points given in equation \@ref(eq:2-1). The projection of the variables is then calculated from the directions $\mathbf{u}$, which define the factorial axes of the cloud of row-points.

\begin{equation}
\varphi_{j \alpha} = \sqrt{\lambda_{\alpha}} \hspace{1mm} u_{j \alpha}
(\#eq:2-10)
\end{equation}

The above formula allows us to interpret the simultaneous representation of both the cities and the professions.

In our working example, the results about the cities are displayed in table 2.3

```{r echo = FALSE, comment = ''}
load('data/table-2-3.RData')
cat('Table 2.3: Resutls of the variables\n')
pca_vars
```

The coordinates of all the variables in the first axis have the same sign, which indicates that the cloud of points is not centered.

Notice also that, in the case of normalized analysis, the coordinate coincides with the correlation of a variable and the principal component (projection of the points onto the factorial axis of same rank):

\begin{equation}
\varphi_{j \alpha} = cor(\mathbf{x_j}, \boldsymbol{\psi_{\alpha}})
(\#eq:2-11)
\end{equation}

This formula plays an important role in the interpretation of the results because it connects the representation of the row-points with the representation of the column-points.

A high correlation implies that the configuration of the individuals on a factorial axis resembles the positions of the individuals on that variable (a unit correlation would indicate that the principal component is a linear function of the variable). A correlation close to zero indicates that there is no _linear_ association between the principal component and the variable.



### Size Effect

As we've described it, the first principal component arises from the high correlation that exists among all the variables, which geometrically forms a very homogeneous array of vectors. This first component approximately corresponds to the bisector of this array of vectors, and will therefore be highly correlated to the original variables.

How can we interpret this phenomenon? Broadly speaking, for any city, if a salary is high in a given profession, then this is also true for the set of professions in that city. This general phenomenon is actually present in the entire data table as a structural pattern, which generates the first factor. And it is for this reason that we call the first component the __size factor__ or __size effect__.

```{r echo = FALSE, comment = ''}
load('data/table-2-4.RData')
cat('Table 2.4: Correlation matrix ordered based on the size factor\n\n')
print(var_corrs, na.print = "", print.gap = 2)
```

Having a first principal component that captures the _size effect_ is a common phenomenon in PCA. A distinctive trait is that the matrix of correlations between the variables can be arranged based on the correlations with the first principal component. As we can tell from table 2.4, there are high correlations close to the diagonal, and then they decrease as one moves away from this diagonal.


#### Interpretation of the Axes {-}

To better interpret a factorial axis we should take into account the variables that have a relative high correlation with the axis.

We have seen that the first axis is interpreted as a factor of size: differentiating cities according to their overall salary levels. The subsequent principal components comprise factors that are orthogonal to the first one.

The first principal component (projection of the cities onto the first direction of the cloud of row-points) provides an ordination of the cities depending on their salary levels. This first component opposes Swiss cities (Zurich and Geneva) and Tokyo to cities like Mumbai, Manila and Nairobi.

The second factorial axis, showing lower correlations with the original variables, opposes the professions `factory_manager`, `engineer`, `bank_clerk`, and `cook_chef` to `textile_worker`, `construction_worker`, `mechanic`, and `metalworker`. In other words, the second axis has to do with the fact that, independently from the salaries of a city, certain professions have a higher salaries than others.

The projection obto the second axis allows to distinguish cities with similar overall level of salaries: certain cities tend to value the _managerial_ jobs, whereas other cities tend to value the professions less socially appreciated.


### Interpretation tools for Variable Interpretation

#### Cosine Squares {-}

In a similar fashion to the analysis of the individuals (i.e. the cities), we can also define a set of coefficients, squared cosines, and contributions, that will help us in the interpretation of results in the analysis of the variables.

The squared cosines are defined as the quotient between the projected squared distance on an axis and the squared distance to the origin.

We know that the squared distance of a variable to the origin is equal to its variance:

\begin{equation}
COS^2(j, \alpha) = \frac{\varphi^{2}_{j \alpha}}{var(j)}
(\#eq:2-12)
\end{equation}


The sum of the squared cosines over all the axes is always equal to one:

\begin{equation}
\sum_{\alpha = 1}^{p} COS^2(j, \alpha) = 1
(\#eq:2-13)
\end{equation}

In the normalized PCA, the variances are equal to one; thus the squared cosines will be squared of the coordinates of the variables:

$$
COS^2(j, \alpha) = \varphi_{j \alpha}^{2} \qquad \text{in normalized PCA}
$$

In general, we have that:

$$
COS^2(j, \alpha) = CORR^2(\text{variable}, \text{factor})
$$


#### Contributions {-}

The contribution of each variable to the inertia of an axis is the part of the inertia accounted for the variable. The inertia of an axis (see eq \@ref(eq:2-9)) is expressed as:

$$
\lambda_{\alpha} = \sum_{j=1}^{p} \varphi_{j \alpha}^{2}
$$

The contribution of a variable to the construction of an axis is:

\begin{equation}
CTR(j, \alpha) = \frac{\varphi_{j \alpha}^{2}}{\lambda_{\alpha}} = \frac{(\sqrt{\lambda_{\alpha}} \hspace{1mm} u_{j \alpha})^2}{\lambda_{\alpha}} = u_{j \alpha}^{2}
(\#eq:2-14)
\end{equation}

where $u_{j \alpha}^{2}$ is the coordinate of the former unit axis associated to the variable $j$, projected on the axis $\alpha$.

We have the following result:

$$
CTR(j, \alpha) = (\text{former unit axis})^2
$$

To find how much a variable contributes to the construction of an axis, it suffices to square each component of the vector $\mathbf{u}$. These contributions indicate which variables are responsible for the construction of each axis. The sum of all the contributions to an axis is equal to 1 (or 100 in percentage).

\begin{equation}
\sum_{j=1}^{p} CTR(j, \alpha) = 100
(\#eq:2-15)
\end{equation}

The elements of $\mathbf{u}$ define the linear combinations of the original variables, orthogonal among each other, and of maximum variance (i.e. the principal components). For example, the linear combination of the first component is:

$$
\boldsymbol{\psi_1} = 0.30 \texttt{ teacher} + 0.30 \texttt{ bus_driver} + 0.29 \texttt{ mechanic} \\
+ 0.28 \texttt{ construction_worker} + 0.30 \texttt{ metalworker} + 0.27 \texttt{ cook_chef} + \\
+0.26 \texttt{ factory_manager} + 0.28 \texttt{ engineer} + 0.28 \texttt{ bank_clerk} + \\
+ 0.31 \texttt{ executive_secretary} + 0.30 \texttt{ salesperson} + 0.29 \texttt{ textile_worker}
$$

where each variable has been mena-centered and standardized (because we have performed a normalized PCA).

The first component is therefore defined by a set of coefficients, very similar to each other. In this particular case, the first component is not that different from the average of all the profession salaries.

The set of components $u_{j \alpha}$ also define the projection of the former unit axes onto the new factorial axes.






