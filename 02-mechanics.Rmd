# How Does PCA Work? {#mechanics}

At its heart, performing a Principal Component Analysis involves taking a data table that contains the information about a certain phenomenon, in order to transform such data into a set of visual representations in some optimal sense. During the transformation process part of the information is "lost". However, PCA seeks to minimize this loss of information. There is a tradeoff between the amount of information that is lost, in exchange of gaining understanding and insight. We go from a raw data table to a set of graphical representations that should be easier to understand. In order to be able to _read_ the results and graphics obtained from a PCA, we need to discuss the mechanics of this technique and its underlying rationale.



## Principal Components

Let's consider the cloud of row-points, also known as the cloud of individuals. As we've mentioned, we are interested in decomposing the inertia (i.e. the spread) of this cloud in terms of a series of orthogonal directions.

The first step consists of looking for the most basic type of subspace, namely, a line. Geometrically, a line can be defined by a vector $mathbf{u}$ of unit norm. Based on the discusion from the previous chapter, we will attempt to define $\mathbf{u}$ in such a way that the projected points on this direction have maximum inertia (see figure \@ref(fig:fig-2-1)). In other words, $\mathbf{u}$ will be defined such that the distances between pairs of projected points are as close as possible to the original distances.


```{r fig-2-1, echo = FALSE, out.width = '75%', fig.cap='Projection of a row-point on the direction defined by a unit vector'}
knitr::include_graphics("images/figure-2-1.png")
```


The projection (or coordinate) of a row-point on the direction defined by $\mathbf{u}$ is given by:

\begin{equation}
\psi_i = \sum_{j=1}^{p} (x_{ij} - \bar{x}_j) u_j
(\#eq:2-1)
\end{equation}


The inertia (or variance) of all the projected points on $\mathbf{u}$ is then:

\begin{equation}
\sum_{i=1}^{n} = p_i \hspace{1mm} \psi_{i}^{2} = \lambda
(\#eq:2-2)
\end{equation}

The goal is to look for a line $\mathbf{u}$ that maximizes the value $\lambda$.

Let $\mathbf{X}$ be the mean-centered data matrix. Obtaining $\mathbf{u}$ implies diagonalizing the cross-product matrix $\mathbf{X^\mathsf{T} X}$. This matrix is the correlation matrix in a normalized PCA, whereas in a non-normalized PCA this matrix becomes teh covariance matrix.

It turns out that the unit vector $\mathbf{u}$ is the eigenvector associated to the largest eigenvalue from diagonalizing $\mathbf{X^\mathsf{T} X}$.

Analogously, the orthogonal direction to $\mathbf{u}$ that maximizes the projected inertia in this new direction corresponds to the eigenvector associated to the second largest eigenvalue from diagonalizing $\mathbf{X^\mathsf{T} X}$. This maximized inertia is equal to the second eigenvalue, so on and so forth.

The eigenvalues provide the projected inertias on each of the desired directions. Moreover, the sum of the eigenvalues is the sum of the inertias on the orthogonal directions, and this sum is equal to the global inertia of the cloud of points.

| Eigenvalues | Eigenvectors   |
|:-----------:|:--------------:|
| $\lambda_1$ | $\mathbf{u_1}$ |
| $\lambda_2$ | $\mathbf{u_2}$ |
|   $\dots$   |   $\dots$      |
| $\lambda_p$ | $\mathbf{u_p}$ |


\begin{equation}
I_T = \lambda_1 + \lambda_2 + \dots + \lambda_p = \begin{cases}
  0 & \text{in normalized PCA} \\
  \sum_{j=1}^{p} var(j) & \text{in non-normalized PCA}
\end{cases}
(\#eq:2-3)
\end{equation}

The eigenvectors give the directions of maximum inertia y we call them factorial axes.

On these directions we project the individuals, obtaining what is called the __principal components__ (see formula \@ref(eq:2-3)). As we can tell, each component is obtained as a linear combination of the original variables:

$$
\boldsymbol{\psi}_{\alpha} = u_1 \mathbf{x_1} + \dots + u_p \mathbf{x_p}
$$

Likewise, each component has a variance equal to its associated eigenvalue:

$$
var(\boldsymbol{\psi}_{\alpha}) = \lambda_\alpha
$$

In summary, a Principal Component Analysis can be seen as a technique in which we go from $p$ original variables $\mathbf{x_j}$, each having an importance given by its variance, into $p$ new variables $\boldsymbol{\psi}_{\alpha}$. These new variables are linear combination of the original variables, and have an importance given by their variance which turns out to be their eigenvalues (see figure \@ref(fig:fig-2-2)).

```{r fig-2-2, echo = FALSE, out.width = '75%', fig.cap='Change of basis and dimension reduction'}
knitr::include_graphics("images/figure-2-2.png")
```



### Interpreting the Inertia Proportions

In our working examples with the data about the cities, we obtain the following 12 eigenvalues:

```{r table-2-1, echo = FALSE}
# table of eigenvalues
eigs <- read.csv('data/eigenvalues.csv', row.names = 1)

knitr::kable(
  eigs,
  booktabs = TRUE,
  caption = 'Distribution of eigenvalues.'
)
```

Notice that we obtain a first principal component that stands out from the rest. 

The column `eigenvalue` provides the explained inertia for each direction. The sum of all of the inertias corresponds to the global inertia of the cloud of cities. Observe that this global inertia is equal to 12, which is the number of variables. Recall that this is property from a normalized PCA.

The column `percentage`, in tuerns, expresses the porpotion of the explained inertia by each axis. As we can tell from the table, the first direction explains about 85% of the global inertia, which is contained in a 12-dimensional space. Because of the very large value of this principal component, one could be tempted to neglect the rest of the components. However, we'll see that such an attitude is not excempt of risks. This does not imply that the rest of the components are useless or uninteresting. Quite the opposite, they may help reveal systematic patterns of variation in the data.

The last column of table \@ref(tab:table-2-1) provides the cumulative percentage of inertia. With the first three factorial axes we summarize about 95% of the inertia (or spread) of the cloud of points.



### How many axes to retain?

From the previous results, we've seen that with the first principal components, we get to recover or capture most of the spread in the cloud of points. A natural question arises: How many axes should we keep?

This is actually not an easy question, and the truth is that there is no definitive answer. In order to attempt answering this question, we have to consider another inquiry: What will the axes be used for? Let's see some examples.

__Example 1.__ One possibility involves using the axes to obtain a simple graphic representation of the data. In this case, the conventional number of axes to retain is 2, which are used to graph a scatter diagram: say we call these axes $F_1$ and $F_2$. With a third axis, we could even try to get a three-dimensional representation ($F_1$, $F_2$, and $F_3$). Beyond three dimensions, we can't get any visual representations.

Optionally, we could try to look at partial displays of the $p$-dimensional space. For instance, we can get a scatterplot with $F_2$ and $F_3$, and then another scatterplot with $F_1$ and $F_4$. Keep in mind that all these partial views require a considerable "intelectual" effort. Why? Because of the fact that in any of these partial configurations, the distances between points come from compressed spaces in which some directions have dissapeared. If the goal is to simply obtain a two-dimensional visualization, it usually suffices with looking at the first factorial plane ($F_1$ and $F_2$). To look "beyond" this plane, we will use outputs from clustering methods.

__Example 2.__ If the purpose is to keep the factorial axes as an intermediate stage of a clustering procedure, then this changes things drastically. In this situation, we want to retain several axes (so that we get to keep as much of the spread of the original variables). Usually we would discard those directions associated to the smallest eigenvalues. The reason to do this is because such directions typically reflect random fluctuations---"noise"---and not really a signal in the data.

__Example 3.__ If the goal is to use the factorial axes as explanatory variables in a regression model or in a classification model, we will try to keep a reduced number of axes, although not necessarily the first ones. It is certainly possible to find discriminant directions among axes that are not in the first positions.

As you can tell, deciding on the number of axes to retain is not that simple. This is a decision that is also linked to the stability of results.

We recommend not to blindly trust in automatic rules of thumb for deciding the number of directions to be kept. Our experience tells us that it is possible to find stable factorial axes with relatively small eigenvalue.

_Note:_ To decrease the percentage of inertia of each axis, one can add new uncorrelated variables to the data table (i.e. white noise). Doing so should not have an effect on the first factorial axes, which should still be able to capture most of the summarized "information".



### Coordinates of row-points

The table below contains the results about the cities with respect to the first three factorial axes:

```{r echo = FALSE, comment = ''}
load('data/table-2-2.RData')
pca_inds
```

The column `wgt` indicates the weight of each city, which in this case is a uniform weight of 100 / 51 = 1.96.

The column `disto` provides the squared distance of each city to the center of gravity. This column allows us to find which cities are the _typical_ cities (i.e. the closest ones to the center of gravity), such as Helsinki. Likewise, it allows us to identify the _unusual_ or _unique_ cities (i.e. those that have a large distance to the center of gravity) like Zurich or Tokyo. In general, the distance to the center of gravity is a criterion of the _singularity_ of each city.

The third and fourth columns correspond to the coordinates obtained from projecting the cities onto the first two factorial axes. The representation on the first factorial plane is obtained with these coordinateas ($F_1$ and $F_2$), given in figure \@ref(fig:fig-1-9).

It is important to mention that the orientation of a factorial axis is arbitrary: the important trait is the direction. We could change the orientation of an axis by changing the sign of the coordinates on this axis. Graphically, this means that all symmetries are possible. The user has to make the decision of which orientation is the most convenient.



### Interpretation Tools

#### Contributions {-}

The next two columns, `contr1` and `contr2`, provide the _contributions_ (in percentages) of the cities to the explained inertia by each axis. The inertia of an axis is obtained via formula \@ref(eq:2-2). Thus, we can measure the part of an axis' inertia that is due to a given row-point by means of the quotient:

\begin{equation}
CTR(i, \alpha) = \frac{p_i \psi^{2}_{i \alpha}}{\lambda_{\alpha}} \times 100
(\#eq:2-4)
\end{equation}


The above quotient is the contribution of a point $i$ to the construction of axis $\alpha$.

We can use the contributions to identify the cities that contribute the most to the construction of the factorial axes.

If all cities had the same contribution, this would have a value of about 2% (100/51). Consequently, all those cities with contributions greater than 2% can be considered to have influence above the average.

_When is a contribution considered to be "high"?_

The answer to this question is not straightforward. A contribution will be considered "high" when, compared to the rest of contributions, it has an unusual large value.

For example, the city that contributes the most to the second axis is Abu Dhabi (48%). Almost half of the inertia of this axis is due to this city alone. Abu Dhabi is clearly very influent in the construction of this axis. We can actually ask about the stability of this axis, meaning, how much the result would change if Abu Dhabi were to be eliminated?

The next figure (__FIGURE 2.3__) shows the cities with a size proportional to their contributions on the first factorial plane (sum of the contributions of the first two axes).

All the active points play a role in the construction of an axis. We can check that the sum of all the contributions in an axis add up to 100.

$$
\sum_{i=1}^{n} CTR(i, \alpha) = 100
$$

__INSERT FIGURE 2.3__



#### Squared Cosines {-}

The last columns of the table of results, `cosqr1` and `cosqr2`, contain the values of the __squared cosines__. These are used to assess the quality of the obtained factorial configuration when compared to the original configuration of the row-points.

Because the obtained representations are an approximation of the real distances between points, it is expected that some distances between pairs of points will be better represented whereas other distances will not reliably reflect the real distance between two points.

The goal is to have a good idea of how close is a point with respect to the factorial plane. If two points are close to the factorial plane, then the projected distance will be a good approximation to the actual distance in the original space. However, if at least one point is further away from the projection plane, then the real distance can be very different from that represented in the factorial plane.

