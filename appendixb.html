<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Appendix B: PCA Formulae | Principal Component Analysis for Data Science (PCA4DS)</title>
  <meta name="description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Appendix B: PCA Formulae | Principal Component Analysis for Data Science (PCA4DS)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="github-repo" content="gastonstat/pca4ds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Appendix B: PCA Formulae | Principal Component Analysis for Data Science (PCA4DS)" />
  
  <meta name="twitter:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  

<meta name="author" content="Tomas Aluja-Banet Alain Morineau Gaston Sanchez" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="appendixa.html"/>
<link rel="next" href="appendixc.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>PCA for Data Science</b><br><small>T. Aluja, A. Morineau, G. Sanchez</small></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i>Terminology</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>1</b> Basic Elements</a><ul>
<li class="chapter" data-level="1.1" data-path="basic.html"><a href="basic.html#data-and-goals"><i class="fa fa-check"></i><b>1.1</b> Data and Goals</a><ul>
<li class="chapter" data-level="1.1.1" data-path="basic.html"><a href="basic.html#active-variables"><i class="fa fa-check"></i><b>1.1.1</b> Active Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic.html"><a href="basic.html#analysis-of-distances"><i class="fa fa-check"></i><b>1.2</b> Analysis of Distances</a><ul>
<li class="chapter" data-level="1.2.1" data-path="basic.html"><a href="basic.html#cloud-of-row-points"><i class="fa fa-check"></i><b>1.2.1</b> Cloud of Row-Points</a></li>
<li class="chapter" data-level="1.2.2" data-path="basic.html"><a href="basic.html#cloud-of-column-points"><i class="fa fa-check"></i><b>1.2.2</b> Cloud of Column-Points</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="basic.html"><a href="basic.html#how-to-see-the-distances-between-points"><i class="fa fa-check"></i><b>1.3</b> How to see the distances between points</a><ul>
<li class="chapter" data-level="1.3.1" data-path="basic.html"><a href="basic.html#how-to-find-the-projection-planes"><i class="fa fa-check"></i><b>1.3.1</b> How to find the projection planes</a></li>
<li class="chapter" data-level="1.3.2" data-path="basic.html"><a href="basic.html#how-to-take-into-account-the-importance-of-individuals"><i class="fa fa-check"></i><b>1.3.2</b> How to take into account the importance of individuals</a></li>
<li class="chapter" data-level="1.3.3" data-path="basic.html"><a href="basic.html#inertia-decomposition"><i class="fa fa-check"></i><b>1.3.3</b> Inertia Decomposition</a></li>
<li class="chapter" data-level="1.3.4" data-path="basic.html"><a href="basic.html#visualizing-association-between-variables."><i class="fa fa-check"></i><b>1.3.4</b> Visualizing association between variables.</a></li>
<li class="chapter" data-level="1.3.5" data-path="basic.html"><a href="basic.html#normalized-pca-or-non-normalized-pca"><i class="fa fa-check"></i><b>1.3.5</b> Normalized PCA or non-normalized PCA?</a></li>
<li class="chapter" data-level="1.3.6" data-path="basic.html"><a href="basic.html#distance-matrices"><i class="fa fa-check"></i><b>1.3.6</b> Distance Matrices</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Mechanics</b></span></li>
<li class="chapter" data-level="2" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>2</b> How Does PCA Work?</a><ul>
<li class="chapter" data-level="2.1" data-path="mechanics.html"><a href="mechanics.html#principal-components"><i class="fa fa-check"></i><b>2.1</b> Principal Components</a><ul>
<li class="chapter" data-level="2.1.1" data-path="mechanics.html"><a href="mechanics.html#interpreting-the-inertia-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Interpreting the Inertia Proportions</a></li>
<li class="chapter" data-level="2.1.2" data-path="mechanics.html"><a href="mechanics.html#how-many-axes-to-retain"><i class="fa fa-check"></i><b>2.1.2</b> How many axes to retain?</a></li>
<li class="chapter" data-level="2.1.3" data-path="mechanics.html"><a href="mechanics.html#coordinates-of-row-points"><i class="fa fa-check"></i><b>2.1.3</b> Coordinates of row-points</a></li>
<li class="chapter" data-level="2.1.4" data-path="mechanics.html"><a href="mechanics.html#interpretation-tools"><i class="fa fa-check"></i><b>2.1.4</b> Interpretation Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="mechanics.html"><a href="mechanics.html#projections-of-variables"><i class="fa fa-check"></i><b>2.2</b> Projections of Variables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="mechanics.html"><a href="mechanics.html#size-effect"><i class="fa fa-check"></i><b>2.2.1</b> Size Effect</a></li>
<li class="chapter" data-level="2.2.2" data-path="mechanics.html"><a href="mechanics.html#tools-for-interpreting-components"><i class="fa fa-check"></i><b>2.2.2</b> Tools for Interpreting Components</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="mechanics.html"><a href="mechanics.html#size-factor"><i class="fa fa-check"></i><b>2.3</b> Beyond the First Factor</a></li>
<li class="chapter" data-level="2.4" data-path="mechanics.html"><a href="mechanics.html#using-supplementary-elements"><i class="fa fa-check"></i><b>2.4</b> Using Supplementary Elements</a><ul>
<li class="chapter" data-level="2.4.1" data-path="mechanics.html"><a href="mechanics.html#continuous-supplementary-variables"><i class="fa fa-check"></i><b>2.4.1</b> Continuous Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="mechanics.html"><a href="mechanics.html#nominal-supplementary-variables"><i class="fa fa-check"></i><b>2.4.2</b> Nominal Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="mechanics.html"><a href="mechanics.html#profiling-with-v-test"><i class="fa fa-check"></i><b>2.4.3</b> Profiling with V-test</a></li>
<li class="chapter" data-level="2.4.4" data-path="mechanics.html"><a href="mechanics.html#axes-characterization-using-continuous-variables"><i class="fa fa-check"></i><b>2.4.4</b> Axes Characterization using Continuous Variables</a></li>
<li class="chapter" data-level="2.4.5" data-path="mechanics.html"><a href="mechanics.html#v-test-and-data-science"><i class="fa fa-check"></i><b>2.4.5</b> V-test and Data Science</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="mechanics.html"><a href="mechanics.html#simultaneous-representations"><i class="fa fa-check"></i><b>2.5</b> Simultaneous Representations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="mechanics.html"><a href="mechanics.html#old-unit-axes"><i class="fa fa-check"></i><b>2.5.1</b> Old Unit Axes</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Practice</b></span></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#themescope"><i class="fa fa-check"></i><b>3.1</b> Themescope</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#conditions-of-application"><i class="fa fa-check"></i><b>3.2</b> Conditions of Application</a><ul>
<li class="chapter" data-level="3.2.1" data-path="analysis.html"><a href="analysis.html#linearity-and-symmetry"><i class="fa fa-check"></i><b>3.2.1</b> Linearity and Symmetry</a></li>
<li class="chapter" data-level="3.2.2" data-path="analysis.html"><a href="analysis.html#balancing-the-content-of-active-variables"><i class="fa fa-check"></i><b>3.2.2</b> Balancing the content of active variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#validation-stability-and-significance"><i class="fa fa-check"></i><b>3.3</b> Validation: stability and significance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#how-many-axes-to-study-and-retain"><i class="fa fa-check"></i><b>3.3.1</b> How many axes to study and retain?</a></li>
<li class="chapter" data-level="3.3.2" data-path="analysis.html"><a href="analysis.html#simulations-random-effects-on-individuals"><i class="fa fa-check"></i><b>3.3.2</b> Simulations, random effects on individuals</a></li>
<li class="chapter" data-level="3.3.3" data-path="analysis.html"><a href="analysis.html#bootstrap-simulations"><i class="fa fa-check"></i><b>3.3.3</b> Bootstrap Simulations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#analysis-of-table-of-ranks"><i class="fa fa-check"></i><b>3.4</b> Analysis of Table of Ranks</a></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#optimal-reconstitution-of-data"><i class="fa fa-check"></i><b>3.5</b> Optimal Reconstitution of Data</a></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#synthetic-variables-and-indices"><i class="fa fa-check"></i><b>3.6</b> Synthetic Variables and Indices</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#handling-missing-values"><i class="fa fa-check"></i><b>3.7</b> Handling Missing Values</a></li>
<li class="chapter" data-level="3.8" data-path="analysis.html"><a href="analysis.html#pca-and-clustering"><i class="fa fa-check"></i><b>3.8</b> PCA and Clustering</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis.html"><a href="analysis.html#real-groups-or-instrumental-groups"><i class="fa fa-check"></i><b>3.8.1</b> Real Groups or Instrumental Groups?</a></li>
<li class="chapter" data-level="3.8.2" data-path="analysis.html"><a href="analysis.html#representants-of-groups"><i class="fa fa-check"></i><b>3.8.2</b> Representants of Groups</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis.html"><a href="analysis.html#data-weighing"><i class="fa fa-check"></i><b>3.9</b> Data Weighing</a></li>
<li class="chapter" data-level="3.10" data-path="analysis.html"><a href="analysis.html#pca-as-an-intermediate-analytical-stage"><i class="fa fa-check"></i><b>3.10</b> PCA as an Intermediate Analytical Stage</a></li>
<li class="chapter" data-level="3.11" data-path="analysis.html"><a href="analysis.html#comparing-various-tables"><i class="fa fa-check"></i><b>3.11</b> Comparing Various Tables</a></li>
<li class="chapter" data-level="3.12" data-path="analysis.html"><a href="analysis.html#analysis-of-a-table-of-means"><i class="fa fa-check"></i><b>3.12</b> Analysis of a Table of Means</a></li>
<li class="chapter" data-level="3.13" data-path="analysis.html"><a href="analysis.html#analysis-of-a-binary-table"><i class="fa fa-check"></i><b>3.13</b> Analysis of a Binary Table</a></li>
<li class="chapter" data-level="3.14" data-path="analysis.html"><a href="analysis.html#analysis-of-a-table-of-distances"><i class="fa fa-check"></i><b>3.14</b> Analysis of a Table of Distances</a></li>
<li class="chapter" data-level="3.15" data-path="analysis.html"><a href="analysis.html#conditional-pca"><i class="fa fa-check"></i><b>3.15</b> Conditional PCA</a><ul>
<li class="chapter" data-level="3.15.1" data-path="analysis.html"><a href="analysis.html#pca-on-model-residuals"><i class="fa fa-check"></i><b>3.15.1</b> PCA on Model Residuals</a></li>
<li class="chapter" data-level="3.15.2" data-path="analysis.html"><a href="analysis.html#analysis-of-local-variation"><i class="fa fa-check"></i><b>3.15.2</b> Analysis of Local Variation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Examples</b></span></li>
<li class="chapter" data-level="4" data-path="application-examples.html"><a href="application-examples.html"><i class="fa fa-check"></i><b>4</b> Application Examples</a><ul>
<li class="chapter" data-level="4.1" data-path="application-examples.html"><a href="application-examples.html#lascaux"><i class="fa fa-check"></i><b>4.1</b> Lascaux Cave Temperatures</a><ul>
<li class="chapter" data-level="4.1.1" data-path="application-examples.html"><a href="application-examples.html#temperature-data"><i class="fa fa-check"></i><b>4.1.1</b> Temperature Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="application-examples.html"><a href="application-examples.html#pca"><i class="fa fa-check"></i><b>4.1.2</b> PCA</a></li>
<li class="chapter" data-level="4.1.3" data-path="application-examples.html"><a href="application-examples.html#seasonal-phenomenon"><i class="fa fa-check"></i><b>4.1.3</b> Seasonal Phenomenon</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="5" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>5</b> Appendix A: Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="appendixa.html"><a href="appendixa.html#space-of-p-dimensions"><i class="fa fa-check"></i><b>5.1</b> Space of p-Dimensions</a></li>
<li class="chapter" data-level="5.2" data-path="appendixa.html"><a href="appendixa.html#distances-between-points"><i class="fa fa-check"></i><b>5.2</b> Distances between points</a></li>
<li class="chapter" data-level="5.3" data-path="appendixa.html"><a href="appendixa.html#center-of-gravity"><i class="fa fa-check"></i><b>5.3</b> Center of Gravity</a></li>
<li class="chapter" data-level="5.4" data-path="appendixa.html"><a href="appendixa.html#inertia-of-a-cloud-of-points"><i class="fa fa-check"></i><b>5.4</b> Inertia of a cloud of points</a></li>
<li class="chapter" data-level="5.5" data-path="appendixa.html"><a href="appendixa.html#projection-of-the-cloud-of-points-on-a-line"><i class="fa fa-check"></i><b>5.5</b> Projection of the cloud of points on a line</a></li>
<li class="chapter" data-level="5.6" data-path="appendixa.html"><a href="appendixa.html#centered-and-standardized-variable"><i class="fa fa-check"></i><b>5.6</b> Centered and Standardized Variable</a></li>
<li class="chapter" data-level="5.7" data-path="appendixa.html"><a href="appendixa.html#correlation-coefficient"><i class="fa fa-check"></i><b>5.7</b> Correlation Coefficient</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>6</b> Appendix B: PCA Formulae</a><ul>
<li class="chapter" data-level="6.1" data-path="appendixb.html"><a href="appendixb.html#general-analysis"><i class="fa fa-check"></i><b>6.1</b> General Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="appendixb.html"><a href="appendixb.html#formulas-for-pca"><i class="fa fa-check"></i><b>6.2</b> Formulas for PCA</a></li>
<li class="chapter" data-level="6.3" data-path="appendixb.html"><a href="appendixb.html#biplot-and-pca"><i class="fa fa-check"></i><b>6.3</b> Biplot and PCA</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendixc.html"><a href="appendixc.html"><i class="fa fa-check"></i><b>7</b> Appendix C: Data Analysis Reminder</a><ul>
<li class="chapter" data-level="7.1" data-path="appendixc.html"><a href="appendixc.html#normalized-principal-component-analysis"><i class="fa fa-check"></i><b>7.1</b> Normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="appendixc.html"><a href="appendixc.html#non-normalized-principal-component-analysis"><i class="fa fa-check"></i><b>7.2</b> Non-normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.3" data-path="appendixc.html"><a href="appendixc.html#simple-correpondence-analysis"><i class="fa fa-check"></i><b>7.3</b> Simple Correpondence Analysis</a></li>
<li class="chapter" data-level="7.4" data-path="appendixc.html"><a href="appendixc.html#multiple-correspondence-analysis"><i class="fa fa-check"></i><b>7.4</b> Multiple Correspondence Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="appendixc.html"><a href="appendixc.html#clustering-of-factors"><i class="fa fa-check"></i><b>7.5</b> Clustering of Factors</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Principal Component Analysis for Data Science (PCA4DS)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendixb" class="section level1">
<h1><span class="header-section-number">6</span> Appendix B: PCA Formulae</h1>
<p>In this appendix we present in a synthetic way the matrix notation of the expressions and formulae used in PCA.</p>
<div id="general-analysis" class="section level2">
<h2><span class="header-section-number">6.1</span> General Analysis</h2>
<p>Notation:</p>
<p><span class="math inline">\(\mathbf{X}\)</span>: matrix of <span class="math inline">\(n\)</span> rows (corresponding to the individuals), and <span class="math inline">\(p\)</span> columns (corresponding to the variables). We assume mean-centered variables.</p>
<p><span class="math inline">\(\mathbf{N}\)</span>: diagonal matrix of dimension <span class="math inline">\(n \times n\)</span>, with <span class="math inline">\(n_{ii} \geq 0\)</span></p>
<p><span class="math inline">\(\mathbf{M}\)</span>: matrix of dimension <span class="math inline">\(p \times p\)</span>, positive definite (<span class="math inline">\(\mathbf{a^\mathsf{T} M a} &gt; \mathbf{0}\)</span> for all <span class="math inline">\(\mathbf{a}\)</span>).</p>
<div id="cloud-of-points" class="section level4 unnumbered">
<h4>Cloud of Points</h4>
<p>In <span class="math inline">\(\mathbb{R}^p\)</span> we assume <span class="math inline">\(n\)</span> points weighted by <span class="math inline">\(\mathbf{N}\)</span>, and with metric <span class="math inline">\(\mathbf{M}.\)</span></p>
<p>In <span class="math inline">\(\mathbb{R}^n\)</span> we assume <span class="math inline">\(p\)</span> points weighted by <span class="math inline">\(\mathbf{M}\)</span>, and with metric <span class="math inline">\(\mathbf{N}.\)</span></p>
</div>
<div id="fit-in-p-dimensions" class="section level4 unnumbered">
<h4>Fit in p-dimensions</h4>
<p>Let <span class="math inline">\(\dot{\mathbf{u}}\)</span> be a unit vector (e.g. <span class="math inline">\(\dot{\mathbf{u}}^\mathsf{T} \mathbf{Mu} = \mathbf{1}\)</span>), defining a direction in <span class="math inline">\(\mathbb{R}^p\)</span>. In this direction we want to maximize the inertia of the projection of the individuals.</p>
<p>Projection of individuals: <span class="math inline">\(\mathbf{\Psi} = \mathbf{XM} \dot{\mathbf{u}}\)</span></p>
<p>Inertia of projection: <span class="math inline">\(\mathbf{\Psi^\mathsf{T} N \Psi} = (\mathbf{XM}\dot{\mathbf{u}})^\mathsf{T} \mathbf{N} (\mathbf{XM}\dot{\mathbf{u}})\)</span></p>
<p>The maximum projected inertia can be obtained by maximizing: <span class="math inline">\(\dot{\mathbf{u}}^\mathsf{T} (\mathbf{MX^\mathsf{T}NXM}) \dot{\mathbf{u}}\)</span>, with the condition <span class="math inline">\(\dot{\mathbf{u}}^\mathsf{T} \mathbf{M} \dot{\mathbf{u}} = 1\)</span>.</p>
<p>The vector <span class="math inline">\(\dot{\mathbf{u}}\)</span> that maximizes the previous expression is the eigenvector associated to the eigenvalue of:</p>
<p><span class="math display">\[
\mathbf{M}^{-1} (\mathbf{M X^\mathsf{T} NXM}) \dot{\mathbf{u}} = \lambda \dot{\mathbf{u}}
\]</span></p>
<p>Simplifying we get:</p>
<p><span class="math display">\[
\mathbf{X^\mathsf{T} NXM} \dot{\mathbf{u}} = \lambda \dot{\mathbf{u}}
\]</span></p>
<p>The maximum is obtained by:</p>
<p><span class="math display">\[
(\dot{\mathbf{u}} \mathbf{M})(\mathbf{X^\mathsf{T}NXM}) \dot{\mathbf{u}} = \lambda (\dot{\mathbf{u}}^\mathsf{T} \mathbf{M} \dot{\mathbf{u}}) = \lambda
\]</span></p>
<p>The image of <span class="math inline">\(\dot{\mathbf{u}}\)</span> by the metric <span class="math inline">\(\mathbf{M}\)</span> is called <em>factor</em>: <span class="math inline">\(\hat{\mathbf{u}} = \mathbf{M} \dot{\mathbf{u}} = \mathbf{M}^{1/2} \mathbf{u}\)</span></p>
<p>Then:</p>
<p><span class="math display">\[
\mathbf{M}(\mathbf{X^\mathsf{T}NX}) \hat{\mathbf{u}} = \lambda \hat{\mathbf{u}}
\]</span></p>
<p>where <span class="math inline">\(\hat{\mathbf{u}}\)</span> is normalized by <span class="math inline">\(\mathbf{M}^{-1}\)</span>:</p>
<p><span class="math display">\[
\dot{\mathbf{u}}^\mathsf{T} \mathbf{M} \dot{\mathbf{u}} = \hat{\mathbf{u}}^\mathsf{T} \mathbf{M}^{-1} \hat{\mathbf{u}} = 1
\]</span></p>
</div>
<div id="diagonalizable-matrix" class="section level4 unnumbered">
<h4>Diagonalizable Matrix</h4>
<p>In practice, the matrix that is diagonalized is:</p>
<p><span class="math display">\[
\mathbf{M}^{1/2} \mathbf{X^\mathsf{T}NXM}^{1/2} \mathbf{u} = \lambda \mathbf{u} 
\]</span></p>
<p>being <span class="math inline">\(\mathbf{u} = \mathbf{M}^{1/2} \dot{\mathbf{u}}\)</span>, and consequently, <span class="math inline">\(\mathbf{u^\mathsf{T}u} = 1\)</span></p>
<p>Let <span class="math inline">\(\mathbf{Y} = \mathbf{N}^{1/2} \mathbf{XM}^{1/2}\)</span>, we can express the previous relation as:</p>
<p><span class="math display">\[
\mathbf{Y^\mathsf{T}Yu} = \lambda \mathbf{u}
\]</span></p>
</div>
<div id="fit-in-n-dimensions" class="section level4 unnumbered">
<h4>Fit in n-dimensions</h4>
<p>Let <span class="math inline">\(\dot{\mathbf{v}}\)</span> a unit vector in <span class="math inline">\(\mathbb{R}^n\)</span>, with <span class="math inline">\(\dot{\mathbf{v}}^\mathsf{T} \mathbf{N} \dot{\mathbf{v}} = \mathbf{1}\)</span>.</p>
<p>The maximum projected inertia is obtained by maximizing:</p>
<p><span class="math display">\[
\dot{\mathbf{v}}^\mathsf{T} (\mathbf{NXMX^\mathsf{T}N}) \dot{\mathbf{v}}
\]</span></p>
<p>We find the maximum by diagonalizing the matrix:</p>
<p><span class="math display">\[
(\mathbf{XMX^\mathsf{T}N}) \dot{\mathbf{v}} = \lambda \dot{\mathbf{v}}
\]</span></p>
<p>The eigenvector <span class="math inline">\(\dot{\mathbf{v}}\)</span> associated to the largest eigenvalue defines the direction of <span class="math inline">\(\mathbb{R}^n\)</span> with maximum inertia.</p>
<p>We call <em>factor</em> to the vector in <span class="math inline">\(\mathbb{R}^n\)</span>: <span class="math inline">\(\hat{\mathbf{v}} = \mathbf{N} \dot{\mathbf{v}}\)</span></p>
<p>This factor verifies the relationship: <span class="math inline">\((\mathbf{NXMX^\mathsf{T}}) \hat{\mathbf{v}} = \lambda \hat{\mathbf{v}}\)</span>, with <span class="math inline">\(\hat{\mathbf{v}}^\mathsf{T} \mathbf{N}^{-1} \hat{\mathbf{v}} = 1\)</span></p>
</div>
<div id="symmetric-matrix" class="section level4 unnumbered">
<h4>Symmetric Matrix</h4>
<p>Introducing the metric in the coordinates: <span class="math inline">\(\mathbf{v} = \mathbf{N}^{1/2} \dot{\mathbf{v}}\)</span></p>
<p><span class="math display">\[
\mathbf{N}^{1/2} \mathbf{XMX^\mathsf{T}N}^{1/2}\mathbf{v} = \lambda \mathbf{v} \quad \text{with} \quad \mathbf{v^\mathsf{T}v} = 1
\]</span></p>
<p>Utilizing the matrix <span class="math inline">\(\mathbf{Y}\)</span> we have:</p>
<p><span class="math display">\[
\mathbf{Y^\mathsf{T}Yv} = \lambda \mathbf{v}
\]</span></p>
</div>
<div id="transition-relations" class="section level4 unnumbered">
<h4>Transition Relations</h4>
<p>Fit in <span class="math inline">\(\mathbb{R}^p\)</span>: <span class="math inline">\((\mathbf{X^\mathsf{T}NXM}) \dot{\mathbf{u}} = \lambda \dot{\mathbf{u}}\)</span></p>
<p>Fit in <span class="math inline">\(\mathbb{R}^n\)</span>: <span class="math inline">\((\mathbf{XMX^\mathsf{T}N}) \dot{\mathbf{v}} = \lambda \dot{\mathbf{v}}\)</span></p>
<p><span class="math display">\[
\mathbf{XMX^\mathsf{T}N} (\mathbf{XM} \dot{\mathbf{u}}) = \lambda (\mathbf{XM} \dot{\mathbf{u}})
\]</span></p>
<p><span class="math display">\[
\mathbf{X^\mathsf{T}NXM} (\mathbf{X^\mathsf{T}N} \dot{\mathbf{v}}) = \lambda (\mathbf{X^\mathsf{T}N} \dot{\mathbf{v}})
\]</span></p>
<p>Comparing the previous two relations, and imposing a normalizing restriction on the eigenvectors we have:</p>
<p><span class="math display">\[
(\mathbf{XM}\dot{\mathbf{u}})^\mathsf{T} \mathbf{N} (\mathbf{XM}\dot{\mathbf{u}}) = \lambda
\]</span></p>
<p>and</p>
<p><span class="math display">\[
(\mathbf{X^\mathsf{T}N}\dot{\mathbf{v}})^\mathsf{T} \mathbf{M} (\mathbf{X^\mathsf{T}N}\dot{\mathbf{v}}) = \lambda
\]</span></p>
<p>We deduct the so-called <em>transition relations</em>:</p>
<p><span class="math display">\[
\dot{\mathbf{v}} = \frac{1}{\sqrt{\lambda}} \mathbf{XM} \dot{\mathbf{u}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\dot{\mathbf{u}} = \frac{1}{\sqrt{\lambda}} \mathbf{X^\mathsf{T}N} \dot{\mathbf{v}}
\]</span></p>
<p>In practice, we operate with symmetric matrices and thus the <em>transition relations</em> become:</p>
<p><span class="math display">\[
\mathbf{v} = \frac{1}{\sqrt{\lambda}} \mathbf{N}^{1/2} \mathbf{XM}^{1/2}\mathbf{u}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbf{u} = \frac{1}{\sqrt{\lambda}} \mathbf{M}^{1/2} \mathbf{X^\mathsf{T}N}^{1/2}\mathbf{v}
\]</span></p>
</div>
</div>
<div id="formulas-for-pca" class="section level2">
<h2><span class="header-section-number">6.2</span> Formulas for PCA</h2>
<p>From a matrix standpoint, PCA consists of studying a data matrix <span class="math inline">\(\mathbf{Z}\)</span>, endowed with a metric matrix <span class="math inline">\(\mathbf{I}_p\)</span> defined in <span class="math inline">\(\mathbb{R}^p\)</span>, and another metric <span class="math inline">\(\mathbf{N}\)</span> defined in <span class="math inline">\(\mathbb{R}^n\)</span> (generally <span class="math inline">\(\mathbf{N} = (1/n) \mathbf{I}_n\)</span>).</p>
<p>The matrix <span class="math inline">\(\mathbf{Z}\)</span> comes defined in the following way:</p>
<ul>
<li><p>under a normalized PCA: <span class="math inline">\(\mathbf{Z} = \mathbf{XS}^{-1}\)</span>, where <span class="math inline">\(\mathbf{S}\)</span> is the diagonal matrix of standard deviations.</p></li>
<li><p>under a non-normalized PCA: <span class="math inline">\(\mathbf{Z} = \mathbf{X}\)</span></p></li>
</ul>
<p>The fit in <span class="math inline">\(\mathbb{R}^p\)</span> has to do with: <span class="math inline">\(\mathbf{Z^\mathsf{T}NZu} = \lambda \mathbf{u}\)</span>, with <span class="math inline">\(\mathbf{u^\mathsf{T}u} = 1\)</span>.</p>
<p>The fit in <span class="math inline">\(\mathbb{R}^n\)</span> has to do with: <span class="math inline">\(\mathbf{N}^{1/2} \mathbf{ZZ^\mathsf{T}N}^{1/2} \mathbf{v} = \lambda \mathbf{v}\)</span>, with <span class="math inline">\(\mathbf{v^\mathsf{T}v} = 1\)</span>.</p>
<p>The transition relations can be written as:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{u} &amp;= \frac{1}{\sqrt{\lambda}} \mathbf{Z^\mathsf{T}N}^{1/2} \mathbf{v} \\
&amp; \\
\mathbf{v} &amp;= \frac{1}{\sqrt{\lambda}} \mathbf{N}^{1/2} \mathbf{Z} \mathbf{u}
\end{align*}\]</span></p>
<p>The symmetric matrix to be diagonalized is <span class="math inline">\(\mathbf{Z^\mathsf{T}NZ}\)</span>. This matrix coincides with the matrix of correlations in the case of a normalized PCA; or with a covariance matrix in the case of a non-normalized PCA.</p>
<div id="coordinates-of-individuals" class="section level4 unnumbered">
<h4>Coordinates of Individuals</h4>
<p>Regardless of whether we are analyzing active individuals or supplementary ones, the coordinates of individuals are calculated by orthogonally projecting the rows of the data matrix <span class="math inline">\(\mathbf{Z}\)</span> onto the directions of the eigenvectors <span class="math inline">\(\mathbf{u}_{\alpha}\)</span>.</p>
<p><span class="math display">\[
\boldsymbol{\psi}_{\alpha} = \mathbf{Zu}_{\alpha} = 
\begin{cases}
\mathbf{XS}^{-1}\mathbf{u}_{\alpha} &amp; \text{(normalized PCA)} \\
\\
\mathbf{Xu}_{\alpha} &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
<p>with <span class="math inline">\(i\)</span>-th element:</p>
<p><span class="math display">\[
\psi_{i \alpha} = 
\begin{cases}
\sum_{j=1}^{p} \frac{x_{ij}}{s_j} u_{j\alpha} &amp; \text{(normalized PCA)} \\
\\
\sum_{j=1}^{p} x_{ij} u_{j\alpha} &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
</div>
<div id="coordinates-of-active-variables" class="section level4 unnumbered">
<h4>Coordinates of Active Variables</h4>
<p>The coordinates of the active variables are obtained by the orthogonal projection of the columns of <span class="math inline">\(\mathbf{Z}\)</span> onto the directions defined by <span class="math inline">\(\dot{\mathbf{v}}_{\alpha}\)</span> with the metric <span class="math inline">\(\mathbf{N}\)</span>.</p>
<p>The projection of the active variables on an axis <span class="math inline">\(\alpha\)</span> are given by:</p>
<p><span class="math display">\[
\mathbf{\Phi}_{\alpha} = \mathbf{Z^\mathsf{T}N}^{1/2} \mathbf{v}_{\alpha} = \frac{1}{\sqrt{\lambda_{\alpha}}} \mathbf{Z^\mathsf{T}NZu}_{\alpha} = \sqrt{\lambda_{\alpha}} \hspace{1mm} \mathbf{u}_{\alpha}
\]</span></p>
<p>with the <span class="math inline">\(j\)</span>-th element:</p>
<p><span class="math display">\[
\phi_{j \alpha} = \sqrt{\lambda_{\alpha}} \hspace{1mm} u_{j \alpha}
\]</span></p>
</div>
<div id="correlation-between-variables-and-pcs" class="section level4 unnumbered">
<h4>Correlation between Variables and PCs</h4>
<p>The correlation bewtween a variable <span class="math inline">\(\mathbf{x_j}\)</span> and a principal component <span class="math inline">\(\boldsymbol{\psi}_{\alpha}\)</span> is given by:</p>
<p><span class="math display">\[
cor(\alpha, j) = \sum_{i=1}^{n} p_i \left (\frac{x_{ij}}{s_j} \right ) \left  (\frac{\psi_{i \alpha}}{\sqrt{\lambda_{\alpha}}} \right )
\]</span></p>
<p>Using matrix notation we have:</p>
<p><span class="math display">\[
\mathbf{cor}_{\alpha} = \frac{1}{\sqrt{\lambda_{\alpha}}} (\mathbf{XS}^{-1})^\mathsf{T} \mathbf{N} (\mathbf{Zu}_{\alpha}) = (\mathbf{XS}^{-1})^\mathsf{T} \mathbf{N}^{1/2} \mathbf{v}_{\alpha}
\]</span></p>
<p><span class="math display">\[
\mathbf{cor}_{\alpha} = 
\begin{cases}
\mathbf{Z^\mathsf{T}N}^{1/2} \mathbf{v}_{\alpha} = \mathbf{\Phi}_{\alpha} &amp; \text{(normalized PCA)} \\
\\
\mathbf{S}^{-1} \mathbf{Z^\mathsf{T}N}^{1/2}\mathbf{v}_{\alpha} = \mathbf{S}^{-1} \mathbf{\Phi}_{\alpha} &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
<p><span class="math display">\[
cor(j, \alpha) = 
\begin{cases}
\phi_{j \alpha} &amp; \text{(normalized PCA)} \\
\\
\phi_{j \alpha} / s_j &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
</div>
<div id="coordinates-of-supplementary-variables" class="section level4 unnumbered">
<h4>Coordinates of Supplementary Variables</h4>
<p>The supplementary variables are located by using the previous rule about the the computation of the coordinates. Let <span class="math inline">\(\mathbf{Z}_{+}\)</span> the data matrix containing the supplementary variables. Taking into account the transition relations we have that:</p>
<p><span class="math display">\[
\mathbf{\Phi}_{\alpha}^{+} = \mathbf{Z^\mathsf{T}N}^{1/2} \mathbf{v}_{\alpha} = \mathbf{Z_{+}^{\mathsf{T}} N} \left (\frac{\mathbf{Zu}_{\alpha}}{\sqrt{\lambda_{\alpha}}} \right )
\]</span></p>
<p>The projection of the supplementary variables is computed from this relation between the coordinate of a variable and the projection of the individuals. In a normalized PCA, this projection is equal to the correlation between the variables and the principal component.</p>
<p><span class="math display">\[
\phi_{j \alpha}^{+} =
\begin{cases}
\sum_{i} p_i \frac{x_{ij}}{s_j} \frac{\psi_{i \alpha}}{\sqrt{\lambda_{\alpha}}} &amp; \text{(normalized PCA)}
\\
\sum_{i} p_i x_{ij} \frac{\psi_{i\alpha}}{\sqrt{\lambda_{\alpha}}} &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
</div>
<div id="old-unit-vectors-in-rp" class="section level4">
<h4><span class="header-section-number">6.2.0.1</span> Old Unit-Vectors in <span class="math inline">\(R^p\)</span></h4>
<p>Let <span class="math inline">\(\mathbf{e_j}\)</span> be a unit vector of the original basis in <span class="math inline">\(\mathbb{R}^p\)</span>. The projection of this vector onto the new basis is:</p>
<p><span class="math display">\[
\mathbf{e_j}^\mathsf{T} \mathbf{u}_{\alpha} = u_{j\alpha}
\]</span></p>
<p>The elements of vectors <span class="math inline">\(\mathbf{u}_{\alpha}\)</span> directly provide the projection of the original axes of <span class="math inline">\(\mathbb{R}^{p}\)</span>. Each axis of the original basis indicates the direction of growth of a variable. These directions can be jointly represented with the projection of the individual-points.</p>
</div>
<div id="distance-of-individuals-to-the-origin" class="section level4 unnumbered">
<h4>Distance of Individuals to the Origin</h4>
<p>The squared distance of an individual to the origin is the sum of the squares of the values in each row of <span class="math inline">\(\mathbf{Z}\)</span> (assuming centered data):</p>
<p><span class="math display">\[
d^2(i,G) = \sum_{j=1}^{p} z_{ij}^{2} =
\begin{cases}
\sum_{j} \left (\frac{x_{ij}}{s_j} \right )^2 &amp; \text{(normalized PCA)}
\\
\sum_{j} x_{ij}^{2} &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
<p>This formula works for both active and supplementary individuals.</p>
</div>
<div id="distance-of-variables-to-the-origin" class="section level4 unnumbered">
<h4>Distance of Variables to the Origin</h4>
<p>The distance of a variable to the origin is the sum of the squares of the values in the columns of <span class="math inline">\(\mathbf{Z}\)</span>, taking into account the metric <span class="math inline">\(\mathbf{N}\)</span>:</p>
<p><span class="math display">\[
d^2(j,O) = \sum_{i=1}^{n} p_i \hspace{1mm} z_{ij}^{2} =
\begin{cases}
\frac{\sum_{p_i x_{ij}^{2}}}{s_{j}^{2}} = 1 &amp; \text{(normalized PCA)}
\\
\sum_{i} p_i x_{ij}^{2} = s{_j}^{2} &amp; \text{(non-normalized PCA)}
\end{cases}
\]</span></p>
</div>
<div id="contribution-of-individuals-to-an-axis-inertia" class="section level4">
<h4><span class="header-section-number">6.2.0.2</span> Contribution of Individuals to an Axis’ Inertia</h4>
<p>The projected inertia on an axis is: <span class="math inline">\(\sum_{i=1}^{n} p_i \psi_{i \alpha}^{2} = \lambda_{\alpha}\)</span>.</p>
<p>The part of the inertia due to an individual is:</p>
<p><span class="math display">\[
CTR(i, \alpha) = \frac{p_i \psi_{i \alpha}^{2}}{\lambda_{\alpha}} \times 100
\]</span></p>
<p>this applies to both a normalized and a non-normalized PCA.</p>
</div>
<div id="squared-cosines-of-individuals" class="section level4 unnumbered">
<h4>Squared Cosines of Individuals</h4>
<p>The squared cosine of an individual is the projection of an individual onto an axis, divided by the squared of its distance to the origin:</p>
<p><span class="math display">\[
cos^2(i, \alpha) = \frac{\psi_{i \alpha}^{2}}{d^2(i,G)}
\]</span></p>
</div>
<div id="contributions-of-variables-to-the-inertia" class="section level4 unnumbered">
<h4>Contributions of Variables to the Inertia</h4>
<p>The projected inertia onto an axis in <span class="math inline">\(\mathbb{R}^{n}\)</span> is: <span class="math inline">\(\lambda_{\alpha} = \sum_{j}^{p} \varphi_{j\alpha}^{2}\)</span>.</p>
<p>The contribution of a variable to the inertia of the axis is:</p>
<p><span class="math display">\[
CTR(j, \alpha) = \frac{\varphi_{j\alpha}^{2}}{\lambda_{\alpha}} \times 100
\]</span></p>
<p>Taking into account the formula to compute the coordinates of the variables:</p>
<p><span class="math display">\[
CTR(j, \alpha) = u_{j\alpha}^{2} \times 100
\]</span></p>
</div>
<div id="squared-cosines-of-variables" class="section level4 unnumbered">
<h4>Squared Cosines of Variables</h4>
<p><span class="math display">\[
cos^2(j, \alpha) = \frac{\phi_{j\alpha}^{2}}{d^2(j,O)}
\]</span></p>
<p>The distance of a variable to the origin coincides with the standard deviation of the variable under a non-normalized PCA. In turn, when performing a normalized-PCA, the distance is equal to 1.</p>
<p><span class="math display">\[
cos^2 (j, \alpha) = cor^2(j, \alpha)
\]</span></p>
</div>
<div id="coordinates-of-categories-of-nominal-variables" class="section level4 unnumbered">
<h4>Coordinates of Categories of Nominal Variables</h4>
<p>A category point is the center of gravity of the individuals that have such category:</p>
<p><span class="math display">\[
\bar{\psi}_{k \alpha} = \frac{\sum_{i \in k} p_i \psi_{i \alpha}}{\sum_{i \in k} p_i}
\]</span></p>
</div>
<div id="distance-of-categories-to-the-origin" class="section level4 unnumbered">
<h4>Distance of Categories to the Origin</h4>
<p><span class="math display">\[
d^2(k,O) = \sum_{\alpha = 1}^{p} \bar{\psi}_{k \alpha}^{2}
\]</span></p>
</div>
<div id="v-test-of-categories" class="section level4 unnumbered">
<h4>V-test of Categories</h4>
<p>In a v-test we are interested in calculating the critical probability corresponding to the following hypothesis:</p>
<p><span class="math display">\[\begin{align*}
H_0: &amp; \bar{\psi}_{k \alpha} = 0 \\
H_1: &amp; \bar{\psi}_{k \alpha} &gt; 0 \quad \text{or} \quad \bar{\psi}_{k \alpha} &lt; 0
\end{align*}\]</span></p>
<p>Under the assumption of random election of individuals with category <span class="math inline">\(k\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
E(\bar{\psi}_{k \alpha}) &amp;= 0 \\
var(\bar{\psi}_{k \alpha}) &amp;= \frac{n - n_k}{n_k - 1} \frac{\lambda_{\alpha}}{n_k}
\end{align*}\]</span></p>
<p>By the central limit theorem, the variable <span class="math inline">\(\bar{\psi}_{k \alpha}\)</span> will (approximately) follow a normal distribution.</p>
<p>The v-test is the value of the standardized variable <span class="math inline">\(v_{k\alpha}\)</span> with the same level of significance:</p>
<p><span class="math display">\[
v_{k \alpha} = \frac{\bar{\psi}_{k \alpha}}{\sqrt{\frac{n-n_k}{n_k - 1}} \frac{\lambda_{\alpha}}{n_k}}
\]</span></p>
</div>
<div id="v-test-of-continuous-variables" class="section level4 unnumbered">
<h4>V-test of Continuous Variables</h4>
<p>Let <span class="math inline">\(\bar{x}_{kj}\)</span> be the mean of the variable <span class="math inline">\(j\)</span> in the group <span class="math inline">\(k\)</span>. We are interested in calculating the critical probability of the following hypothesis test:</p>
<p><span class="math display">\[\begin{align*}
H_0: &amp; \mu_{k j} = \bar{x}_{j} \\
H_1: &amp; \mu_{k j} &gt; \bar{x}_{j}  \quad \text{or} \quad \mu_{kj} &lt; \bar{x}_{j}
\end{align*}\]</span></p>
<p>Under the null hypothesis, we assume that individuals with category <span class="math inline">\(k\)</span> are randomly selected:</p>
<p><span class="math display">\[\begin{align*}
E(\bar{x}_{kj}) &amp;= \bar{x}_{j} \\
var(\bar{x}_{kj}) &amp;= \frac{n - n_k}{n_k - 1} \frac{s_{j}^{2}}{n_k} = s_{kj}^{2}
\end{align*}\]</span></p>
<p>By the cental limit theorem, the variable <span class="math inline">\(\bar{x}_{kj}\)</span> follows (approximately) a normmal distribution.</p>
<p>The v-test is the value of the standardized variable with the same level of significance.</p>
<p><span class="math display">\[
v_{k\alpha} = \frac{\bar{x}_{k\alpha} - \bar{x}_{j}}{\sqrt{\frac{n - n_k}{n_k - 1} \frac{s_{j}^{2}}{n_k}}}
\]</span></p>
</div>
</div>
<div id="biplot-and-pca" class="section level2">
<h2><span class="header-section-number">6.3</span> Biplot and PCA</h2>
<p>The so-called <em>biplot</em> is a general method for simultaneously representing the rows and columns of a data table. This graphing method consists of approximating the data table by a matrix product of dimension 2. The goal is to obtain a plane of the rows and columns. The techniques behind a biplot involves an eigendecomposition, such as the one performed in PCA. Usually, the biplot is carried out with mean-centered and scaled data.</p>
<p>Recall that PCA provides three types of graphics to visualize the active elements:</p>
<ol style="list-style-type: decimal">
<li><p>The “circle of correlations” where we represent the continuous variables (the cosine of the angle between two variables is the same as the correlation between variables).</p></li>
<li><p>The configuration of the individuals in the factorial plane; the utilized distance is the classic euclidean distance.</p></li>
<li><p>The simultaneous representation—in the orthonormed basis—of the original variables in the center of gravity of the cloud of points of individuals.</p></li>
</ol>
<p>We should keep in mind that the aim of a biplot is to get a projection of the individuals on the directions of the original variables that respects as much as possible the distribution of the initial data.</p>
<p>In a biplot, we overlap in the same graphic both the rows and the columns, according to three types of simultaneous representations:</p>
<ol style="list-style-type: decimal">
<li><p>In the space of variables: the cosine of the angle between two variables approximates the correlation between these two variables; likewise, the distance between two individuals approximates the Mahalanbis distance (not the typical euclidean distance in PCA).</p></li>
<li><p>In the space of individuals: the distance between individuals approximates the euclidean distance, but the distance between variables is not directly interpretable.</p></li>
<li><p>In an intermediate space: the distances, between individuals and variables, are not directly interpretable, but we obtain a “balanced” plot.</p></li>
</ol>
<p>Every matrix <span class="math inline">\(\mathbf{Y}\)</span> can be decomposed into the following product:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{AB^\mathsf{T}}
\]</span></p>
<p>with dimensions: <span class="math inline">\((n,p) = (n,k) \times (k,p)\)</span>, where <span class="math inline">\(k\)</span> is the rank of <span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>In a biplot, like in PCA, we graphically represent the individuals as points, and the variables as vectors (i.e. arrows). The biplot involves approximating <span class="math inline">\(\mathbf{Y}\)</span> by the product:</p>
<p><span class="math display">\[
\mathbf{\hat{Y}} \approx \mathbf{AB^\mathsf{T}}
\]</span></p>
<p>with dimensions: <span class="math inline">\((n,p) = (n,2) \times (2,p)\)</span>. The rows of the matrix <span class="math inline">\(\mathbf{A}\)</span> represent the individuals, and the rows of <span class="math inline">\(\mathbf{B}\)</span> represent the variables. In order to achieve this decomposition, we use the same decomposition in a PCA, that is, the eigendecomposition of <span class="math inline">\(\mathbf{Y}\)</span>:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{V \Lambda U^\mathsf{T}}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{U}\)</span> contains the eigenvectors of <span class="math inline">\(\mathbf{Y^\mathsf{T} Y}\)</span>, and <span class="math inline">\(\mathbf{\Lambda}\)</span> is the diagonal matrix of singular values (i.e. square root of the eigenvalues of <span class="math inline">\(\mathbf{Y^\mathsf{T}Y}\)</span>). We have that:</p>
<p><span class="math display">\[
\mathbf{V} = \mathbf{YU\Lambda}^{-1}
\]</span></p>
<p>Retaining only the first two eigenvalues, we obtain the rank-2 approximation of <span class="math inline">\(\mathbf{Y}\)</span> by:</p>
<p><span class="math display">\[
\mathbf{Y} \approx \hat{\mathbf{Y}} = \underset{(n,2)}{\mathbf{V}} \mathbf{\Lambda} \underset{(2,p)}{\mathbf{U}}
\]</span></p>
<p>We can define three decompositions of <span class="math inline">\(\mathbf{Y}\)</span> in therms of <span class="math inline">\(\mathbf{AB^\mathsf{T}}\)</span> based on thr form in which we assign the singular values between individuals (<span class="math inline">\(\mathbf{V}\)</span>) or between variables (<span class="math inline">\(\mathbf{U}\)</span>).</p>
<table>
<colgroup>
<col width="30%" />
<col width="25%" />
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Representation</th>
<th align="left"><span class="math inline">\(\mathbf{A}\)</span></th>
<th align="left"><span class="math inline">\(\mathbf{B^\mathsf{T}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Space of variables</td>
<td align="left"><span class="math inline">\(\mathbf{V}\)</span></td>
<td align="left"><span class="math inline">\(\mathbf{\Lambda U^\mathsf{T}}\)</span></td>
</tr>
<tr class="even">
<td align="left">Balanced</td>
<td align="left"><span class="math inline">\(\mathbf{V\Lambda}^{1/2}\)</span></td>
<td align="left"><span class="math inline">\(\mathbf{\Lambda}^{1/2} \mathbf{U^\mathsf{T}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Space of individuals</td>
<td align="left"><span class="math inline">\(\mathbf{V\Lambda}\)</span></td>
<td align="left"><span class="math inline">\(\mathbf{U^\mathsf{T}}\)</span></td>
</tr>
</tbody>
</table>
<p>Consider the expression <span class="math inline">\(y_{ij} \approx \mathbf{a_i}^{\mathsf{T}} \mathbf{b_j}\)</span></p>
<p>This scalar product shows that the projections of the points <span class="math inline">\(\mathbf{a_i}\)</span> on the directions defined by <span class="math inline">\(\mathbf{b_j}\)</span> apprixmates the distribution of the initial data of variable <span class="math inline">\(\mathbf{y_j}\)</span>, regardless of the performed decomposition.</p>
<div id="simultaneous-representation-in-the-variables-space" class="section level4 unnumbered">
<h4>Simultaneous Representation in the Variables Space</h4>
<p><span class="math display">\[
\mathbf{Y} \approx \mathbf{AB^\mathsf{T}} = (\mathbf{V})(\mathbf{\Lambda U^\mathsf{T}})
\]</span></p>
<p>The cosine of the angle formed by the vectors <span class="math inline">\(\mathbf{b_j}\)</span> and <span class="math inline">\(\mathbf{b_l}\)</span> corresponds to the correlation between variables <span class="math inline">\(\mathbf{y_j}\)</span> and <span class="math inline">\(\mathbf{y_l}\)</span>. Like in PCA, this property also holds for the active variables. With respect to the supplementary variables, this property is hold only through the axes.</p>
<p>The euclidean distance between the individuals <span class="math inline">\(\mathbf{a_i}\)</span> and <span class="math inline">\(\mathbf{a_h}\)</span> is proportional to the Mahalanobis distance between the individuals <span class="math inline">\(\mathbf{y_i}\)</span> and <span class="math inline">\(\mathbf{y_h}\)</span> of the partitioned table.</p>
<p>The Mahalanobis distance is a distance that takes into account the correlations between the variables. This distance transforms the cloud of row points, usually in an elliptical shape, into a circular shape. The Mahalnobis distance is given by:</p>
<p><span class="math display">\[
\delta^2 (i, h) = (\mathbf{y_i} - \mathbf{y_h})^\mathsf{T} \mathbf{W}^{-1} (\mathbf{y_i} - \mathbf{y_h})
\]</span></p>
<p>where <span class="math inline">\(\mathbf{W}\)</span> is the covariance-variance matrix.</p>
</div>
<div id="simultaneous-representation-in-the-individuals-space" class="section level4 unnumbered">
<h4>Simultaneous Representation in the Individuals Space</h4>
<p><span class="math display">\[
\mathbf{Y} \approx \mathbf{AB^\mathsf{T}} = (\mathbf{V \Lambda})(\mathbf{U})
\]</span></p>
<p>The euclidean distance between two individuals <span class="math inline">\(\mathbf{a_i}\)</span> and <span class="math inline">\(\mathbf{a_h}\)</span> approximates the euclidean distance between the individuals <span class="math inline">\(\mathbf{y_i}\)</span> and <span class="math inline">\(\mathbf{y_h}\)</span> of the partitioned data table. In this case there are no special properties relative to the proximity between variables: the distances are not directly interpretable.</p>
</div>
<div id="balanced-simultaneous-representation" class="section level4 unnumbered">
<h4>Balanced Simultaneous Representation</h4>
<p><span class="math display">\[
\mathbf{Y} \approx \mathbf{AB^\mathsf{T}} = (\mathbf{V \Lambda}^{1/2})(\mathbf{\Lambda}^{1/2} \mathbf{U^\mathsf{T}})
\]</span></p>
<p>This option tends to balance the representation between the rows and the columns in the sense that, for each axis, the sum of the squared of the distances to the axis is the same for the cloud of individuals as for the cloud of variables.</p>
<p>We obtain a “balanced” graphic. Except by the common property of all the decompositions (i.e. the projection of individuals onto the variables approximates the data table), there are no specific properties for the interpretation of the proximities between individuals, and neither for the proximities between variables.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="appendixa.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendixc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pca4ds/pca4ds.github.io/edit/master/06-appendixb.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
