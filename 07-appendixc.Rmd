# Appendix C: Data Analysis Reminder {#appendixc}

In this appendix we present in a brief reminder of multivariate exploratory data analysis (factorial analysis, descriptive and clustering).


## Normalized Principal Component Analysis

__Active variables__: these are continuous (or quantitative) variables.

- _Distance between variables_: based on correlation

$$
d^2(j,l) = 2(1 - cor(j,l))
$$

The more associated two variables are, the smaller the angle between them.

- _Distance between individuals_: classic (squared) euclidean distance (sum of the squares of the differences of standardized data).


__Factorial Planes for Variables__: This has to do with the representation of the linear association between variables. We can interpret the directions in the plane in terms of the correlations between the variables and the axes.

__Factorial Planes for Individuals__: This has to do with the representation of the similarities between individuals. We interpret the positions of the individuals on the factorial plane in terms of the meaning assigned to the axes.


__Supplementary Variables__

- _Continuous_: correlations with the axes; they are positioned in the factorial plane of the variables.

- _Categorical_: representation of each category as the center of gravity of the individuals having such category. We use the v-test to assess their "characterization power."



## Non-normalized Principal Component Analysis

__Active variables__

- _Distance between variables_: based on the covariances of the variables

$$
d^2(j,l) = var(j) + var(h) - 2 cov(j,h)
$$

- _Distance between individuals_: classic (squared) euclidean distance (sus of the squares of the differences with centered data).


__Factorial Planes for Variables__: This has to do with the representation of the variances as well as the association among variables. We can interpret the directions in the plane in terms of the covariances between the variables and the axes.

__Factorial Planes for Individuals__: This has to do with the representation of the similarities between individuals. We interpret the positions of the individuals on the factorial plane in terms of their values, and taking into account the meaning assigned to the axes.


__Supplementary Variables__

- _Continuous_: we pay attention to the covariances of the continuous variables with the factorial axes.

- _Categorical_: representation of each category in the cloud of row-points as the center of gravity of the individuals having such category. We use the v-test to assess their "characterization power."



## Simple Correpondence Analysis

__Analyzed Data__: two-way table or contingency table obtained by crossing two categorical variables.

- _Rows_: analysis of the proximities between the row-profiles.

- _Columns_: analysis of the proximities between the column-profiles.

The utilized distance in both cases is the _chi-squared_ distance.


__Interpretation__

- Overlapped representation of the row-profiles and the column-profiles.

- Barycentric Property: besides the dilation coefficient $(1/\sqrt{\lambda})$, a column-point is located in the barycenter of all the row-categories, weighted by the column-profile. Likewise, a row-point is located in the barycenter of all the column-categories, weighted by the row-profile.

- In the periphery of the cloud, two row-points that are close to each other indicate similar proportions in their column-categories. The same applies to column-points.

- The same rules of the active elements apply to the supplementary rows and columns. A supplementary row is located in a pseudo-barycenter of all the active columns, and viceversa.



## Multiple Correspondence Analysis

Multiple Correspondence Analysis (MCA) is the generalization of (simple) correspondence analysis to the case when we have more than two categorical variables. This analysis can also be regarded as a generalization of a normalized PCA for a data table of categorical variables.

One of the advantages of MCA is that it allows to take into account non-linear associations among variables.

The analyzed (raw) data table in this case is formed by multiple categorical variables. This table can be transformed into a complete disjoint table, or into a Burt table crossing all variables two-by-two).

- _For individuals_: we analyze the proximities between the row-profiles of the complete disjoint table.

- _For variables_: we analyze the proximities between the column-profiles of the complete disjoint table.

In both cases (rows and columns) the utilized distance is the _chi-squared_ distance.


__Interpretation__

Besides a dilation coefficient:

- an individual is the average point of its caegories.

- a category is an average point of the individuals that have chosen such category.

- the global center of gravity is also the center of gravity of al the categories.

- the part of the inertia due to a given category growths when its effective decreases.

- it is recommended to avoid categories with very few effectives (or to project them as a supplementary category).

- the inertia due to a variable growths when the number of its categories grows (it is recommended to balance the number of categories in the variables).

- in the peryphery of the cloud, two categories appear close to each other 
if the individuals with such categories have answered in a similar the set of active variables.

- a factorial plane is interpreted in terms of how close or how far the projected categories are. The interpretation should be supported by looking at the v-test, the contributions to the axes, the cosine squares, etc.

- a supplementary category is the average point of the individuals having such category.



## Clustering of Factors

We can use clustering techniques to find a reduced number of groups of individuals, homogenous and well separated. 

- using a clustering technique allows to handle a considerable number of dimensions from a dimension reduction procedure (e.g. PCA).

- using a clustering technique also allows us to reduce the amount of "noise" in the data (by using only the most important dimensions).

- it also enables us to cluster the individuals for the active variables.

- different clustering techniques can be used: hierarchical clustering methods (e.f. War criterion, single linkage, maximum linkage), K-means.

- the final partition can depend on the centers of the initial groups.

