<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 How to see the distances between points | Principal Component Analysis for Data Science (pca4ds)</title>
  <meta name="description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 How to see the distances between points | Principal Component Analysis for Data Science (pca4ds)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="github-repo" content="gastonstat/pca4ds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 How to see the distances between points | Principal Component Analysis for Data Science (pca4ds)" />
  
  <meta name="twitter:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  

<meta name="author" content="Tomas Aluja-Banet Alain Morineau Gaston Sanchez" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="analysis-of-distances.html"/>
<link rel="next" href="mechanics.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>PCA for Data Science</b><br><small>T. Aluja, A. Morineau, G. Sanchez</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Preface</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>PCA4DS</a></li>
<li class="chapter" data-level="" data-path="from-lanalyse-des-données-to-data-science.html"><a href="from-lanalyse-des-données-to-data-science.html"><i class="fa fa-check"></i>From “L’Analyse des Données” to Data Science</a></li>
<li class="chapter" data-level="" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i>Terminology</a></li>
<li class="part"><span><b>II Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>1</b> Basic Elements</a><ul>
<li class="chapter" data-level="1.1" data-path="data-and-goals.html"><a href="data-and-goals.html"><i class="fa fa-check"></i><b>1.1</b> Data and Goals</a><ul>
<li class="chapter" data-level="1.1.1" data-path="data-and-goals.html"><a href="data-and-goals.html#active-variables"><i class="fa fa-check"></i><b>1.1.1</b> Active Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="analysis-of-distances.html"><a href="analysis-of-distances.html"><i class="fa fa-check"></i><b>1.2</b> Analysis of Distances</a><ul>
<li class="chapter" data-level="1.2.1" data-path="analysis-of-distances.html"><a href="analysis-of-distances.html#cloud-of-row-points"><i class="fa fa-check"></i><b>1.2.1</b> Cloud of Row-Points</a></li>
<li class="chapter" data-level="1.2.2" data-path="analysis-of-distances.html"><a href="analysis-of-distances.html#cloud-of-column-points"><i class="fa fa-check"></i><b>1.2.2</b> Cloud of Column-Points</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html"><i class="fa fa-check"></i><b>1.3</b> How to see the distances between points</a><ul>
<li class="chapter" data-level="1.3.1" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#how-to-find-the-projection-planes"><i class="fa fa-check"></i><b>1.3.1</b> How to find the projection planes</a></li>
<li class="chapter" data-level="1.3.2" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#how-to-take-into-account-the-importance-of-individuals"><i class="fa fa-check"></i><b>1.3.2</b> How to take into account the importance of individuals</a></li>
<li class="chapter" data-level="1.3.3" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#inertia-decomposition"><i class="fa fa-check"></i><b>1.3.3</b> Inertia Decomposition</a></li>
<li class="chapter" data-level="1.3.4" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#visualizing-association-between-variables."><i class="fa fa-check"></i><b>1.3.4</b> Visualizing association between variables.</a></li>
<li class="chapter" data-level="1.3.5" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#normalized-pca-or-non-normalized-pca"><i class="fa fa-check"></i><b>1.3.5</b> Normalized PCA or non-normalized PCA?</a></li>
<li class="chapter" data-level="1.3.6" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#distance-matrices"><i class="fa fa-check"></i><b>1.3.6</b> Distance Matrices</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Mechanics</b></span></li>
<li class="chapter" data-level="2" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>2</b> How Does PCA Work?</a><ul>
<li class="chapter" data-level="2.1" data-path="principal-components.html"><a href="principal-components.html"><i class="fa fa-check"></i><b>2.1</b> Principal Components</a><ul>
<li class="chapter" data-level="2.1.1" data-path="principal-components.html"><a href="principal-components.html#interpreting-the-inertia-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Interpreting the Inertia Proportions</a></li>
<li class="chapter" data-level="2.1.2" data-path="principal-components.html"><a href="principal-components.html#how-many-axes-to-retain"><i class="fa fa-check"></i><b>2.1.2</b> How many axes to retain?</a></li>
<li class="chapter" data-level="2.1.3" data-path="principal-components.html"><a href="principal-components.html#coordinates-of-row-points"><i class="fa fa-check"></i><b>2.1.3</b> Coordinates of row-points</a></li>
<li class="chapter" data-level="2.1.4" data-path="principal-components.html"><a href="principal-components.html#interpretation-tools"><i class="fa fa-check"></i><b>2.1.4</b> Interpretation Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="projections-of-variables.html"><a href="projections-of-variables.html"><i class="fa fa-check"></i><b>2.2</b> Projections of Variables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="projections-of-variables.html"><a href="projections-of-variables.html#size-effect"><i class="fa fa-check"></i><b>2.2.1</b> Size Effect</a></li>
<li class="chapter" data-level="2.2.2" data-path="projections-of-variables.html"><a href="projections-of-variables.html#tools-for-interpreting-components"><i class="fa fa-check"></i><b>2.2.2</b> Tools for Interpreting Components</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="size-factor.html"><a href="size-factor.html"><i class="fa fa-check"></i><b>2.3</b> Beyond the First Factor</a></li>
<li class="chapter" data-level="2.4" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html"><i class="fa fa-check"></i><b>2.4</b> Using Supplementary Elements</a><ul>
<li class="chapter" data-level="2.4.1" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#continuous-supplementary-variables"><i class="fa fa-check"></i><b>2.4.1</b> Continuous Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#nominal-supplementary-variables"><i class="fa fa-check"></i><b>2.4.2</b> Nominal Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#profiling-with-v-test"><i class="fa fa-check"></i><b>2.4.3</b> Profiling with V-test</a></li>
<li class="chapter" data-level="2.4.4" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#axes-characterization-using-continuous-variables"><i class="fa fa-check"></i><b>2.4.4</b> Axes Characterization using Continuous Variables</a></li>
<li class="chapter" data-level="2.4.5" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#v-test-and-data-science"><i class="fa fa-check"></i><b>2.4.5</b> V-test and Data Science</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="simultaneous-representations.html"><a href="simultaneous-representations.html"><i class="fa fa-check"></i><b>2.5</b> Simultaneous Representations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="simultaneous-representations.html"><a href="simultaneous-representations.html#old-unit-axes"><i class="fa fa-check"></i><b>2.5.1</b> Old Unit Axes</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Practice</b></span></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="themescope.html"><a href="themescope.html"><i class="fa fa-check"></i><b>3.1</b> Themescope</a></li>
<li class="chapter" data-level="3.2" data-path="conditions-of-application.html"><a href="conditions-of-application.html"><i class="fa fa-check"></i><b>3.2</b> Conditions of Application</a><ul>
<li class="chapter" data-level="3.2.1" data-path="conditions-of-application.html"><a href="conditions-of-application.html#linearity-and-symmetry"><i class="fa fa-check"></i><b>3.2.1</b> Linearity and Symmetry</a></li>
<li class="chapter" data-level="3.2.2" data-path="conditions-of-application.html"><a href="conditions-of-application.html#balancing-the-content-of-active-variables"><i class="fa fa-check"></i><b>3.2.2</b> Balancing the content of active variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html"><i class="fa fa-check"></i><b>3.3</b> Validation: stability and significance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html#how-many-axes-to-study-and-retain"><i class="fa fa-check"></i><b>3.3.1</b> How many axes to study and retain?</a></li>
<li class="chapter" data-level="3.3.2" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html#simulations-random-effects-on-individuals"><i class="fa fa-check"></i><b>3.3.2</b> Simulations, random effects on individuals</a></li>
<li class="chapter" data-level="3.3.3" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html#bootstrap-simulations"><i class="fa fa-check"></i><b>3.3.3</b> Bootstrap Simulations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-table-of-ranks.html"><a href="analysis-of-table-of-ranks.html"><i class="fa fa-check"></i><b>3.4</b> Analysis of Table of Ranks</a></li>
<li class="chapter" data-level="3.5" data-path="optimal-reconstitution-of-data.html"><a href="optimal-reconstitution-of-data.html"><i class="fa fa-check"></i><b>3.5</b> Optimal Reconstitution of Data</a></li>
<li class="chapter" data-level="3.6" data-path="synthetic-variables-and-indices.html"><a href="synthetic-variables-and-indices.html"><i class="fa fa-check"></i><b>3.6</b> Synthetic Variables and Indices</a></li>
<li class="chapter" data-level="3.7" data-path="handling-missing-values.html"><a href="handling-missing-values.html"><i class="fa fa-check"></i><b>3.7</b> Handling Missing Values</a></li>
<li class="chapter" data-level="3.8" data-path="pca-and-clustering.html"><a href="pca-and-clustering.html"><i class="fa fa-check"></i><b>3.8</b> PCA and Clustering</a><ul>
<li class="chapter" data-level="3.8.1" data-path="pca-and-clustering.html"><a href="pca-and-clustering.html#real-groups-or-instrumental-groups"><i class="fa fa-check"></i><b>3.8.1</b> Real Groups or Instrumental Groups?</a></li>
<li class="chapter" data-level="3.8.2" data-path="pca-and-clustering.html"><a href="pca-and-clustering.html#representants-of-groups"><i class="fa fa-check"></i><b>3.8.2</b> Representants of Groups</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="data-weighing.html"><a href="data-weighing.html"><i class="fa fa-check"></i><b>3.9</b> Data Weighing</a></li>
<li class="chapter" data-level="3.10" data-path="pca-as-an-intermediate-analytical-stage.html"><a href="pca-as-an-intermediate-analytical-stage.html"><i class="fa fa-check"></i><b>3.10</b> PCA as an Intermediate Analytical Stage</a></li>
<li class="chapter" data-level="3.11" data-path="comparing-various-tables.html"><a href="comparing-various-tables.html"><i class="fa fa-check"></i><b>3.11</b> Comparing Various Tables</a></li>
<li class="chapter" data-level="3.12" data-path="analysis-of-a-table-of-means.html"><a href="analysis-of-a-table-of-means.html"><i class="fa fa-check"></i><b>3.12</b> Analysis of a Table of Means</a></li>
<li class="chapter" data-level="3.13" data-path="analysis-of-a-binary-table.html"><a href="analysis-of-a-binary-table.html"><i class="fa fa-check"></i><b>3.13</b> Analysis of a Binary Table</a></li>
<li class="chapter" data-level="3.14" data-path="analysis-of-a-table-of-distances.html"><a href="analysis-of-a-table-of-distances.html"><i class="fa fa-check"></i><b>3.14</b> Analysis of a Table of Distances</a></li>
<li class="chapter" data-level="3.15" data-path="conditional-pca.html"><a href="conditional-pca.html"><i class="fa fa-check"></i><b>3.15</b> Conditional PCA</a><ul>
<li class="chapter" data-level="3.15.1" data-path="conditional-pca.html"><a href="conditional-pca.html#pca-on-model-residuals"><i class="fa fa-check"></i><b>3.15.1</b> PCA on Model Residuals</a></li>
<li class="chapter" data-level="3.15.2" data-path="conditional-pca.html"><a href="conditional-pca.html#analysis-of-local-variation"><i class="fa fa-check"></i><b>3.15.2</b> Analysis of Local Variation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Examples</b></span></li>
<li class="chapter" data-level="4" data-path="application-examples.html"><a href="application-examples.html"><i class="fa fa-check"></i><b>4</b> Application Examples</a><ul>
<li class="chapter" data-level="4.1" data-path="lascaux.html"><a href="lascaux.html"><i class="fa fa-check"></i><b>4.1</b> Lascaux Cave Temperatures</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lascaux.html"><a href="lascaux.html#temperature-data"><i class="fa fa-check"></i><b>4.1.1</b> Temperature Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="lascaux.html"><a href="lascaux.html#pca"><i class="fa fa-check"></i><b>4.1.2</b> PCA</a></li>
<li class="chapter" data-level="4.1.3" data-path="lascaux.html"><a href="lascaux.html#seasonal-phenomenon"><i class="fa fa-check"></i><b>4.1.3</b> Seasonal Phenomenon</a></li>
<li class="chapter" data-level="4.1.4" data-path="lascaux.html"><a href="lascaux.html#modeling-propagation-of-thermal-wave"><i class="fa fa-check"></i><b>4.1.4</b> Modeling Propagation of Thermal Wave</a></li>
<li class="chapter" data-level="4.1.5" data-path="lascaux.html"><a href="lascaux.html#stability-of-the-axes"><i class="fa fa-check"></i><b>4.1.5</b> Stability of the Axes</a></li>
<li class="chapter" data-level="4.1.6" data-path="lascaux.html"><a href="lascaux.html#selecting-best-temperature-reading-locations"><i class="fa fa-check"></i><b>4.1.6</b> Selecting Best Temperature Reading Locations</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html"><i class="fa fa-check"></i><b>4.2</b> Design of Experiments and PCA</a><ul>
<li class="chapter" data-level="4.2.1" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html#pca-1"><i class="fa fa-check"></i><b>4.2.1</b> PCA</a></li>
<li class="chapter" data-level="4.2.2" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html#evolution-of-factor-trajectories-over-time"><i class="fa fa-check"></i><b>4.2.2</b> Evolution of Factor Trajectories over Time</a></li>
<li class="chapter" data-level="4.2.3" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html#analysis-of-variance"><i class="fa fa-check"></i><b>4.2.3</b> Analysis of Variance</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="defining-an-economic-capacity-index.html"><a href="defining-an-economic-capacity-index.html"><i class="fa fa-check"></i><b>4.3</b> Defining an Economic Capacity Index</a><ul>
<li class="chapter" data-level="4.3.1" data-path="defining-an-economic-capacity-index.html"><a href="defining-an-economic-capacity-index.html#analyzed-information"><i class="fa fa-check"></i><b>4.3.1</b> Analyzed Information</a></li>
<li class="chapter" data-level="4.3.2" data-path="defining-an-economic-capacity-index.html"><a href="defining-an-economic-capacity-index.html#pca-2"><i class="fa fa-check"></i><b>4.3.2</b> PCA</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Appendix</b></span></li>
<li class="chapter" data-level="5" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>5</b> Appendix A: Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="space-of-p-dimensions.html"><a href="space-of-p-dimensions.html"><i class="fa fa-check"></i><b>5.1</b> Space of p-Dimensions</a></li>
<li class="chapter" data-level="5.2" data-path="distances-between-points.html"><a href="distances-between-points.html"><i class="fa fa-check"></i><b>5.2</b> Distances between points</a></li>
<li class="chapter" data-level="5.3" data-path="center-of-gravity.html"><a href="center-of-gravity.html"><i class="fa fa-check"></i><b>5.3</b> Center of Gravity</a></li>
<li class="chapter" data-level="5.4" data-path="inertia-of-a-cloud-of-points.html"><a href="inertia-of-a-cloud-of-points.html"><i class="fa fa-check"></i><b>5.4</b> Inertia of a cloud of points</a></li>
<li class="chapter" data-level="5.5" data-path="projection-of-the-cloud-of-points-on-a-line.html"><a href="projection-of-the-cloud-of-points-on-a-line.html"><i class="fa fa-check"></i><b>5.5</b> Projection of the cloud of points on a line</a></li>
<li class="chapter" data-level="5.6" data-path="centered-and-standardized-variable.html"><a href="centered-and-standardized-variable.html"><i class="fa fa-check"></i><b>5.6</b> Centered and Standardized Variable</a></li>
<li class="chapter" data-level="5.7" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html"><i class="fa fa-check"></i><b>5.7</b> Correlation Coefficient</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>6</b> Appendix B: PCA Formulae</a><ul>
<li class="chapter" data-level="6.1" data-path="general-analysis.html"><a href="general-analysis.html"><i class="fa fa-check"></i><b>6.1</b> General Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="formulas-for-pca.html"><a href="formulas-for-pca.html"><i class="fa fa-check"></i><b>6.2</b> Formulas for PCA</a></li>
<li class="chapter" data-level="6.3" data-path="biplot-and-pca.html"><a href="biplot-and-pca.html"><i class="fa fa-check"></i><b>6.3</b> Biplot and PCA</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendixc.html"><a href="appendixc.html"><i class="fa fa-check"></i><b>7</b> Appendix C: Data Analysis Reminder</a><ul>
<li class="chapter" data-level="7.1" data-path="normalized-principal-component-analysis.html"><a href="normalized-principal-component-analysis.html"><i class="fa fa-check"></i><b>7.1</b> Normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="non-normalized-principal-component-analysis.html"><a href="non-normalized-principal-component-analysis.html"><i class="fa fa-check"></i><b>7.2</b> Non-normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.3" data-path="simple-correpondence-analysis.html"><a href="simple-correpondence-analysis.html"><i class="fa fa-check"></i><b>7.3</b> Simple Correpondence Analysis</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-correspondence-analysis.html"><a href="multiple-correspondence-analysis.html"><i class="fa fa-check"></i><b>7.4</b> Multiple Correspondence Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="clustering-of-factors.html"><a href="clustering-of-factors.html"><i class="fa fa-check"></i><b>7.5</b> Clustering of Factors</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Principal Component Analysis for Data Science (pca4ds)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="how-to-see-the-distances-between-points" class="section level2">
<h2><span class="header-section-number">1.3</span> How to see the distances between points</h2>
<p>Because both types of clouds—row-points and column-points—are located in
high dimensional spaces, we cannot observe them directly. The essence of
Principal Component Analysis involves searching for a plane on which we project
the cloud of points in such a way that the obtained configuration is as close
as possible to the original configuration of the cloud in the high-dimensional
space. We call this plane the <em>factorial plane</em>.</p>
<p>The way in which we obtain the desired plane, is by making the overall distances
between projected points as close as possible to the real distances between
points in the space of origin.</p>
<p>Let’s consider in first place the cloud of <span class="math inline">\(n\)</span> individual-points located in
the space where each axis corresponds to a variable. The following figure
depicts this idea when we have only three variables.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-5"></span>
<img src="images/figure-1-5.png" alt="Cloud of row-points in first factorial plane" width="90%" />
<p class="caption">
Figure 1.5: Cloud of row-points in first factorial plane
</p>
</div>
<p>The problem consists of finding the factorial plane such that the set of
of all pairs of distances <span class="math inline">\(d_F(i,i&#39;)\)</span> between points, is as close as possible
to the real distances <span class="math inline">\(d_X(i,i&#39;)\)</span> measured in the space of origin.</p>
<div id="how-to-find-the-projection-planes" class="section level3">
<h3><span class="header-section-number">1.3.1</span> How to find the projection planes</h3>
<p>Our goal has to do with finding a subspace of reduced dimension that conserves
tha maximum of information from the original configuration of the cloud. For
instance, let’s pretend that the original cloud has the shape of a mug, like
in the following figure:</p>
<p><img src="images/mug-shaped-cloud.png" width="30%" style="display: block; margin: auto;" /></p>
<p>Furthermore, let’s assume that we can only observe the projection of the mug
on a plane of reduced dimension. The question is: Which plane should we choose?</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-6"></span>
<img src="images/figure-1-6.png" alt="Three projections of a mug-shaped cloud points" width="685" />
<p class="caption">
Figure 1.6: Three projections of a mug-shaped cloud points
</p>
</div>
<p>We can consider of projecting this mug cloud over different planes.
As you can tell, the projection on the plane <span class="math inline">\(H_A\)</span> is much more informative
that the projection on the plane <span class="math inline">\(H_B\)</span>. At least we can see that the figure
on <span class="math inline">\(H_A\)</span> has to do with a lengthened object, and that one of its ends is wider
than the other end. In contrast, all the points of the projected cloud on
the plane <span class="math inline">\(H_B\)</span> are confounded, and it does not convey a clear idea the
original cloud, except for the shadow of the handle. However, the best
projection among the three planes is that of <span class="math inline">\(H_C\)</span>.</p>
<p>We obtain the plane <span class="math inline">\(H_C\)</span> by searching for the plane that makes the dispersion
of the projected points as large as possible:</p>
<p><span class="math display" id="eq:2">\[
Max_H \sum_i \sum_{i&#39;} d^{2}_{H} (i, i&#39;)
\tag{1.2}
\]</span></p>
<p>where <span class="math inline">\(H\)</span> represents the subspace of the projection.</p>
<p>Searching for the maximum can be written as:</p>
<p><span class="math display" id="eq:3">\[
Max_H \sum_i \sum_{i&#39;} d^{2}_{H} (i, i&#39;) = Max_H \left \{ 2n \sum_i d^{2}_{H} (i, G) \right \}
\tag{1.3}
\]</span></p>
<p>The problem of preserving the projected distances between all pairs of points
becomes a problem of preserving the distances between each point and the center of
gravity <span class="math inline">\(G\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-7"></span>
<img src="images/figure-1-7.png" alt="Decomposition of the distance between a row-point and the center of gravity" width="60%" />
<p class="caption">
Figure 1.7: Decomposition of the distance between a row-point and the center of gravity
</p>
</div>
<p>The formula in equation <a href="how-to-see-the-distances-between-points.html#eq:4">(1.4)</a> is actually an expression for the Pythagorean theorem. The spread of the cloud of points can be decomposed into two terms: the spread in the projection plane, and another (orthogonal) term given by the sum of the distances of the points to the projection plane:</p>
<p><span class="math display" id="eq:4">\[
\sum_{i} d^{2} (i, G) = \sum_i d^{2}_{H} (i, G) + \sum_i d^{2}_{\bar{H}} (i, G)
\tag{1.4}
\]</span></p>
<p>In this way, the projection plane that guarantees the maximum dispersion between the points, is also the plane that gets as close as possible to the original cloud (in the sense of least squares criterion). This is expressed in the following relation <a href="how-to-see-the-distances-between-points.html#eq:5">(1.5)</a></p>
<p><span class="math display" id="eq:5">\[
Max \hspace{2mm} \sum_{i} d^{2}_{H} (i, G) \quad \Longleftrightarrow \quad Min \hspace{2mm} d^{2}_{\bar{H}} (i, G)
\tag{1.5}
\]</span></p>
</div>
<div id="how-to-take-into-account-the-importance-of-individuals" class="section level3">
<h3><span class="header-section-number">1.3.2</span> How to take into account the importance of individuals</h3>
<p>Sometimes we may be interested in assigning weights to the individuals based on their relative importance or relevance. When all the individuals have the same importance, we can give a weight equal to <span class="math inline">\(1/n\)</span> to each of them. Thus, the fit criterion becomes:</p>
<p><span class="math display" id="eq:6">\[
Max \hspace{2mm} \sum_{i} \frac{1}{n} d^{2}_{H} (i, G) = Max \hspace{2mm} \frac{\sum_i (x_{iH} - \bar{x}_H)^2}{n}
\tag{1.6}
\]</span></p>
<p>In the general case where each individual has its own weight <span class="math inline">\(p_i\)</span> with <span class="math inline">\(\sum_i p_i = 1\)</span>, then the fit criterion is expressed as:</p>
<p><span class="math display" id="eq:7">\[
Max \hspace{2mm} \sum_{i} p_i \hspace{1mm} d^{2}_{H} (i, G)
\tag{1.7}
\]</span></p>
<p>The product of the weight of a point, <span class="math inline">\(p_i\)</span>, times the squared of its distance to the center of gravity, <span class="math inline">\(d^{2}_{H} (i, G)\)</span>, is known as <em>inertia of the point</em>. In this case, the problem involves looking for the projection plane that maximizes the projected inertia.</p>
</div>
<div id="inertia-decomposition" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Inertia Decomposition</h3>
<p>The total inertia is defined as:</p>
<p><span class="math display" id="eq:8">\[
I = \sum_{i=1}^{n} p_i \hspace{1mm} d^2(i,G)
\tag{1.8}
\]</span></p>
<p>The total inertia can be broken down into two additive terms:</p>
<ul>
<li>projected inertia on a subspace <span class="math inline">\(H\)</span></li>
<li>inertia orthogonally projected on a subspace <span class="math inline">\(\bar{H}\)</span></li>
</ul>
<p><span class="math display" id="eq:9">\[
I = I_H + I_{\bar{H}}
\tag{1.9}
\]</span></p>
<p>The problem of searching for the subspace that makes the dispersion of the projected points as large as possible can also be put in terms of inertias. Namely, we look for a plane <span class="math inline">\(H\)</span> that maximizes the projected inertia (see figure below).</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-8"></span>
<img src="images/figure-1-8.png" alt="Successive directions of maximum inertia" width="60%" />
<p class="caption">
Figure 1.8: Successive directions of maximum inertia
</p>
</div>
<p>In order to find the optimal subspace, we begin by looking for a one-dimensional space (i.e. a line) of maximum projected inertia. If all individuals have the same weight, then this first subspace coincides with the direction of maximum stretch of the cloud.</p>
<p>Having found the first one-dimensional subspace, the next step involves finding a two-dimensional subspace (i.e. a plane) with maximum projected inertia. Then, we look for a three-dimensional space, and so on and so forth. At each step, we look for a higher dimensional space such that the projected inertia is as large as possible.</p>
<p>It can be proved that the two-dimensional plane must contain the one-dimensional space (i.e. the line) of maximum projected inertia. Having found the first direction of maximum inertia, one needs to find another line, orthogonal to the first one, such that the plane formed by them has maximum inertia. Analogously, the subspace of three dimensions is formed with the two-dimensional space by adding an orthogonal direction to this plane (see figure <a href="how-to-see-the-distances-between-points.html#fig:fig-1-8">1.8</a>).</p>
<p>Because the inertia is additive in orthogonal directions, PCA involves the decomposition of the total inertia in <span class="math inline">\(p\)</span> additive components, as many as the number of dimensions in the original space.</p>
<p><span class="math display" id="eq:10">\[
I = I_1 + I_2 + \dots + I_p
\tag{1.10}
\]</span></p>
<p>Interestingly, the inertias (the dispersions) are decreasingly ordered, which means that subsequent orthogonal directions become smaller, that is, <span class="math inline">\(I_1 \geq I_2 \geq \dots \geq I_p\)</span>.</p>
<p>This implies that the configuration of the projected cloud on the first fatorial plane is as close as possible (in the least squares sense) to the original configuration.</p>
<p>For example, the figure below shows the plane of maximum projected inertia for in the data of the Cities.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-9"></span>
<img src="images/figure-1-9.png" alt="Cloud of variable-points in the standardized PCA" width="80%" />
<p class="caption">
Figure 1.9: Cloud of variable-points in the standardized PCA
</p>
</div>
<p>This is the representation of the cities, in a factorial plane, that best resembles the existing distances between the cities according to the salary level. For instance, we can tell that Tokyo, Zurich and Geneva are close to each other, indicating similar salaries among the analyzed professions. The same can be said about Manila, Mumbai, Lagos, and Prague; these cities have similar salaries among them, but they have a contrasting difference with the former group (Tokyo, Zurich, and Geneva).</p>
<p>We can also assume a certain similarity between northen cities such as Stockholm, and Copenhagen, whereas there seems to be a substantial salary difference between Paris and American cities (Los Angeles, Chicago, Toronto, Houston).</p>
<p>The center of the graph represents the average point of the cloud. This corresponds to cities with salaries closer to the mean values.</p>
<p>Notice the difference in scale utilized in the x-axis and the y-axis. Actually, the cloud of points is very extended along the x-axis <span class="math inline">\(F_1\)</span>.</p>
</div>
<div id="visualizing-association-between-variables." class="section level3">
<h3><span class="header-section-number">1.3.4</span> Visualizing association between variables.</h3>
<p>Let’s discuss what the analysis involves regarding the cloud of variable-points. Recall that the cloud of variable-points is defined by the columns of the data matrix <span class="math inline">\(\mathbf{X}\)</span>. Without loss of generality, we will assume that the variables are mean-centered and normalized by the standard deviation.</p>
<p>The first thing to do is to define a measure of distance between variables. One way to represent a certain notion of proximity among variables is given by the following formula:</p>
<p><span class="math display" id="eq:11">\[
d^2(j, j&#39;) = 2 (1 - cor(j, j&#39;))
\tag{1.11}
\]</span></p>
<p>If two variables <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span> measure the same thing (in the sense of having a linear correlation of 1) then their points will be overlapped.</p>
<p>In the case where two variables <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span> have a linear correlation equal to -1 (when one increases, the other decreases), the two variable-points will have a maximum distance in opposite directions.</p>
<p>When one variable does not provide any information about the other one, we have a situation when their correlation coefficient will be zero. This would correspond to an intermediate distance in which the variables form an orthogonal angle. These cases are illustrated in figure <a href="how-to-see-the-distances-between-points.html#fig:fig-1-10">1.10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-10"></span>
<img src="images/figure-1-10.png" alt="Cloud of variable-points in the standardized PCA" width="70%" />
<p class="caption">
Figure 1.10: Cloud of variable-points in the standardized PCA
</p>
</div>
<p>If the correlation is equal to 1, the squared distance is zero. Likewise, if the correlation is -1 the distance is 4, and if the correlation is zero then the distance is 2. This formulae define the so-called standardized Principal Components Analysis.</p>
<p>It can be shown that all the variable-points are located within a one-unit distance from the origin (inside a hypersphere of radius 1). The cloud of variable-points is defined by a set of vectors that start from the center of the sphere. The correlation coefficient between two variables coindices with the cosine of the angle formed by two corresponding vectors (associated to the variables).</p>
<p>Two variables that are close to each other will form a small angle, which corresponds to a large correlation coefficient. Two independent variables will have a zero correlation coefficient, thus forming a square angle between them. In turn, two opposed variables will have a correlation coefficient close to -1 and thus will appear on two opposite locations in the sphere.</p>
<p>Analogously to the cloud of row-points, we also seek to find a projection plane providing the largest amount of information about the associations between the variables. More precisely, we look for a plane that provides information about the angles between variables, that is, about their correlations.</p>
<p>In the case of the 12 variables observed on the 51 cities, we obtain the configuration on the first factorial plane, displayed in figure <a href="how-to-see-the-distances-between-points.html#fig:fig-1-11">1.11</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-11"></span>
<img src="images/figure-1-11.png" alt="Circle of correlations on the first factorial plane" width="70%" />
<p class="caption">
Figure 1.11: Circle of correlations on the first factorial plane
</p>
</div>
<p>Notice that in this case all the variables are positively correlated. Moreover, from the circle of correlations, it is possible to observe two groups of variables. One group is formed by <code>departmental_head</code>, <code>engineers</code>, <code>bank_clerk</code>, and <code>cook_chef</code>. The other group is formed by the rest of the professions.</p>
<p>In the matrix of correlations (see table <a href="how-to-see-the-distances-between-points.html#tab:table-1-3">1.1</a>) we can see the magnitude of the association between the variables. As you can tell, all the correlations are positive and with large value tanging from 0.59 to 0.96.</p>
<table>
<caption><span id="tab:table-1-3">Table 1.1: </span>Matrix of correlations.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">tea</th>
<th align="right">bus</th>
<th align="right">mec</th>
<th align="right">con</th>
<th align="right">met</th>
<th align="right">coo</th>
<th align="right">dep</th>
<th align="right">eng</th>
<th align="right">ban</th>
<th align="right">exe</th>
<th align="right">sal</th>
<th align="right">tex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tea</td>
<td align="right">1.00</td>
<td align="right">0.96</td>
<td align="right">0.84</td>
<td align="right">0.83</td>
<td align="right">0.91</td>
<td align="right">0.75</td>
<td align="right">0.78</td>
<td align="right">0.81</td>
<td align="right">0.82</td>
<td align="right">0.92</td>
<td align="right">0.88</td>
<td align="right">0.88</td>
</tr>
<tr class="even">
<td>bus</td>
<td align="right">0.96</td>
<td align="right">1.00</td>
<td align="right">0.89</td>
<td align="right">0.88</td>
<td align="right">0.94</td>
<td align="right">0.76</td>
<td align="right">0.74</td>
<td align="right">0.82</td>
<td align="right">0.80</td>
<td align="right">0.93</td>
<td align="right">0.89</td>
<td align="right">0.92</td>
</tr>
<tr class="odd">
<td>mec</td>
<td align="right">0.84</td>
<td align="right">0.89</td>
<td align="right">1.00</td>
<td align="right">0.95</td>
<td align="right">0.93</td>
<td align="right">0.80</td>
<td align="right">0.64</td>
<td align="right">0.74</td>
<td align="right">0.70</td>
<td align="right">0.88</td>
<td align="right">0.89</td>
<td align="right">0.89</td>
</tr>
<tr class="even">
<td>con</td>
<td align="right">0.83</td>
<td align="right">0.88</td>
<td align="right">0.95</td>
<td align="right">1.00</td>
<td align="right">0.93</td>
<td align="right">0.72</td>
<td align="right">0.59</td>
<td align="right">0.70</td>
<td align="right">0.64</td>
<td align="right">0.86</td>
<td align="right">0.86</td>
<td align="right">0.92</td>
</tr>
<tr class="odd">
<td>met</td>
<td align="right">0.91</td>
<td align="right">0.94</td>
<td align="right">0.93</td>
<td align="right">0.93</td>
<td align="right">1.00</td>
<td align="right">0.76</td>
<td align="right">0.69</td>
<td align="right">0.80</td>
<td align="right">0.72</td>
<td align="right">0.92</td>
<td align="right">0.88</td>
<td align="right">0.94</td>
</tr>
<tr class="even">
<td>coo</td>
<td align="right">0.75</td>
<td align="right">0.76</td>
<td align="right">0.80</td>
<td align="right">0.72</td>
<td align="right">0.76</td>
<td align="right">1.00</td>
<td align="right">0.82</td>
<td align="right">0.82</td>
<td align="right">0.79</td>
<td align="right">0.80</td>
<td align="right">0.85</td>
<td align="right">0.71</td>
</tr>
<tr class="odd">
<td>dep</td>
<td align="right">0.78</td>
<td align="right">0.74</td>
<td align="right">0.64</td>
<td align="right">0.59</td>
<td align="right">0.69</td>
<td align="right">0.82</td>
<td align="right">1.00</td>
<td align="right">0.87</td>
<td align="right">0.89</td>
<td align="right">0.80</td>
<td align="right">0.79</td>
<td align="right">0.65</td>
</tr>
<tr class="even">
<td>eng</td>
<td align="right">0.81</td>
<td align="right">0.82</td>
<td align="right">0.74</td>
<td align="right">0.70</td>
<td align="right">0.80</td>
<td align="right">0.82</td>
<td align="right">0.87</td>
<td align="right">1.00</td>
<td align="right">0.85</td>
<td align="right">0.87</td>
<td align="right">0.85</td>
<td align="right">0.81</td>
</tr>
<tr class="odd">
<td>ban</td>
<td align="right">0.82</td>
<td align="right">0.80</td>
<td align="right">0.70</td>
<td align="right">0.64</td>
<td align="right">0.72</td>
<td align="right">0.79</td>
<td align="right">0.89</td>
<td align="right">0.85</td>
<td align="right">1.00</td>
<td align="right">0.87</td>
<td align="right">0.85</td>
<td align="right">0.73</td>
</tr>
<tr class="even">
<td>exe</td>
<td align="right">0.92</td>
<td align="right">0.93</td>
<td align="right">0.88</td>
<td align="right">0.86</td>
<td align="right">0.92</td>
<td align="right">0.80</td>
<td align="right">0.80</td>
<td align="right">0.87</td>
<td align="right">0.87</td>
<td align="right">1.00</td>
<td align="right">0.94</td>
<td align="right">0.93</td>
</tr>
<tr class="odd">
<td>sal</td>
<td align="right">0.88</td>
<td align="right">0.89</td>
<td align="right">0.89</td>
<td align="right">0.86</td>
<td align="right">0.88</td>
<td align="right">0.85</td>
<td align="right">0.79</td>
<td align="right">0.85</td>
<td align="right">0.85</td>
<td align="right">0.94</td>
<td align="right">1.00</td>
<td align="right">0.89</td>
</tr>
<tr class="even">
<td>tex</td>
<td align="right">0.88</td>
<td align="right">0.92</td>
<td align="right">0.89</td>
<td align="right">0.92</td>
<td align="right">0.94</td>
<td align="right">0.71</td>
<td align="right">0.65</td>
<td align="right">0.81</td>
<td align="right">0.73</td>
<td align="right">0.93</td>
<td align="right">0.89</td>
<td align="right">1.00</td>
</tr>
</tbody>
</table>
<p>The fact that all the variables are positively correlated implies that if one type of salary in a city is high, then the other salaries in that city will also be high.</p>
<p>In later sections of the book we will emphasize the idea that Principal Component Analysis can be approached from the point of view of the variable-points having a distance based on the correlation between the variables. In other words, a PCA on the row-points is not an independent analysis from a PCA on the variable-points. Quite the opposite in fact, it is possible to obtain the results of a PCA on a set of points (e.g. variables) given the results of the other set (e.g. the rows). This, as we’ll see, provides a set of rules extremely valuable in the interpretation of results.</p>
</div>
<div id="normalized-pca-or-non-normalized-pca" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Normalized PCA or non-normalized PCA?</h3>
<p>As we’ve seen, Principal Component Analysis consists of decomposing the inertia of the original cloud, based on some orthogonal directions. Each of the obtained orthodonal directions accounts for a proportion of the original inertia.</p>
<p>But, what is the contribution of each variable to the original inertia?</p>
<p>The distance between variable-points defined in equation <a href="how-to-see-the-distances-between-points.html#eq:11">(1.11)</a> implies that the contribution, of each variable, to the total inertia is the same for all variables, equal to <span class="math inline">\(1/p\)</span>.</p>
<p>The inertia of the cloud of variable-points with respect to the origin coincides with the number of active variables.</p>
<p><span class="math display" id="eq:12">\[
I_T = \sum_{j=1}^{p} d^2(j, O) = p
\tag{1.12}
\]</span></p>
<p>In order for equation <a href="how-to-see-the-distances-between-points.html#eq:12">(1.12)</a> to make sense, we need to use standardized values (mean-centered and unit-variance):</p>
<p><span class="math display" id="eq:13">\[
z_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j}
\tag{1.13}
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the mean of variable <span class="math inline">\(j\)</span> and <span class="math inline">\(s_j\)</span> is the corresponding standard deviation. In this case we talk about <em>Normalized Principal Component Analysis</em>.</p>
<p>With standardized data, the distance of each variable and the origin is equal to 1:</p>
<p><span class="math display" id="eq:14">\[
d^2(j, O) = \sum_{j=1}^{n} \frac{1}{n} \left ( \frac{x_{ij} - \bar{x}_j}{s_j} \right )^2 = \frac{\frac{1}{n} \sum_j (x_{ij} - \bar{x}_j)^2}{s_{j}^{2}} = 1
\tag{1.14}
\]</span></p>
<p>We should mention that the use of Normalized PCA is not always justified. For example, if a PCA is performed by an analyst working for a bank, if may be more interesting to assign a larger weight to the products that contribute to the volume of the deposits. That is, the importance given to different variables should be done by taking into account the goal of the analysis.</p>
<p>If we use the <em>raw</em> data (i.e. mean-centered only, no scaling by the standard deviation), the squared distance of one variable to the origin is given by:</p>
<p><span class="math display" id="eq:15">\[
d^2(j,O) = \sum_{j=1}^{n} \frac{1}{n} (x_{ij} - \bar{x}_j)^2 = var(j)
\tag{1.15}
\]</span></p>
<p>With non-normalized data, the variables are no longer found inside a sphere of radious 1. Instead, the length of the segment (i.e. the vector) of each variable is equal to its standard deviation. We can then think of the cloud of variable-points as a set of vectors, each one of length equal to the standard deviation of the variable, and forming angles defined by the correlation coefficient between the variables.</p>
<p>This type of analysis is called <em>Non-Normalized Principal Component Analysis</em>.</p>
<p>With this type of PCA, the distance between two variables depends on the correlation (i.e. the angle between them), as well as on their variances.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-12"></span>
<img src="images/figure-1-12.png" alt="Cloud of variable-points in a non-normalized PCA" width="50%" />
<p class="caption">
Figure 1.12: Cloud of variable-points in a non-normalized PCA
</p>
</div>
<p>The total inertia of the cloud of variable-points is given by the sum of the variances of each variable:</p>
<p><span class="math display" id="eq:16">\[
I_T = \sum_{j=1}^{p} d^2(j,O) = \sum_{j=1}^{p} var(j)
\tag{1.16}
\]</span></p>
<p>The contribution of each variable to the total inertia is given by:</p>
<p><span class="math display" id="eq:17">\[
ctr_j = \frac{var(j)}{\sum_j var(j)}
\tag{1.17}
\]</span></p>
<p>The variance is a function of the unit of measurement in which a variable is measured. This provides a valuable degree of freedom to tune the importance of each variable in the analysis.</p>
<p>In practice, it is preferable to assigne the same importance to all the variables. This is a requirement for when the active variables have different units of measurement (e.g. euros, grams, meters, etc).</p>
<p>In our working example with the data about salaries in various cities, the summary statistics for the active variables are displayed in the following table <a href="how-to-see-the-distances-between-points.html#tab:table-1-4">1.2</a></p>
<table>
<caption><span id="tab:table-1-4">Table 1.2: </span>Summary statistics of active variables.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">weight</th>
<th align="right">mean</th>
<th align="right">stdev</th>
<th align="right">min</th>
<th align="right">max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>teacher</td>
<td align="right">51</td>
<td align="right">16801.96</td>
<td align="right">13375.19</td>
<td align="right">600</td>
<td align="right">56800</td>
</tr>
<tr class="even">
<td>bus_driver</td>
<td align="right">51</td>
<td align="right">14311.76</td>
<td align="right">10927.24</td>
<td align="right">400</td>
<td align="right">46100</td>
</tr>
<tr class="odd">
<td>mechanic</td>
<td align="right">51</td>
<td align="right">12384.31</td>
<td align="right">8605.61</td>
<td align="right">700</td>
<td align="right">30500</td>
</tr>
<tr class="even">
<td>construction_worker</td>
<td align="right">51</td>
<td align="right">10343.14</td>
<td align="right">8321.81</td>
<td align="right">200</td>
<td align="right">28000</td>
</tr>
<tr class="odd">
<td>metalworker</td>
<td align="right">51</td>
<td align="right">15145.10</td>
<td align="right">10346.23</td>
<td align="right">800</td>
<td align="right">38700</td>
</tr>
<tr class="even">
<td>cook_chef</td>
<td align="right">51</td>
<td align="right">15615.69</td>
<td align="right">8855.67</td>
<td align="right">500</td>
<td align="right">33900</td>
</tr>
<tr class="odd">
<td>factory_manager</td>
<td align="right">51</td>
<td align="right">30933.33</td>
<td align="right">21462.03</td>
<td align="right">1500</td>
<td align="right">95000</td>
</tr>
<tr class="even">
<td>engineer</td>
<td align="right">51</td>
<td align="right">24664.71</td>
<td align="right">14158.57</td>
<td align="right">1600</td>
<td align="right">59700</td>
</tr>
<tr class="odd">
<td>bank_clerk</td>
<td align="right">51</td>
<td align="right">18749.02</td>
<td align="right">13547.30</td>
<td align="right">1200</td>
<td align="right">58800</td>
</tr>
<tr class="even">
<td>executive_secretary</td>
<td align="right">51</td>
<td align="right">13311.76</td>
<td align="right">7645.12</td>
<td align="right">1400</td>
<td align="right">31500</td>
</tr>
<tr class="odd">
<td>salesperson</td>
<td align="right">51</td>
<td align="right">9658.82</td>
<td align="right">6124.87</td>
<td align="right">400</td>
<td align="right">24700</td>
</tr>
<tr class="even">
<td>textile_worker</td>
<td align="right">51</td>
<td align="right">9247.06</td>
<td align="right">6493.76</td>
<td align="right">300</td>
<td align="right">23800</td>
</tr>
</tbody>
</table>
<p>From the table above, we see that professions <code>factory manager</code> and <code>engineer</code> are the ones that have, on average, the higher salaries. Then we have <code>bank clerk</code>, <code>teacher</code>, <code>cook chef</code>, and <code>metalworker</code>. And in the last two places there is <code>salesperson</code> and <code>textile worker</code>, which are considered to be low-skills professions mostly performed by women.</p>
<p>Because all active variables are measured in the same unit (in dollars), we could carry out a non-normalized PCA. However, if we did this type of analysis, it would imply giving more importance to those professions with higher salary. Why? Because these are the variables that have a larger spread. If our goal is to compare the cities by giving the same importance to all professions, then it is more suitable to apply a normalized PCA. This, in turn, let us focus on the matrix of correlations among the active variables.</p>
<p>Interestingly, both cloud of points—individuals and variables—have the same inertia. On one hand, the inertia of the cloud of row-points is the sum of the squared distances between each point and the center of gravity, weighed by the importance of each individual. This inertia can be expressed with respect to each axis in the original space (in which each axis corresponds to a variable):</p>
<p><span class="math display" id="eq:18">\[
I_T = \sum_{i=1}^{n} \frac{1}{n} \sum_{j=1}^{p} (x_{ij} - \bar{x}_j)^2 =
\sum_{j=1}^{p} \sum_{i=1}^{n} \frac{1}{n} (x_{ij} - \bar{x}_j)^2 = \sum_{j=1}^{p} var(j)
\tag{1.18}
\]</span></p>
<p>The variance along a given axis is the spread of the projected coud on that axis. Because the axes are orthogonal, the total inertia is equal to the sum of the variances of the variables. Therefore, the inertias of both clouds are the same.</p>
</div>
<div id="distance-matrices" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Distance Matrices</h3>
<p>The rows (i.e. the cities) are located in a space where we measure distance in the classic sense. In a normalized PCA we have:</p>
<p><span class="math display" id="eq:19">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} \left ( \frac{x_{ij} - x_{i&#39;j}}{s_j} \right )^2
\tag{1.19}
\]</span></p>
<p>And in the case of a non-normalized PCA:</p>
<p><span class="math display" id="eq:20">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} (x_{ij} - x_{i&#39;j})^2
\tag{1.20}
\]</span></p>
<p>In the cloud of variable-points, the distance is defined by the formula <a href="how-to-see-the-distances-between-points.html#eq:11">(1.11)</a> when we are carrying out a normalized PCA. In the case of a non-normalized PCA then distance becomes:</p>
<p><span class="math display" id="eq:21">\[
d^2(j, j&#39;) = var(j) + var(j&#39;) - 2 cov(j,j&#39;)
\tag{1.21}
\]</span></p>
<p>All these distances can be organized in square matrices: an <span class="math inline">\(n \times n\)</span> distance matrix for the distances between individuals, and a <span class="math inline">\(p \times p\)</span> matrix for the distances between variables.</p>
<p>If we don’t defined how to measure the distances between points, then the clouds of points remain undefined. In our approximation, we assume that the rows (the cities) are located in a metric (Euclidean) space, which means that the distance is measured by the classic formula of the sum of squared differences:</p>
<p><span class="math display" id="eq:22">\[
d^2 (i, i&#39;) = \sum_{j=1}^{p} (x_{ij} - x_{i&#39;j})^2 = (x_i - x_{i&#39;}) \mathbf{I} (x_i - x_{i&#39;})
\tag{1.22}
\]</span></p>
<p>Notice that in this case, calculating the distance between two individuals takes the form of a scalar product, where the metric matrix is the identity matrix. This corresponds to a non-normalized PCA.</p>
<p>In contrast, when we carry out a normalized PCA, the distance between two individuals is measured by the formula:</p>
<p><span class="math display" id="eq:23">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} \left ( \frac{x_{ij} - x_{i&#39;j}}{s_j} \right )^2 = 
(x_i - x_{i&#39;}) \mathbf{S}^{-2} (x_i - x_{i&#39;})
\tag{1.23}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{S}^{-2} = \left(\begin{array}{cccc}
\dots &amp; 0 &amp; 0 &amp; 0  \\
0 &amp; 1/s_{j}^{2} &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \dots  \\
\end{array}\right)
\]</span></p>
<p>In this case, the metric matrix is given by <span class="math inline">\(\mathbf{S}^{-2}\)</span> (diagonal matrix of inverses of variable variances). Notice that all euclidean metrics can be expressed in a canonical form by using a change of coordinates. When using <span class="math inline">\(\mathbf{S}^{-2}\)</span> such change of coordinates involves dividing the coordinates of the points by the standard deviation of each variable.</p>
<p>Regarding the cloud of variable-points, the distance between variables is defined based on the correlation between two variables. In general, this distance is given by the scalar product between two vectors:</p>
<p><span class="math display" id="eq:24">\[
cor(j,j&#39;) = \sum_{i=1}^{n} \frac{1}{n} \left ( \frac{x_{ij} - \bar{x}_j}{s_j} \right ) \left ( \frac{x_{ij&#39;} - \bar{x}_{j&#39;}}{s_{j&#39;}} \right ) = \mathbf{z_{j}^{\mathsf{T}} N z_j}
\tag{1.24}
\]</span></p>
<p>The natural metric matrix to be used in this case is the diagonal matrix <span class="math inline">\(\mathbf{N}\)</span> of weights for individuals (<span class="math inline">\(1/n\)</span> or <span class="math inline">\(p_i\)</span>). Thus, the distance between variables is defined as:</p>
<p><span class="math display" id="eq:25">\[
d^2(j,j&#39;) = &lt;z_j, z_{j&#39;}&gt; + &lt;z_j, z_{j&#39;}&gt; - 2&lt;z_j, z_{j&#39;}&gt;
\tag{1.25}
\]</span></p>
<p>which coincides with <a href="how-to-see-the-distances-between-points.html#eq:17">(1.17)</a> when using mean-centered data and standardized (normalized PCA), or with <a href="how-to-see-the-distances-between-points.html#eq:16">(1.16)</a> when using non-centenred data
(non-normalized PCA; <a href="how-to-see-the-distances-between-points.html#eq:11">(1.11)</a> and <a href="how-to-see-the-distances-between-points.html#eq:21">(1.21)</a>).</p>

</div>
</div>
<!-- </div> -->



            </section>

          </div>
        </div>
      </div>
<a href="analysis-of-distances.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mechanics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
