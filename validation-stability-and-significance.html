<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Validation: stability and significance | Principal Component Analysis for Data Science (pca4ds)</title>
  <meta name="description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Validation: stability and significance | Principal Component Analysis for Data Science (pca4ds)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="github-repo" content="gastonstat/pca4ds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Validation: stability and significance | Principal Component Analysis for Data Science (pca4ds)" />
  
  <meta name="twitter:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  

<meta name="author" content="Tomas Aluja-Banet Alain Morineau Gaston Sanchez" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conditions-of-application.html"/>
<link rel="next" href="analysis-of-table-of-ranks.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>PCA for Data Science</b><br><small>T. Aluja, A. Morineau, G. Sanchez</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Preface</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>PCA4DS</a></li>
<li class="chapter" data-level="" data-path="from-lanalyse-des-données-to-data-science.html"><a href="from-lanalyse-des-données-to-data-science.html"><i class="fa fa-check"></i>From “L’Analyse des Données” to Data Science</a></li>
<li class="chapter" data-level="" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i>Terminology</a></li>
<li class="part"><span><b>II Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>1</b> Basic Elements</a><ul>
<li class="chapter" data-level="1.1" data-path="data-and-goals.html"><a href="data-and-goals.html"><i class="fa fa-check"></i><b>1.1</b> Data and Goals</a><ul>
<li class="chapter" data-level="1.1.1" data-path="data-and-goals.html"><a href="data-and-goals.html#active-variables"><i class="fa fa-check"></i><b>1.1.1</b> Active Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="analysis-of-distances.html"><a href="analysis-of-distances.html"><i class="fa fa-check"></i><b>1.2</b> Analysis of Distances</a><ul>
<li class="chapter" data-level="1.2.1" data-path="analysis-of-distances.html"><a href="analysis-of-distances.html#cloud-of-row-points"><i class="fa fa-check"></i><b>1.2.1</b> Cloud of Row-Points</a></li>
<li class="chapter" data-level="1.2.2" data-path="analysis-of-distances.html"><a href="analysis-of-distances.html#cloud-of-column-points"><i class="fa fa-check"></i><b>1.2.2</b> Cloud of Column-Points</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html"><i class="fa fa-check"></i><b>1.3</b> How to see the distances between points</a><ul>
<li class="chapter" data-level="1.3.1" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#how-to-find-the-projection-planes"><i class="fa fa-check"></i><b>1.3.1</b> How to find the projection planes</a></li>
<li class="chapter" data-level="1.3.2" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#how-to-take-into-account-the-importance-of-individuals"><i class="fa fa-check"></i><b>1.3.2</b> How to take into account the importance of individuals</a></li>
<li class="chapter" data-level="1.3.3" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#inertia-decomposition"><i class="fa fa-check"></i><b>1.3.3</b> Inertia Decomposition</a></li>
<li class="chapter" data-level="1.3.4" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#visualizing-association-between-variables."><i class="fa fa-check"></i><b>1.3.4</b> Visualizing association between variables.</a></li>
<li class="chapter" data-level="1.3.5" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#normalized-pca-or-non-normalized-pca"><i class="fa fa-check"></i><b>1.3.5</b> Normalized PCA or non-normalized PCA?</a></li>
<li class="chapter" data-level="1.3.6" data-path="how-to-see-the-distances-between-points.html"><a href="how-to-see-the-distances-between-points.html#distance-matrices"><i class="fa fa-check"></i><b>1.3.6</b> Distance Matrices</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Mechanics</b></span></li>
<li class="chapter" data-level="2" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>2</b> How Does PCA Work?</a><ul>
<li class="chapter" data-level="2.1" data-path="principal-components.html"><a href="principal-components.html"><i class="fa fa-check"></i><b>2.1</b> Principal Components</a><ul>
<li class="chapter" data-level="2.1.1" data-path="principal-components.html"><a href="principal-components.html#interpreting-the-inertia-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Interpreting the Inertia Proportions</a></li>
<li class="chapter" data-level="2.1.2" data-path="principal-components.html"><a href="principal-components.html#how-many-axes-to-retain"><i class="fa fa-check"></i><b>2.1.2</b> How many axes to retain?</a></li>
<li class="chapter" data-level="2.1.3" data-path="principal-components.html"><a href="principal-components.html#coordinates-of-row-points"><i class="fa fa-check"></i><b>2.1.3</b> Coordinates of row-points</a></li>
<li class="chapter" data-level="2.1.4" data-path="principal-components.html"><a href="principal-components.html#interpretation-tools"><i class="fa fa-check"></i><b>2.1.4</b> Interpretation Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="projections-of-variables.html"><a href="projections-of-variables.html"><i class="fa fa-check"></i><b>2.2</b> Projections of Variables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="projections-of-variables.html"><a href="projections-of-variables.html#size-effect"><i class="fa fa-check"></i><b>2.2.1</b> Size Effect</a></li>
<li class="chapter" data-level="2.2.2" data-path="projections-of-variables.html"><a href="projections-of-variables.html#tools-for-interpreting-components"><i class="fa fa-check"></i><b>2.2.2</b> Tools for Interpreting Components</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="size-factor.html"><a href="size-factor.html"><i class="fa fa-check"></i><b>2.3</b> Beyond the First Factor</a></li>
<li class="chapter" data-level="2.4" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html"><i class="fa fa-check"></i><b>2.4</b> Using Supplementary Elements</a><ul>
<li class="chapter" data-level="2.4.1" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#continuous-supplementary-variables"><i class="fa fa-check"></i><b>2.4.1</b> Continuous Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#nominal-supplementary-variables"><i class="fa fa-check"></i><b>2.4.2</b> Nominal Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#profiling-with-v-test"><i class="fa fa-check"></i><b>2.4.3</b> Profiling with V-test</a></li>
<li class="chapter" data-level="2.4.4" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#axes-characterization-using-continuous-variables"><i class="fa fa-check"></i><b>2.4.4</b> Axes Characterization using Continuous Variables</a></li>
<li class="chapter" data-level="2.4.5" data-path="using-supplementary-elements.html"><a href="using-supplementary-elements.html#v-test-and-data-science"><i class="fa fa-check"></i><b>2.4.5</b> V-test and Data Science</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="simultaneous-representations.html"><a href="simultaneous-representations.html"><i class="fa fa-check"></i><b>2.5</b> Simultaneous Representations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="simultaneous-representations.html"><a href="simultaneous-representations.html#old-unit-axes"><i class="fa fa-check"></i><b>2.5.1</b> Old Unit Axes</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Practice</b></span></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="themescope.html"><a href="themescope.html"><i class="fa fa-check"></i><b>3.1</b> Themescope</a></li>
<li class="chapter" data-level="3.2" data-path="conditions-of-application.html"><a href="conditions-of-application.html"><i class="fa fa-check"></i><b>3.2</b> Conditions of Application</a><ul>
<li class="chapter" data-level="3.2.1" data-path="conditions-of-application.html"><a href="conditions-of-application.html#linearity-and-symmetry"><i class="fa fa-check"></i><b>3.2.1</b> Linearity and Symmetry</a></li>
<li class="chapter" data-level="3.2.2" data-path="conditions-of-application.html"><a href="conditions-of-application.html#balancing-the-content-of-active-variables"><i class="fa fa-check"></i><b>3.2.2</b> Balancing the content of active variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html"><i class="fa fa-check"></i><b>3.3</b> Validation: stability and significance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html#how-many-axes-to-study-and-retain"><i class="fa fa-check"></i><b>3.3.1</b> How many axes to study and retain?</a></li>
<li class="chapter" data-level="3.3.2" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html#simulations-random-effects-on-individuals"><i class="fa fa-check"></i><b>3.3.2</b> Simulations, random effects on individuals</a></li>
<li class="chapter" data-level="3.3.3" data-path="validation-stability-and-significance.html"><a href="validation-stability-and-significance.html#bootstrap-simulations"><i class="fa fa-check"></i><b>3.3.3</b> Bootstrap Simulations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-table-of-ranks.html"><a href="analysis-of-table-of-ranks.html"><i class="fa fa-check"></i><b>3.4</b> Analysis of Table of Ranks</a></li>
<li class="chapter" data-level="3.5" data-path="optimal-reconstitution-of-data.html"><a href="optimal-reconstitution-of-data.html"><i class="fa fa-check"></i><b>3.5</b> Optimal Reconstitution of Data</a></li>
<li class="chapter" data-level="3.6" data-path="synthetic-variables-and-indices.html"><a href="synthetic-variables-and-indices.html"><i class="fa fa-check"></i><b>3.6</b> Synthetic Variables and Indices</a></li>
<li class="chapter" data-level="3.7" data-path="handling-missing-values.html"><a href="handling-missing-values.html"><i class="fa fa-check"></i><b>3.7</b> Handling Missing Values</a></li>
<li class="chapter" data-level="3.8" data-path="pca-and-clustering.html"><a href="pca-and-clustering.html"><i class="fa fa-check"></i><b>3.8</b> PCA and Clustering</a><ul>
<li class="chapter" data-level="3.8.1" data-path="pca-and-clustering.html"><a href="pca-and-clustering.html#real-groups-or-instrumental-groups"><i class="fa fa-check"></i><b>3.8.1</b> Real Groups or Instrumental Groups?</a></li>
<li class="chapter" data-level="3.8.2" data-path="pca-and-clustering.html"><a href="pca-and-clustering.html#representants-of-groups"><i class="fa fa-check"></i><b>3.8.2</b> Representants of Groups</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="data-weighing.html"><a href="data-weighing.html"><i class="fa fa-check"></i><b>3.9</b> Data Weighing</a></li>
<li class="chapter" data-level="3.10" data-path="pca-as-an-intermediate-analytical-stage.html"><a href="pca-as-an-intermediate-analytical-stage.html"><i class="fa fa-check"></i><b>3.10</b> PCA as an Intermediate Analytical Stage</a></li>
<li class="chapter" data-level="3.11" data-path="comparing-various-tables.html"><a href="comparing-various-tables.html"><i class="fa fa-check"></i><b>3.11</b> Comparing Various Tables</a></li>
<li class="chapter" data-level="3.12" data-path="analysis-of-a-table-of-means.html"><a href="analysis-of-a-table-of-means.html"><i class="fa fa-check"></i><b>3.12</b> Analysis of a Table of Means</a></li>
<li class="chapter" data-level="3.13" data-path="analysis-of-a-binary-table.html"><a href="analysis-of-a-binary-table.html"><i class="fa fa-check"></i><b>3.13</b> Analysis of a Binary Table</a></li>
<li class="chapter" data-level="3.14" data-path="analysis-of-a-table-of-distances.html"><a href="analysis-of-a-table-of-distances.html"><i class="fa fa-check"></i><b>3.14</b> Analysis of a Table of Distances</a></li>
<li class="chapter" data-level="3.15" data-path="conditional-pca.html"><a href="conditional-pca.html"><i class="fa fa-check"></i><b>3.15</b> Conditional PCA</a><ul>
<li class="chapter" data-level="3.15.1" data-path="conditional-pca.html"><a href="conditional-pca.html#pca-on-model-residuals"><i class="fa fa-check"></i><b>3.15.1</b> PCA on Model Residuals</a></li>
<li class="chapter" data-level="3.15.2" data-path="conditional-pca.html"><a href="conditional-pca.html#analysis-of-local-variation"><i class="fa fa-check"></i><b>3.15.2</b> Analysis of Local Variation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Examples</b></span></li>
<li class="chapter" data-level="4" data-path="application-examples.html"><a href="application-examples.html"><i class="fa fa-check"></i><b>4</b> Application Examples</a><ul>
<li class="chapter" data-level="4.1" data-path="lascaux.html"><a href="lascaux.html"><i class="fa fa-check"></i><b>4.1</b> Lascaux Cave Temperatures</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lascaux.html"><a href="lascaux.html#temperature-data"><i class="fa fa-check"></i><b>4.1.1</b> Temperature Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="lascaux.html"><a href="lascaux.html#pca"><i class="fa fa-check"></i><b>4.1.2</b> PCA</a></li>
<li class="chapter" data-level="4.1.3" data-path="lascaux.html"><a href="lascaux.html#seasonal-phenomenon"><i class="fa fa-check"></i><b>4.1.3</b> Seasonal Phenomenon</a></li>
<li class="chapter" data-level="4.1.4" data-path="lascaux.html"><a href="lascaux.html#modeling-propagation-of-thermal-wave"><i class="fa fa-check"></i><b>4.1.4</b> Modeling Propagation of Thermal Wave</a></li>
<li class="chapter" data-level="4.1.5" data-path="lascaux.html"><a href="lascaux.html#stability-of-the-axes"><i class="fa fa-check"></i><b>4.1.5</b> Stability of the Axes</a></li>
<li class="chapter" data-level="4.1.6" data-path="lascaux.html"><a href="lascaux.html#selecting-best-temperature-reading-locations"><i class="fa fa-check"></i><b>4.1.6</b> Selecting Best Temperature Reading Locations</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html"><i class="fa fa-check"></i><b>4.2</b> Design of Experiments and PCA</a><ul>
<li class="chapter" data-level="4.2.1" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html#pca-1"><i class="fa fa-check"></i><b>4.2.1</b> PCA</a></li>
<li class="chapter" data-level="4.2.2" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html#evolution-of-factor-trajectories-over-time"><i class="fa fa-check"></i><b>4.2.2</b> Evolution of Factor Trajectories over Time</a></li>
<li class="chapter" data-level="4.2.3" data-path="design-of-experiments-and-pca.html"><a href="design-of-experiments-and-pca.html#analysis-of-variance"><i class="fa fa-check"></i><b>4.2.3</b> Analysis of Variance</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="defining-an-economic-capacity-index.html"><a href="defining-an-economic-capacity-index.html"><i class="fa fa-check"></i><b>4.3</b> Defining an Economic Capacity Index</a><ul>
<li class="chapter" data-level="4.3.1" data-path="defining-an-economic-capacity-index.html"><a href="defining-an-economic-capacity-index.html#analyzed-information"><i class="fa fa-check"></i><b>4.3.1</b> Analyzed Information</a></li>
<li class="chapter" data-level="4.3.2" data-path="defining-an-economic-capacity-index.html"><a href="defining-an-economic-capacity-index.html#pca-2"><i class="fa fa-check"></i><b>4.3.2</b> PCA</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Appendix</b></span></li>
<li class="chapter" data-level="5" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>5</b> Appendix A: Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="space-of-p-dimensions.html"><a href="space-of-p-dimensions.html"><i class="fa fa-check"></i><b>5.1</b> Space of p-Dimensions</a></li>
<li class="chapter" data-level="5.2" data-path="distances-between-points.html"><a href="distances-between-points.html"><i class="fa fa-check"></i><b>5.2</b> Distances between points</a></li>
<li class="chapter" data-level="5.3" data-path="center-of-gravity.html"><a href="center-of-gravity.html"><i class="fa fa-check"></i><b>5.3</b> Center of Gravity</a></li>
<li class="chapter" data-level="5.4" data-path="inertia-of-a-cloud-of-points.html"><a href="inertia-of-a-cloud-of-points.html"><i class="fa fa-check"></i><b>5.4</b> Inertia of a cloud of points</a></li>
<li class="chapter" data-level="5.5" data-path="projection-of-the-cloud-of-points-on-a-line.html"><a href="projection-of-the-cloud-of-points-on-a-line.html"><i class="fa fa-check"></i><b>5.5</b> Projection of the cloud of points on a line</a></li>
<li class="chapter" data-level="5.6" data-path="centered-and-standardized-variable.html"><a href="centered-and-standardized-variable.html"><i class="fa fa-check"></i><b>5.6</b> Centered and Standardized Variable</a></li>
<li class="chapter" data-level="5.7" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html"><i class="fa fa-check"></i><b>5.7</b> Correlation Coefficient</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>6</b> Appendix B: PCA Formulae</a><ul>
<li class="chapter" data-level="6.1" data-path="general-analysis.html"><a href="general-analysis.html"><i class="fa fa-check"></i><b>6.1</b> General Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="formulas-for-pca.html"><a href="formulas-for-pca.html"><i class="fa fa-check"></i><b>6.2</b> Formulas for PCA</a></li>
<li class="chapter" data-level="6.3" data-path="biplot-and-pca.html"><a href="biplot-and-pca.html"><i class="fa fa-check"></i><b>6.3</b> Biplot and PCA</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendixc.html"><a href="appendixc.html"><i class="fa fa-check"></i><b>7</b> Appendix C: Data Analysis Reminder</a><ul>
<li class="chapter" data-level="7.1" data-path="normalized-principal-component-analysis.html"><a href="normalized-principal-component-analysis.html"><i class="fa fa-check"></i><b>7.1</b> Normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="non-normalized-principal-component-analysis.html"><a href="non-normalized-principal-component-analysis.html"><i class="fa fa-check"></i><b>7.2</b> Non-normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.3" data-path="simple-correpondence-analysis.html"><a href="simple-correpondence-analysis.html"><i class="fa fa-check"></i><b>7.3</b> Simple Correpondence Analysis</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-correspondence-analysis.html"><a href="multiple-correspondence-analysis.html"><i class="fa fa-check"></i><b>7.4</b> Multiple Correspondence Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="clustering-of-factors.html"><a href="clustering-of-factors.html"><i class="fa fa-check"></i><b>7.5</b> Clustering of Factors</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Principal Component Analysis for Data Science (pca4ds)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="validation-stability-and-significance" class="section level2">
<h2><span class="header-section-number">3.3</span> Validation: stability and significance</h2>
<p>What is the part in PCA results that is not really accounted by the structure of the data, but by the randomness in the data? Are the results stables and reproducibles? Do the configuration of points change based on the studied data? All of these questions make it necessary to assess the stability of the obtained results.</p>
<p>The stability of the results will depend on the randomness of the data collection process (e.g. random samples, sampling surveys), as well as on the measurement errors in the variables.</p>
<div id="how-many-axes-to-study-and-retain" class="section level3">
<h3><span class="header-section-number">3.3.1</span> How many axes to study and retain?</h3>
<p>Are the directions of the first axes well defined and stable? More precisely, are the dispersions in consecutive directions really different? If not, we would have to consider that the factorial plane formed by them is stable but the associated axes are not really different (i.e. indeterminate by a rotation).</p>
<p>One way to answer these questions is to suppose that the data come from a sample drawn from a population with a normal distribution. In this case, the eigenvalues asymptotically follow a normal distribution (Anderson, 1963). Then, we can estimate a 95% confidence interval for each eigenvalue with the formula <a href="validation-stability-and-significance.html#eq:31">(3.1)</a></p>
<p><span class="math display" id="eq:31">\[
\left [ \lambda_{\alpha} \left (1 - 1.96 \sqrt{2/(n-1)} \right ); \hspace{1mm} \lambda_{\alpha} \left (1+1.96\sqrt{2/(n-1)} \right) \right ]
\tag{3.1}
\]</span></p>
<p>The width of this interval gives us an idea of the stability of the eigenvalue with respect to the sample randomness. The overlapping in the intervals of two consecutive eigenvalues suggests that these eigenvalues are equal (or very similar). The corresponding axes are thus indeterminate by one rotation. Under this situation, the analyst should focus on the interpretation of the subspace defined by the first eigenvalues that are well separated.</p>
<p>Although this result has to do with eigenvalues of covariance matrices, it can also be applied to the eigenvalues of correlation matrices. Simulation studies show that the confidence intervals tend to be “cautious”: the coverage percentage of the true eigenvalue, is almost always greater than the anounced confidence level. In any case, the asymptotic nature of the results, and the underlying hypothesis of normality, lead us to consider the results are merely indicative (not a hard rule).</p>
<p>In regards of the factorial axes, it is convenient to distinguish the axes that will be studied, from the axes that will be used. The factorial axes can be seen as an ultimate result, or also as an intermediate stage for further studies.</p>
<p>For example, a PCA can be a preliminary stage before performing a discriminant analysis. In this case, we will try to use the axes with discriminant power, which may not coincide with the axes of largest spread.</p>
<p>If the goal is to classify individuals, it makes sense to retain only the axes expressing real directions of spread, in order to preserve the stable characteristics of the individuals, while excluding those directions that are mainly capturing random noise.</p>
<div id="scree-test-cattells-rule-1966" class="section level4 unnumbered">
<h4>Scree Test (Cattell’s rule, 1966)</h4>
<p>One of the most prevalent questions in PCA is “how many principal components (or factorial axes) to retain?” Unfortunately, there is no simple answer to this question.</p>
<p>If we assume that the <span class="math inline">\(n\)</span> values taken by the <span class="math inline">\(p\)</span> variables come from a random process that uniformly fills up the space, without privileging any direction, then the <span class="math inline">\(p\)</span> eigenvalues of the PCA will slowly decrease in a regular form.</p>
<p>If a PCA provides a histogram of the eigenvalues showing one or more staircase steps, we can think that there are sufficiently strong associations between the variables. These associations would be responsible for the appearance of directions or subspaces where most of the dispersion is concentrated.</p>
<p>Such pragmatic considerations, can be used to determine—in a more or less subjective way—a minimum and a maximum number of axes to retain in the analysis. The main way to do this is through visual inspection of the histogram of eigenvalues following the so-called <em>scree test</em> or <em>elbow criteria</em> proposed by Raymond Cattell (1966). This criteria, which is the simplest and oldest one, involves graphing a line plot of the eigenvalues, ordered from largest to smallest, and then look for the “elbow” of the graph where the eigenvalues seem to level off.</p>
<p>In the example of the cities (first PCA), we obtained the following eigenvalues:</p>
<table>
<caption><span id="tab:table-3-1">Table 3.1: </span>Distribution of eigenvalues in 1st PCA.</caption>
<thead>
<tr class="header">
<th align="right">num</th>
<th align="right">eigenvalues</th>
<th align="right">percentage</th>
<th align="right">cumulative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">10.1390</td>
<td align="right">84.49</td>
<td align="right">84.49</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.8612</td>
<td align="right">7.18</td>
<td align="right">91.67</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.3248</td>
<td align="right">2.71</td>
<td align="right">94.37</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.1715</td>
<td align="right">1.43</td>
<td align="right">95.80</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.1484</td>
<td align="right">1.24</td>
<td align="right">97.04</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.0973</td>
<td align="right">0.81</td>
<td align="right">97.85</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.0682</td>
<td align="right">0.57</td>
<td align="right">98.42</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">0.0525</td>
<td align="right">0.44</td>
<td align="right">98.86</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">0.0505</td>
<td align="right">0.42</td>
<td align="right">99.28</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">0.0332</td>
<td align="right">0.28</td>
<td align="right">99.55</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">0.0309</td>
<td align="right">0.26</td>
<td align="right">99.81</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">0.0226</td>
<td align="right">0.19</td>
<td align="right">100.00</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>We can then plot a histogram of the eigenvales, and add a line connecting the heights of the bars to better see the way in which the sizes of the eigenvalues decrease:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the second PCA of the salaries divided by the mean salary of a city, we obtained the following eigenvalues:</p>
<table>
<caption><span id="tab:table-3-2">Table 3.2: </span>Distribution of eigenvalues in 2nd PCA.</caption>
<thead>
<tr class="header">
<th align="right">num</th>
<th align="right">eigenvalues</th>
<th align="right">percentage</th>
<th align="right">cumulative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">4.4910</td>
<td align="right">37.43</td>
<td align="right">37.43</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1.7148</td>
<td align="right">14.29</td>
<td align="right">51.72</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1.2989</td>
<td align="right">10.82</td>
<td align="right">62.54</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1.0396</td>
<td align="right">8.66</td>
<td align="right">71.20</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.8699</td>
<td align="right">7.25</td>
<td align="right">78.45</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.7831</td>
<td align="right">6.53</td>
<td align="right">84.98</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.5309</td>
<td align="right">4.42</td>
<td align="right">89.40</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">0.3874</td>
<td align="right">3.23</td>
<td align="right">92.63</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">0.3210</td>
<td align="right">2.67</td>
<td align="right">95.31</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">0.2561</td>
<td align="right">2.13</td>
<td align="right">97.44</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">0.2021</td>
<td align="right">1.68</td>
<td align="right">99.12</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">0.1052</td>
<td align="right">0.88</td>
<td align="right">100.00</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Graphing the scree plot we obtain the following display:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>More formally, Cattell’s criteria consist of sorting the lagged differences of second order between eigenvalues, as follows:</p>
<p><span class="math display" id="eq:32">\[\begin{equation}
d(\alpha) = (\lambda_{\alpha + 1} - \lambda_{\alpha}) - (\lambda_{\alpha} - \lambda_{\alpha - 1})
\tag{3.2}
\end{equation}\]</span></p>
<p>The reason why is called <em>scree test</em> has to do with the metaphor of a mountain scree. According to <a href="https://en.wikipedia.org/wiki/Scree">wikipedia</a>, a “scree is a collection of broken rock fragments at the base of crags, mountain cliffs, volcanoes or valley shoulders that has accumulated through periodic rockfall from adjacent cliff faces.”</p>
<p><img src="images/scree-mountain.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="note-1" class="section level4 unnumbered">
<h4>Note</h4>
<p>We have seen that when there is a <em>size effect</em> in the first axis, the subsequent eigenvalues are affected and reduced. However, it is possible that subsequent eigenvalues reflect structural oppositions. This is the case of the second PCA on raw data, which corresponds approximately, to the first axis of the analysis on the ratio data, when the size effect is eliminated.</p>
<p>On the other hands, it is risky to interpret the percentage of inertia as a measure of the information contained in an axis. This percentage can be made as small as possible, just by adding independent random variables to the data of active variables. The overall inertia will increase, while the “information” contained in the first axes will remain the same and, consequently, the percentage of inertia in each axis will decrease.</p>
<table>
<caption><span id="tab:table-3-3">Table 3.3: </span>Distribution of eigenvalues from data with random perturbations.</caption>
<thead>
<tr class="header">
<th align="right">num</th>
<th align="right">eigenvalue</th>
<th align="right">percentage</th>
<th align="right">cumulative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1.7994</td>
<td align="right">15.00</td>
<td align="right">15.00</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1.5473</td>
<td align="right">12.89</td>
<td align="right">27.89</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1.4034</td>
<td align="right">11.69</td>
<td align="right">39.58</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1.2329</td>
<td align="right">10.27</td>
<td align="right">49.86</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">1.1123</td>
<td align="right">9.27</td>
<td align="right">59.13</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">1.0635</td>
<td align="right">8.86</td>
<td align="right">67.99</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.8877</td>
<td align="right">7.40</td>
<td align="right">75.39</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">0.7653</td>
<td align="right">6.38</td>
<td align="right">81.76</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">0.7059</td>
<td align="right">5.88</td>
<td align="right">87.65</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">0.6000</td>
<td align="right">5.00</td>
<td align="right">92.65</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">0.5414</td>
<td align="right">4.51</td>
<td align="right">97.16</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">0.3410</td>
<td align="right">2.84</td>
<td align="right">100.00</td>
</tr>
</tbody>
</table>
<p><img src="_main_files/figure-html/unnamed-chunk-14-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="simulations-random-effects-on-individuals" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Simulations, random effects on individuals</h3>
<p>One way to assess the stability of results involves using the available information in the data, via computational methods to run some simulations. By following this type of approaches, we are able to free ourselves from the probabilistic assupmtions about the data, which are seldom met when dealing with multivariate data.</p>
<p>The strategy that we use is based on random perturbations of the data, in order to simulate a certain natural variability or measurement error in the observations.</p>
<p>Each observation in the data matrix is replaced by the observed value plus a random quantity that follows a normal distribution with mean and variance depending on the variable under modification.</p>
<p>We denote this change of value as:</p>
<p><span class="math display" id="eq:33">\[
x_{ij} = x_{ij} + N(x_{ij}, Ks_j)
\tag{3.3}
\]</span></p>
<p>The observed value <span class="math inline">\(x_{ij}\)</span> is modified by adding a random quantity that follows a normal distribution, centered at <span class="math inline">\(x_{ij}\)</span>, and with standard deviation <span class="math inline">\(K\)</span> times the standard deviation <span class="math inline">\(s_j\)</span> of variable <span class="math inline">\(j\)</span>.</p>
<p>The value of the constant <span class="math inline">\(K\)</span> determines the amount of perturbation that we introduce in the data. <span class="math inline">\(K=0\)</span> indicates that the observations remain unchanged. A value of <span class="math inline">\(K=1\)</span> means that each observation is affected, on average, one standard deviation.</p>
<p>Once we have modified the data table, we can perform a PCA, calculate its directions, the correlation of the extracted directions with the original-unmodified variables, obtaining a matrix of correlations between axis systems.</p>
<p>In this matrix we will inspect, for each original axis, what other modified axes are most correlated with. We will also check if an axis is correlated with all other axes in analogous way. In the former case, this indicates that an axis is stable, despite the random modifications in the data. In the latter case, this indicates that an axis is the result of randomness in the data.</p>
<p>By looking at the matrix of correlations described in the previous paragraph, we can detect up to what extent the rank of the axes are stables, and from what point the “natural” random fluctuations in data begin.</p>
<p>In the example of the international cities, we show in table (TABLE 3.4) the correlation matrices between the axes (in rows) obtained in the analysis of ratios (salaries of professions with respect to the mean salary of the city) and the axes obtained with a random perturbation of 1%, 5% and 10% of the standard deviation of each variable.</p>
<table>
<caption><span id="tab:table-3-4a">Table 3.4: </span>Assessment of random perturbations (rows correspond to axes of 2nd PCA).</caption>
<thead>
<tr class="header">
<th align="left">Variables (perturbation 1%)</th>
<th align="right">F1</th>
<th align="right">F2</th>
<th align="right">F3</th>
<th align="right">F4</th>
<th align="right">F5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Factorial axis 1 (2nd PCA)</td>
<td align="right">1.00</td>
<td align="right">0.01</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="left">Factorial axis 2 (2nd PCA)</td>
<td align="right">-0.01</td>
<td align="right">0.99</td>
<td align="right">-0.05</td>
<td align="right">0.12</td>
<td align="right">-0.06</td>
</tr>
<tr class="odd">
<td align="left">Factorial axis 3 (2nd PCA)</td>
<td align="right">0.00</td>
<td align="right">0.04</td>
<td align="right">0.99</td>
<td align="right">0.04</td>
<td align="right">-0.04</td>
</tr>
<tr class="even">
<td align="left">Factorial axis 4 (2nd PCA)</td>
<td align="right">0.00</td>
<td align="right">-0.08</td>
<td align="right">-0.02</td>
<td align="right">0.91</td>
<td align="right">0.37</td>
</tr>
<tr class="odd">
<td align="left">Factorial axis 5 (2nd PCA)</td>
<td align="right">0.00</td>
<td align="right">-0.10</td>
<td align="right">-0.06</td>
<td align="right">0.35</td>
<td align="right">-0.89</td>
</tr>
</tbody>
</table>
<p><strong>Perturbation of 5%</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">Variables (perturbation 5%)</th>
<th align="right">F1</th>
<th align="right">F2</th>
<th align="right">F3</th>
<th align="right">F4</th>
<th align="right">F5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Factorial axis 1 (2nd PCA)</td>
<td align="right">0.99</td>
<td align="right">0.08</td>
<td align="right">-0.03</td>
<td align="right">0.01</td>
<td align="right">-0.03</td>
</tr>
<tr class="even">
<td align="left">Factorial axis 2 (2nd PCA)</td>
<td align="right">-0.08</td>
<td align="right">0.87</td>
<td align="right">-0.06</td>
<td align="right">0.20</td>
<td align="right">-0.20</td>
</tr>
<tr class="odd">
<td align="left">Factorial axis 3 (2nd PCA)</td>
<td align="right">0.01</td>
<td align="right">0.13</td>
<td align="right">0.85</td>
<td align="right">0.24</td>
<td align="right">0.34</td>
</tr>
<tr class="even">
<td align="left">Factorial axis 4 (2nd PCA)</td>
<td align="right">0.01</td>
<td align="right">0.06</td>
<td align="right">0.36</td>
<td align="right">-0.57</td>
<td align="right">-0.52</td>
</tr>
<tr class="odd">
<td align="left">Factorial axis 5 (2nd PCA)</td>
<td align="right">0.03</td>
<td align="right">0.07</td>
<td align="right">0.07</td>
<td align="right">-0.48</td>
<td align="right">0.47</td>
</tr>
</tbody>
</table>
<p><strong>Perturbation of 10%</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">Variables (perturbation 10%)</th>
<th align="right">F1</th>
<th align="right">F2</th>
<th align="right">F3</th>
<th align="right">F4</th>
<th align="right">F5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Factorial axis 1 (2nd PCA)</td>
<td align="right">-0.64</td>
<td align="right">0.72</td>
<td align="right">-0.11</td>
<td align="right">-0.05</td>
<td align="right">0.01</td>
</tr>
<tr class="even">
<td align="left">Factorial axis 2 (2nd PCA)</td>
<td align="right">-0.39</td>
<td align="right">-0.31</td>
<td align="right">-0.37</td>
<td align="right">0.09</td>
<td align="right">-0.32</td>
</tr>
<tr class="odd">
<td align="left">Factorial axis 3 (2nd PCA)</td>
<td align="right">-0.07</td>
<td align="right">0.01</td>
<td align="right">0.24</td>
<td align="right">0.32</td>
<td align="right">0.68</td>
</tr>
<tr class="even">
<td align="left">Factorial axis 4 (2nd PCA)</td>
<td align="right">0.11</td>
<td align="right">-0.01</td>
<td align="right">-0.54</td>
<td align="right">-0.14</td>
<td align="right">0.24</td>
</tr>
<tr class="odd">
<td align="left">Factorial axis 5 (2nd PCA)</td>
<td align="right">0.14</td>
<td align="right">0.30</td>
<td align="right">0.15</td>
<td align="right">0.04</td>
<td align="right">-0.29</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>By looking at the diagonal of the tables, we observe stability in the first factor, as well as in the second and the third factors, up to a random perturbation of 5% of the original standard deviation. With a high perturbation (of 10%) only the first factor is resistant to the modifications.</p>
<p>Table <a href="validation-stability-and-significance.html#tab:table-3-5">3.5</a> displays the mean and standard deviation of the salary variables (gross salary divided by the city-mean salary), as well as the correlation between the randomly modified variable and the original variable. We can tell that with a random perturbation of 10% the standard deviations increase, while the correlations decrease.</p>
<table>
<caption><span id="tab:table-3-5">Table 3.5: </span>Summary Statistics of Active Variables affected by random perturbations.</caption>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Summary Statistics</th>
<th align="left">Original</th>
<th align="right">P1%</th>
<th align="right">P5%</th>
<th align="right">P10%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">teacher</td>
<td align="left">mean</td>
<td align="left">1.19</td>
<td align="right">1.18</td>
<td align="right">1.18</td>
<td align="right">1.17</td>
</tr>
<tr class="even">
<td align="left">teacher</td>
<td align="left">std deviation</td>
<td align="left">0.37</td>
<td align="right">0.38</td>
<td align="right">0.44</td>
<td align="right">0.54</td>
</tr>
<tr class="odd">
<td align="left">teacher</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.94</td>
<td align="right">0.71</td>
</tr>
<tr class="even">
<td align="left">bus_driver</td>
<td align="left">mean</td>
<td align="left">1.04</td>
<td align="right">1.04</td>
<td align="right">1.03</td>
<td align="right">1.05</td>
</tr>
<tr class="odd">
<td align="left">bus_driver</td>
<td align="left">std deviation</td>
<td align="left">0.25</td>
<td align="right">0.26</td>
<td align="right">0.27</td>
<td align="right">0.35</td>
</tr>
<tr class="even">
<td align="left">bus_driver</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">1.00</td>
<td align="right">0.96</td>
<td align="right">0.60</td>
</tr>
<tr class="odd">
<td align="left">mechanic</td>
<td align="left">mean</td>
<td align="left">0.96</td>
<td align="right">0.96</td>
<td align="right">0.96</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td align="left">mechanic</td>
<td align="left">std deviation</td>
<td align="left">0.24</td>
<td align="right">0.24</td>
<td align="right">0.27</td>
<td align="right">0.52</td>
</tr>
<tr class="odd">
<td align="left">mechanic</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.89</td>
<td align="right">0.46</td>
</tr>
<tr class="even">
<td align="left">construction_worker</td>
<td align="left">mean</td>
<td align="left">0.72</td>
<td align="right">0.73</td>
<td align="right">0.73</td>
<td align="right">0.72</td>
</tr>
<tr class="odd">
<td align="left">construction_worker</td>
<td align="left">std deviation</td>
<td align="left">0.27</td>
<td align="right">0.26</td>
<td align="right">0.26</td>
<td align="right">0.30</td>
</tr>
<tr class="even">
<td align="left">construction_worker</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.94</td>
<td align="right">0.71</td>
</tr>
<tr class="odd">
<td align="left">metal_worker</td>
<td align="left">mean</td>
<td align="left">1.17</td>
<td align="right">1.16</td>
<td align="right">1.16</td>
<td align="right">1.16</td>
</tr>
<tr class="even">
<td align="left">metal_worker</td>
<td align="left">std deviation</td>
<td align="left">0.22</td>
<td align="right">0.23</td>
<td align="right">0.26</td>
<td align="right">0.30</td>
</tr>
<tr class="odd">
<td align="left">metal_worker</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.86</td>
<td align="right">0.79</td>
</tr>
<tr class="even">
<td align="left">cook_chef</td>
<td align="left">mean</td>
<td align="left">1.40</td>
<td align="right">1.40</td>
<td align="right">1.40</td>
<td align="right">1.38</td>
</tr>
<tr class="odd">
<td align="left">cook_chef</td>
<td align="left">std deviation</td>
<td align="left">0.61</td>
<td align="right">0.63</td>
<td align="right">0.61</td>
<td align="right">0.58</td>
</tr>
<tr class="even">
<td align="left">cook_chef</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">1.00</td>
<td align="right">0.99</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="left">departmental_head</td>
<td align="left">mean</td>
<td align="left">2.63</td>
<td align="right">2.62</td>
<td align="right">2.61</td>
<td align="right">2.53</td>
</tr>
<tr class="even">
<td align="left">departmental_head</td>
<td align="left">std deviation</td>
<td align="left">1.31</td>
<td align="right">1.31</td>
<td align="right">1.34</td>
<td align="right">1.44</td>
</tr>
<tr class="odd">
<td align="left">departmental_head</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">1.00</td>
<td align="right">0.98</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">engineer</td>
<td align="left">mean</td>
<td align="left">2.12</td>
<td align="right">2.12</td>
<td align="right">2.07</td>
<td align="right">2.04</td>
</tr>
<tr class="odd">
<td align="left">engineer</td>
<td align="left">std deviation</td>
<td align="left">0.75</td>
<td align="right">0.76</td>
<td align="right">0.77</td>
<td align="right">0.84</td>
</tr>
<tr class="even">
<td align="left">engineer</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">1.00</td>
<td align="right">0.96</td>
<td align="right">0.90</td>
</tr>
<tr class="odd">
<td align="left">bank_clerk</td>
<td align="left">mean</td>
<td align="left">1.51</td>
<td align="right">1.52</td>
<td align="right">1.48</td>
<td align="right">1.48</td>
</tr>
<tr class="even">
<td align="left">bank_clerk</td>
<td align="left">std deviation</td>
<td align="left">0.61</td>
<td align="right">0.61</td>
<td align="right">0.63</td>
<td align="right">0.71</td>
</tr>
<tr class="odd">
<td align="left">bank_clerk</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">1.00</td>
<td align="right">0.98</td>
<td align="right">0.90</td>
</tr>
<tr class="even">
<td align="left">executive_secretary</td>
<td align="left">mean</td>
<td align="left">1.13</td>
<td align="right">1.13</td>
<td align="right">1.13</td>
<td align="right">1.14</td>
</tr>
<tr class="odd">
<td align="left">executive_secretary</td>
<td align="left">std deviation</td>
<td align="left">0.28</td>
<td align="right">0.27</td>
<td align="right">0.27</td>
<td align="right">0.35</td>
</tr>
<tr class="even">
<td align="left">executive_secretary</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.98</td>
<td align="right">0.96</td>
</tr>
<tr class="odd">
<td align="left">salesperson</td>
<td align="left">mean</td>
<td align="left">0.76</td>
<td align="right">0.76</td>
<td align="right">0.75</td>
<td align="right">0.78</td>
</tr>
<tr class="even">
<td align="left">salesperson</td>
<td align="left">std deviation</td>
<td align="left">0.16</td>
<td align="right">0.16</td>
<td align="right">0.16</td>
<td align="right">0.18</td>
</tr>
<tr class="odd">
<td align="left">salesperson</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.97</td>
<td align="right">0.52</td>
</tr>
<tr class="even">
<td align="left">textile_worker</td>
<td align="left">mean</td>
<td align="left">0.68</td>
<td align="right">0.68</td>
<td align="right">0.70</td>
<td align="right">0.66</td>
</tr>
<tr class="odd">
<td align="left">textile_worker</td>
<td align="left">std deviation</td>
<td align="left">0.17</td>
<td align="right">0.18</td>
<td align="right">0.20</td>
<td align="right">0.27</td>
</tr>
<tr class="even">
<td align="left">textile_worker</td>
<td align="left">correlation</td>
<td align="left">-</td>
<td align="right">0.99</td>
<td align="right">0.87</td>
<td align="right">0.72</td>
</tr>
</tbody>
</table>
</div>
<div id="bootstrap-simulations" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Bootstrap Simulations</h3>
<p>Another way of empirical assessment can be done using random resampling methods on the data. The idea is to obtain a number of data tables, all of the same dimension as the original one, by randomly sampling with replacement the observations in the data. This approach is the so-called Bootstrap method (Efron et al, 1993). Following this approach, it is possible to estimate the sampling errors and the distribution of the various PCA results.</p>
<p>How to implement the bootstrap method? First, we form a large number of samples of <span class="math inline">\(n\)</span> individuals which are drawn with replacement from the <span class="math inline">\(n\)</span> original individuals in the data. This set of samples is referred to as the <em>bootstrap samples</em>. For each bootstrap sample, some of the original individuals won’t be part of the sample, while some individuals may appear more than once in the sample. Each bootstrap sample gives place to a data table.</p>
<p>On each of the bootstrap tables we calculate its eigenvalues and eigenvectors. We then obtain a bootstrap distribution of the eigenvalues, as well as the bootstrap distribution of the correlations between the eigenvectors and the original axes.</p>
<p>For each eigenvalue we can obtain a confidence interval. Likewise, for each eigenvector we can obtain a confidence cone around the original eigenvector. Examining the correlation between the axes can then reveal potential rotations among axes.</p>
<p>The bootstrap simulations can also be used to assess the stability of the projections of the variables and the categories. We can position the different bootstrap tables as supplementary information in the analysis of the original table (Lebart et al, 1995). In this way, it is possible to visualize in the factorial planes regions of “natural” fluctuation of the different elements in the data table.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conditions-of-application.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="analysis-of-table-of-ranks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
