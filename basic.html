<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Basic Elements | Principal Component Analysis for Data Science (pca4ds)</title>
  <meta name="description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Basic Elements | Principal Component Analysis for Data Science (pca4ds)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  <meta name="github-repo" content="gastonstat/pca4ds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Basic Elements | Principal Component Analysis for Data Science (pca4ds)" />
  
  <meta name="twitter:description" content="This book will teach you what is Principal Component Analysis and how you can use it for a variety of data analysis purposes: description, exploration, visualization, pre-modeling, dimension reduction, and data compression." />
  

<meta name="author" content="Tomas Aluja-Banet Alain Morineau Gaston Sanchez" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="terminology.html"/>
<link rel="next" href="mechanics.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>PCA for Data Science</b><br><small>T. Aluja, A. Morineau, G. Sanchez</small></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i>Terminology</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>1</b> Basic Elements</a><ul>
<li class="chapter" data-level="1.1" data-path="basic.html"><a href="basic.html#data-and-goals"><i class="fa fa-check"></i><b>1.1</b> Data and Goals</a><ul>
<li class="chapter" data-level="1.1.1" data-path="basic.html"><a href="basic.html#active-variables"><i class="fa fa-check"></i><b>1.1.1</b> Active Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic.html"><a href="basic.html#analysis-of-distances"><i class="fa fa-check"></i><b>1.2</b> Analysis of Distances</a><ul>
<li class="chapter" data-level="1.2.1" data-path="basic.html"><a href="basic.html#cloud-of-row-points"><i class="fa fa-check"></i><b>1.2.1</b> Cloud of Row-Points</a></li>
<li class="chapter" data-level="1.2.2" data-path="basic.html"><a href="basic.html#cloud-of-column-points"><i class="fa fa-check"></i><b>1.2.2</b> Cloud of Column-Points</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="basic.html"><a href="basic.html#how-to-see-the-distances-between-points"><i class="fa fa-check"></i><b>1.3</b> How to see the distances between points</a><ul>
<li class="chapter" data-level="1.3.1" data-path="basic.html"><a href="basic.html#how-to-find-the-projection-planes"><i class="fa fa-check"></i><b>1.3.1</b> How to find the projection planes</a></li>
<li class="chapter" data-level="1.3.2" data-path="basic.html"><a href="basic.html#how-to-take-into-account-the-importance-of-individuals"><i class="fa fa-check"></i><b>1.3.2</b> How to take into account the importance of individuals</a></li>
<li class="chapter" data-level="1.3.3" data-path="basic.html"><a href="basic.html#inertia-decomposition"><i class="fa fa-check"></i><b>1.3.3</b> Inertia Decomposition</a></li>
<li class="chapter" data-level="1.3.4" data-path="basic.html"><a href="basic.html#visualizing-association-between-variables."><i class="fa fa-check"></i><b>1.3.4</b> Visualizing association between variables.</a></li>
<li class="chapter" data-level="1.3.5" data-path="basic.html"><a href="basic.html#normalized-pca-or-non-normalized-pca"><i class="fa fa-check"></i><b>1.3.5</b> Normalized PCA or non-normalized PCA?</a></li>
<li class="chapter" data-level="1.3.6" data-path="basic.html"><a href="basic.html#distance-matrices"><i class="fa fa-check"></i><b>1.3.6</b> Distance Matrices</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Mechanics</b></span></li>
<li class="chapter" data-level="2" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>2</b> How Does PCA Work?</a><ul>
<li class="chapter" data-level="2.1" data-path="mechanics.html"><a href="mechanics.html#principal-components"><i class="fa fa-check"></i><b>2.1</b> Principal Components</a><ul>
<li class="chapter" data-level="2.1.1" data-path="mechanics.html"><a href="mechanics.html#interpreting-the-inertia-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Interpreting the Inertia Proportions</a></li>
<li class="chapter" data-level="2.1.2" data-path="mechanics.html"><a href="mechanics.html#how-many-axes-to-retain"><i class="fa fa-check"></i><b>2.1.2</b> How many axes to retain?</a></li>
<li class="chapter" data-level="2.1.3" data-path="mechanics.html"><a href="mechanics.html#coordinates-of-row-points"><i class="fa fa-check"></i><b>2.1.3</b> Coordinates of row-points</a></li>
<li class="chapter" data-level="2.1.4" data-path="mechanics.html"><a href="mechanics.html#interpretation-tools"><i class="fa fa-check"></i><b>2.1.4</b> Interpretation Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="mechanics.html"><a href="mechanics.html#projections-of-variables"><i class="fa fa-check"></i><b>2.2</b> Projections of Variables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="mechanics.html"><a href="mechanics.html#size-effect"><i class="fa fa-check"></i><b>2.2.1</b> Size Effect</a></li>
<li class="chapter" data-level="2.2.2" data-path="mechanics.html"><a href="mechanics.html#tools-for-interpreting-components"><i class="fa fa-check"></i><b>2.2.2</b> Tools for Interpreting Components</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="mechanics.html"><a href="mechanics.html#size-factor"><i class="fa fa-check"></i><b>2.3</b> Beyond the First Factor</a></li>
<li class="chapter" data-level="2.4" data-path="mechanics.html"><a href="mechanics.html#using-supplementary-elements"><i class="fa fa-check"></i><b>2.4</b> Using Supplementary Elements</a><ul>
<li class="chapter" data-level="2.4.1" data-path="mechanics.html"><a href="mechanics.html#continuous-supplementary-variables"><i class="fa fa-check"></i><b>2.4.1</b> Continuous Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="mechanics.html"><a href="mechanics.html#nominal-supplementary-variables"><i class="fa fa-check"></i><b>2.4.2</b> Nominal Supplementary Variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="mechanics.html"><a href="mechanics.html#profiling-with-v-test"><i class="fa fa-check"></i><b>2.4.3</b> Profiling with V-test</a></li>
<li class="chapter" data-level="2.4.4" data-path="mechanics.html"><a href="mechanics.html#axes-characterization-using-continuous-variables"><i class="fa fa-check"></i><b>2.4.4</b> Axes Characterization using Continuous Variables</a></li>
<li class="chapter" data-level="2.4.5" data-path="mechanics.html"><a href="mechanics.html#v-test-and-data-science"><i class="fa fa-check"></i><b>2.4.5</b> V-test and Data Science</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="mechanics.html"><a href="mechanics.html#simultaneous-representations"><i class="fa fa-check"></i><b>2.5</b> Simultaneous Representations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="mechanics.html"><a href="mechanics.html#old-unit-axes"><i class="fa fa-check"></i><b>2.5.1</b> Old Unit Axes</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Practice</b></span></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#themescope"><i class="fa fa-check"></i><b>3.1</b> Themescope</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#conditions-of-application"><i class="fa fa-check"></i><b>3.2</b> Conditions of Application</a><ul>
<li class="chapter" data-level="3.2.1" data-path="analysis.html"><a href="analysis.html#linearity-and-symmetry"><i class="fa fa-check"></i><b>3.2.1</b> Linearity and Symmetry</a></li>
<li class="chapter" data-level="3.2.2" data-path="analysis.html"><a href="analysis.html#balancing-the-content-of-active-variables"><i class="fa fa-check"></i><b>3.2.2</b> Balancing the content of active variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#validation-stability-and-significance"><i class="fa fa-check"></i><b>3.3</b> Validation: stability and significance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#how-many-axes-to-study-and-retain"><i class="fa fa-check"></i><b>3.3.1</b> How many axes to study and retain?</a></li>
<li class="chapter" data-level="3.3.2" data-path="analysis.html"><a href="analysis.html#simulations-random-effects-on-individuals"><i class="fa fa-check"></i><b>3.3.2</b> Simulations, random effects on individuals</a></li>
<li class="chapter" data-level="3.3.3" data-path="analysis.html"><a href="analysis.html#bootstrap-simulations"><i class="fa fa-check"></i><b>3.3.3</b> Bootstrap Simulations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#analysis-of-table-of-ranks"><i class="fa fa-check"></i><b>3.4</b> Analysis of Table of Ranks</a></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#optimal-reconstitution-of-data"><i class="fa fa-check"></i><b>3.5</b> Optimal Reconstitution of Data</a></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#synthetic-variables-and-indices"><i class="fa fa-check"></i><b>3.6</b> Synthetic Variables and Indices</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#handling-missing-values"><i class="fa fa-check"></i><b>3.7</b> Handling Missing Values</a></li>
<li class="chapter" data-level="3.8" data-path="analysis.html"><a href="analysis.html#pca-and-clustering"><i class="fa fa-check"></i><b>3.8</b> PCA and Clustering</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis.html"><a href="analysis.html#real-groups-or-instrumental-groups"><i class="fa fa-check"></i><b>3.8.1</b> Real Groups or Instrumental Groups?</a></li>
<li class="chapter" data-level="3.8.2" data-path="analysis.html"><a href="analysis.html#representants-of-groups"><i class="fa fa-check"></i><b>3.8.2</b> Representants of Groups</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis.html"><a href="analysis.html#data-weighing"><i class="fa fa-check"></i><b>3.9</b> Data Weighing</a></li>
<li class="chapter" data-level="3.10" data-path="analysis.html"><a href="analysis.html#pca-as-an-intermediate-analytical-stage"><i class="fa fa-check"></i><b>3.10</b> PCA as an Intermediate Analytical Stage</a></li>
<li class="chapter" data-level="3.11" data-path="analysis.html"><a href="analysis.html#comparing-various-tables"><i class="fa fa-check"></i><b>3.11</b> Comparing Various Tables</a></li>
<li class="chapter" data-level="3.12" data-path="analysis.html"><a href="analysis.html#analysis-of-a-table-of-means"><i class="fa fa-check"></i><b>3.12</b> Analysis of a Table of Means</a></li>
<li class="chapter" data-level="3.13" data-path="analysis.html"><a href="analysis.html#analysis-of-a-binary-table"><i class="fa fa-check"></i><b>3.13</b> Analysis of a Binary Table</a></li>
<li class="chapter" data-level="3.14" data-path="analysis.html"><a href="analysis.html#analysis-of-a-table-of-distances"><i class="fa fa-check"></i><b>3.14</b> Analysis of a Table of Distances</a></li>
<li class="chapter" data-level="3.15" data-path="analysis.html"><a href="analysis.html#conditional-pca"><i class="fa fa-check"></i><b>3.15</b> Conditional PCA</a><ul>
<li class="chapter" data-level="3.15.1" data-path="analysis.html"><a href="analysis.html#pca-on-model-residuals"><i class="fa fa-check"></i><b>3.15.1</b> PCA on Model Residuals</a></li>
<li class="chapter" data-level="3.15.2" data-path="analysis.html"><a href="analysis.html#analysis-of-local-variation"><i class="fa fa-check"></i><b>3.15.2</b> Analysis of Local Variation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Examples</b></span></li>
<li class="chapter" data-level="4" data-path="application-examples.html"><a href="application-examples.html"><i class="fa fa-check"></i><b>4</b> Application Examples</a><ul>
<li class="chapter" data-level="4.1" data-path="application-examples.html"><a href="application-examples.html#lascaux"><i class="fa fa-check"></i><b>4.1</b> Lascaux Cave Temperatures</a><ul>
<li class="chapter" data-level="4.1.1" data-path="application-examples.html"><a href="application-examples.html#temperature-data"><i class="fa fa-check"></i><b>4.1.1</b> Temperature Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="application-examples.html"><a href="application-examples.html#pca"><i class="fa fa-check"></i><b>4.1.2</b> PCA</a></li>
<li class="chapter" data-level="4.1.3" data-path="application-examples.html"><a href="application-examples.html#seasonal-phenomenon"><i class="fa fa-check"></i><b>4.1.3</b> Seasonal Phenomenon</a></li>
<li class="chapter" data-level="4.1.4" data-path="application-examples.html"><a href="application-examples.html#modeling-propagation-of-thermal-wave"><i class="fa fa-check"></i><b>4.1.4</b> Modeling Propagation of Thermal Wave</a></li>
<li class="chapter" data-level="4.1.5" data-path="application-examples.html"><a href="application-examples.html#stability-of-the-axes"><i class="fa fa-check"></i><b>4.1.5</b> Stability of the Axes</a></li>
<li class="chapter" data-level="4.1.6" data-path="application-examples.html"><a href="application-examples.html#selecting-best-temperature-reading-locations"><i class="fa fa-check"></i><b>4.1.6</b> Selecting Best Temperature Reading Locations</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="application-examples.html"><a href="application-examples.html#design-of-experiments-and-pca"><i class="fa fa-check"></i><b>4.2</b> Design of Experiments and PCA</a><ul>
<li class="chapter" data-level="4.2.1" data-path="application-examples.html"><a href="application-examples.html#pca-1"><i class="fa fa-check"></i><b>4.2.1</b> PCA</a></li>
<li class="chapter" data-level="4.2.2" data-path="application-examples.html"><a href="application-examples.html#evolution-of-factor-trajectories-over-time"><i class="fa fa-check"></i><b>4.2.2</b> Evolution of Factor Trajectories over Time</a></li>
<li class="chapter" data-level="4.2.3" data-path="application-examples.html"><a href="application-examples.html#analysis-of-variance"><i class="fa fa-check"></i><b>4.2.3</b> Analysis of Variance</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="application-examples.html"><a href="application-examples.html#defining-an-economic-capacity-index"><i class="fa fa-check"></i><b>4.3</b> Defining an Economic Capacity Index</a><ul>
<li class="chapter" data-level="4.3.1" data-path="application-examples.html"><a href="application-examples.html#analyzed-information"><i class="fa fa-check"></i><b>4.3.1</b> Analyzed Information</a></li>
<li class="chapter" data-level="4.3.2" data-path="application-examples.html"><a href="application-examples.html#pca-2"><i class="fa fa-check"></i><b>4.3.2</b> PCA</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="5" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>5</b> Appendix A: Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="appendixa.html"><a href="appendixa.html#space-of-p-dimensions"><i class="fa fa-check"></i><b>5.1</b> Space of p-Dimensions</a></li>
<li class="chapter" data-level="5.2" data-path="appendixa.html"><a href="appendixa.html#distances-between-points"><i class="fa fa-check"></i><b>5.2</b> Distances between points</a></li>
<li class="chapter" data-level="5.3" data-path="appendixa.html"><a href="appendixa.html#center-of-gravity"><i class="fa fa-check"></i><b>5.3</b> Center of Gravity</a></li>
<li class="chapter" data-level="5.4" data-path="appendixa.html"><a href="appendixa.html#inertia-of-a-cloud-of-points"><i class="fa fa-check"></i><b>5.4</b> Inertia of a cloud of points</a></li>
<li class="chapter" data-level="5.5" data-path="appendixa.html"><a href="appendixa.html#projection-of-the-cloud-of-points-on-a-line"><i class="fa fa-check"></i><b>5.5</b> Projection of the cloud of points on a line</a></li>
<li class="chapter" data-level="5.6" data-path="appendixa.html"><a href="appendixa.html#centered-and-standardized-variable"><i class="fa fa-check"></i><b>5.6</b> Centered and Standardized Variable</a></li>
<li class="chapter" data-level="5.7" data-path="appendixa.html"><a href="appendixa.html#correlation-coefficient"><i class="fa fa-check"></i><b>5.7</b> Correlation Coefficient</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>6</b> Appendix B: PCA Formulae</a><ul>
<li class="chapter" data-level="6.1" data-path="appendixb.html"><a href="appendixb.html#general-analysis"><i class="fa fa-check"></i><b>6.1</b> General Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="appendixb.html"><a href="appendixb.html#formulas-for-pca"><i class="fa fa-check"></i><b>6.2</b> Formulas for PCA</a></li>
<li class="chapter" data-level="6.3" data-path="appendixb.html"><a href="appendixb.html#biplot-and-pca"><i class="fa fa-check"></i><b>6.3</b> Biplot and PCA</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendixc.html"><a href="appendixc.html"><i class="fa fa-check"></i><b>7</b> Appendix C: Data Analysis Reminder</a><ul>
<li class="chapter" data-level="7.1" data-path="appendixc.html"><a href="appendixc.html#normalized-principal-component-analysis"><i class="fa fa-check"></i><b>7.1</b> Normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="appendixc.html"><a href="appendixc.html#non-normalized-principal-component-analysis"><i class="fa fa-check"></i><b>7.2</b> Non-normalized Principal Component Analysis</a></li>
<li class="chapter" data-level="7.3" data-path="appendixc.html"><a href="appendixc.html#simple-correpondence-analysis"><i class="fa fa-check"></i><b>7.3</b> Simple Correpondence Analysis</a></li>
<li class="chapter" data-level="7.4" data-path="appendixc.html"><a href="appendixc.html#multiple-correspondence-analysis"><i class="fa fa-check"></i><b>7.4</b> Multiple Correspondence Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="appendixc.html"><a href="appendixc.html#clustering-of-factors"><i class="fa fa-check"></i><b>7.5</b> Clustering of Factors</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Principal Component Analysis for Data Science (pca4ds)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic" class="section level1">
<h1><span class="header-section-number">1</span> Basic Elements</h1>
<p>Principal Component Analysis (PCA) is a statistical technique with a strong
descriptive flavor that can be used to get an approximate visualization
(optimal in a certain sense) of the information contained in a data table.
Simply put, PCA allows us to simultaneously describe the association between
variables, as well as the ressemblance among individuals. PCA can also be
regarded to as a dimension reduction technique of quantitative variables, often
employed as an intermediate step towards a subsequent model building phase.</p>
<p>In this chapter, we describe PCA as an exploratory tool that will allows us
to visualize and gain insight into the structure of a data set.</p>
<div id="data-and-goals" class="section level2">
<h2><span class="header-section-number">1.1</span> Data and Goals</h2>
<p>Behind a Principal Component Analysis, the analyst has to deal with several
continuous variables measured on a number of individuals. The goal is to learn and gain insight about the available data. For instance, a common application of PCA has to do
with building an economic index that measures the economic capacity of a group
of individuals; PCA can also be used to obtain an optimal subset of points
in order to control the polution in a certain geographic region; or to segment
a population in terms of preference evaluations given to a group of similar
products in a certain market.</p>
<p>Often, PCA can be used as an intermediate step in which its outputs will be
part of a subsequent analysis such as regression, clustering, or
classification. Likewise, it is also possible to employ PCA as a data
compression methodology.</p>
<p>The starting point is a data set in which a number of continuous variables
have been measured on a group of individuals. Sometimes, qualitative variables
may also be present in the data.</p>
<p>The typical convention is to have a data set in a tabular format like in a
spreadsheet (e.g. rows and columns). Virtually in all cases, the dimensions of
the table will make it impossible to observe, by simple inspection, which
individuals are similar, or which variables are measuring similar features
among the individuals.
In other words, the association structure of the variables, as well as the
configuration of the similarities among individuals, remains hidden.</p>
<p>Let’s consider a simple example that will allows us to settle the various
concepts underlying a Principal Component Analysis.
The goal is to compare a given number of cities according to the mean
salary-level of a dozen of occupations. The aim is to contrast the coherence
of the description against our global economic knowledge.</p>
<p>The data set pertains to the year 1994, and it consists of 51 cities around the world, on which 40 economic variables have been measured. The cities are grouped in 10 regions around the world.</p>
<table>
<thead>
<tr class="header">
<th align="left">Num</th>
<th align="left">Variable</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left"><code>city</code></td>
<td align="left">Name of the city</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left"><code>region</code></td>
<td align="left">Region of the world</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left"><code>price_index_no_rent</code></td>
<td align="left">Index of prices without renting cost</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left"><code>price_index_with_rent</code></td>
<td align="left">Index of prices with renting cost</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left"><code>gross_salaries</code></td>
<td align="left">Index of gross salaries</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left"><code>net_salaries</code></td>
<td align="left">Index of net salaries</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left"><code>work_hours_year</code></td>
<td align="left">Yearly worked hours</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left"><code>paid_vacations_year</code></td>
<td align="left">Yearly paid vacations</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left"><code>gross_buying_power</code></td>
<td align="left">Gross buying power</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left"><code>net_buying_power</code></td>
<td align="left">Net buying power</td>
</tr>
<tr class="odd">
<td align="left">11</td>
<td align="left"><code>bread_kg_work_time</code></td>
<td align="left">Worked time to buy 1 kg of bread</td>
</tr>
<tr class="even">
<td align="left">12</td>
<td align="left"><code>burger_work_time</code></td>
<td align="left">Worked time to buy a burger</td>
</tr>
<tr class="odd">
<td align="left">13</td>
<td align="left"><code>food_expenses</code></td>
<td align="left">Food expenses</td>
</tr>
<tr class="even">
<td align="left">14</td>
<td align="left"><code>shopping_basket</code></td>
<td align="left">Cost of shopping basket (groceries)</td>
</tr>
<tr class="odd">
<td align="left">15</td>
<td align="left"><code>women_apparel</code></td>
<td align="left">Cost of women apparel</td>
</tr>
<tr class="even">
<td align="left">16</td>
<td align="left"><code>men_apparel</code></td>
<td align="left">Cost of men apparel</td>
</tr>
<tr class="odd">
<td align="left">17</td>
<td align="left"><code>bed4_apt_furnished</code></td>
<td align="left">Cost of 4-bedroom appt. furnished</td>
</tr>
<tr class="even">
<td align="left">18</td>
<td align="left"><code>bed3_apt_unfurnished</code></td>
<td align="left">Cost of 3-bedroom appt. unfurnished</td>
</tr>
<tr class="odd">
<td align="left">19</td>
<td align="left"><code>rent_cost</code></td>
<td align="left">Cost of house rent</td>
</tr>
<tr class="even">
<td align="left">20</td>
<td align="left"><code>home_appliances</code></td>
<td align="left">Cost of home appliances</td>
</tr>
<tr class="odd">
<td align="left">21</td>
<td align="left"><code>public_transportation</code></td>
<td align="left">Public transportation (bus, train, metro)</td>
</tr>
<tr class="even">
<td align="left">22</td>
<td align="left"><code>taxi</code></td>
<td align="left">Cost of taxi</td>
</tr>
<tr class="odd">
<td align="left">23</td>
<td align="left"><code>car</code></td>
<td align="left">Cost of car</td>
</tr>
<tr class="even">
<td align="left">24</td>
<td align="left"><code>restaurant</code></td>
<td align="left">Cost of restaurant</td>
</tr>
<tr class="odd">
<td align="left">25</td>
<td align="left"><code>hotel_night</code></td>
<td align="left">Cost of one hotel night</td>
</tr>
<tr class="even">
<td align="left">26</td>
<td align="left"><code>various_services</code></td>
<td align="left">Cost of various services</td>
</tr>
<tr class="odd">
<td align="left">27</td>
<td align="left"><code>tax_pct_gross_salary</code></td>
<td align="left">Taxes as percentage of gross salary</td>
</tr>
<tr class="even">
<td align="left">28</td>
<td align="left"><code>net_hourly_salary</code></td>
<td align="left">Net hourly salary</td>
</tr>
<tr class="odd">
<td align="left">29</td>
<td align="left"><code>teacher</code></td>
<td align="left">Salary of School teacher</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="left"><code>bus_driver</code></td>
<td align="left">Salary of Bus driver</td>
</tr>
<tr class="odd">
<td align="left">31</td>
<td align="left"><code>mechanic</code></td>
<td align="left">Salary of Car mechanic</td>
</tr>
<tr class="even">
<td align="left">32</td>
<td align="left"><code>construction_worker</code></td>
<td align="left">Salary of Construction worker</td>
</tr>
<tr class="odd">
<td align="left">33</td>
<td align="left"><code>metalworker</code></td>
<td align="left">Salary of Metalworker</td>
</tr>
<tr class="even">
<td align="left">34</td>
<td align="left"><code>cook_chef</code></td>
<td align="left">Salary of Cook chef</td>
</tr>
<tr class="odd">
<td align="left">35</td>
<td align="left"><code>departmental_head</code></td>
<td align="left">Salary of Departmental head</td>
</tr>
<tr class="even">
<td align="left">36</td>
<td align="left"><code>engineer</code></td>
<td align="left">Salary of Engineer</td>
</tr>
<tr class="odd">
<td align="left">37</td>
<td align="left"><code>bank_clerk</code></td>
<td align="left">Salary of Bank clerk (cashier)</td>
</tr>
<tr class="even">
<td align="left">38</td>
<td align="left"><code>executive_secretary</code></td>
<td align="left">Salary of Executive secretary</td>
</tr>
<tr class="odd">
<td align="left">39</td>
<td align="left"><code>salesperson</code></td>
<td align="left">Salary of Salesperson (sales associate)</td>
</tr>
<tr class="even">
<td align="left">40</td>
<td align="left"><code>textile_worker</code></td>
<td align="left">Salary of Textile worker</td>
</tr>
</tbody>
</table>
<p>In the data table, the rows correspond to the <em>individuals</em>, which in this case
have to do with the cities. In turn, the columns correspond to the <em>variables</em>
which have to do with the characteristics measured on the cities.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-1"></span>
<img src="images/figure-1-1.png" alt="Standard format of a data matrix" width="50%" />
<p class="caption">
Figure 1.1: Standard format of a data matrix
</p>
</div>
<p>Before performing the actual PCA, we should always carry out an exploratory
analysis. This analysis refers to computing summary statistics such as maximum
values, miminum values, range, measures of center, measures of spread,
looking at the distribution of the variables (e.g. boxplots, histograms), etc.
This preliminary analysis could help us identify outliers, errors, or other
major anomalie in the data that can disturb that analysis and make the results
worthless.</p>
<div id="active-variables" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Active Variables</h3>
<p>The data set of cities and economic variables is relatively small. However,
the information contained in this data is very rich. There is a wide number
of variables, which is typical of this type of applications. The variables
can be grouped by topics. For instance, there is a series of variables that
correspond to expenses (in clothes, home rent, vehicles, utilities, etc.).
that reflect the cost of living in each city. Other variables involve
information about the salary, broken down into 12 professions. Likewise, other
variables convey information about the quality of life, such as taxes,
payed vacations, work days, and so on.</p>
<p>To compare the cities, we can certainly take all the (continuous) variables
and perform a Principal Component Analysis. Notice that this task will lead us
to compare the cities in terms of prices, salaries, taxes, work-hours necessary
to buy a hamburger, etc. The observed differences among cities are difficult
to interpret; they can have multiple causes, and have values of very different nature.</p>
<p>Instead of selecting all the available variables, it is preferable to select a group of variables, more homogeneous according
to a certain topic, and more aligned with the goals of the analysis. In this sense,
what we call a <em>topic</em> is a group of variables which defines a certain
standpoint, chosen by the analyst, to compare the cities. In this way, the
interpretation of the proximities among cities will be easier.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-2"></span>
<img src="images/figure-1-2.png" alt="Selection of active variables and supplementary variables" width="50%" />
<p class="caption">
Figure 1.2: Selection of active variables and supplementary variables
</p>
</div>
<p>The chosen variables, called <em>active variables</em>, comprise the unique elements
that will be used to compare the cities among them. The rest of the variables
that are not active are called <em>supplementary variables</em>. This does not mean
that the information of the supplementary variables will not be used. We will
use the supplementary variables as additional information that may help us to
explain the observed (dis)similarities among the cities.</p>
<p>In our example, we will take as active variables the net income, measured in
dollars, for the 12 selected professions. Two cities will be close to each
other if the incomes of these 12 professions are very similar, independently
of any other variables that may make them different (e.g. size, prices,
altitude, etc.). In the following list we provide the 12 available professions:</p>
<ul>
<li><code>Teacher</code></li>
<li><code>Bus driver</code></li>
<li><code>Car Mechanic</code></li>
<li><code>Construction worker</code></li>
<li><code>Metalworker</code></li>
<li><code>Cook chef</code></li>
<li><code>Factory manager</code></li>
<li><code>Engineer</code></li>
<li><code>Bank clerk</code></li>
<li><code>Executive secretary</code></li>
<li><code>Salesperson</code></li>
<li><code>Textile worker</code></li>
</ul>
<p>The rest of the variables will be considered supplementary and they will be
employed during the interpretation of the results.</p>
</div>
</div>
<div id="analysis-of-distances" class="section level2">
<h2><span class="header-section-number">1.2</span> Analysis of Distances</h2>
<p>The results obtained in a PCA will allows us to get a visualization of the
differences among the 51 cities, according to the net salaries of the
chosen professions, as well as a visualization of the global association
among such professions.</p>
<p>To get such visual displays, we utilize a geometric approximation that is
fairly simple and intuitive. As a matter of fact, this type of approximation is
the foundation of all component-based exploratory methods, which consists of
regarding a data matrix from the dual point of view of rows and columns.
Each perspective involves considering a <em>cloud of points</em>, one for the rows,
and one for the variables.</p>
<div id="cloud-of-row-points" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Cloud of Row-Points</h3>
<p>We can regard each row of the data table (each city) as one point with 12
coordinates. Each coordinate corresponding to each of the 12 professions.</p>
<p>If we had only recorded three professions, the values taken by each city would
be located in a three dimensional space, defining a cloud of 51 city-points.
We can generalize this idea to any number of professions. If we consider our
12 selected professions, then the cloud of points will be located in a
12-dimensional space.</p>
<p>In general, the rows of a data matrix will form a cloud of <span class="math inline">\(n\)</span> points in a
<span class="math inline">\(p\)</span>-dimensional space (as many dimensions as active variables). We call
this cloud the <em>cloud of row-points</em> or simply the <em>cloud of individuals</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-3"></span>
<img src="images/figure-1-3.png" alt="Cloud of n Row-points" width="484" />
<p class="caption">
Figure 1.3: Cloud of n Row-points
</p>
</div>
<p>In this cloud, two points that are close to each other will indicate two cities
with similar values in each of the 12 professions. In contrast, two points that
are further apart will indicate two cities with distinct salary levels.</p>
<p>To measure the notion of <em>proximity</em> between row-points (the cities) we need
to define a measure of distance. The most intuitive distance measure is the
<strong>euclidean</strong> distance between two points given by:</p>
<p><span class="math display" id="eq:1">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} (x_{ij} - x_{i&#39;j})^2
\tag{1.1}
\]</span></p>
<p>The main problem with visualizing the distance among the points, resides in
the high dimensionality of the cloud of points (12 dimensions in our example)
which makes it impossible to our human vision to visualize.</p>
</div>
<div id="cloud-of-column-points" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Cloud of Column-Points</h3>
<p>In a similar way to the cloud of row-points, we can geometrically represent
the <span class="math inline">\(p\)</span> columns of the data matrix in an <span class="math inline">\(n\)</span>-dimensional space (one dimension
for each individual). The <span class="math inline">\(n\)</span> coordinates of a column-point are given by the
<span class="math inline">\(n\)</span> values of the corresponding variable in the data table.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-4"></span>
<img src="images/figure-1-4.png" alt="Cloud of p Column-points" width="484" />
<p class="caption">
Figure 1.4: Cloud of p Column-points
</p>
</div>
<p>The interesting part of this cloud of variable-points lies in the fact that
it is a representation of the associations between the variables. Each of them
measures an observed characteristic on the cities. Consequently, we can see
which variables measure similar things among the cities. Analogous to the
distance between cities, we need to define a distance between the variable-points
that captures the intensity and the nature of the association between the
variables.</p>
<p>Two variable-points that are close to each other, will indicate two variables
that take related values in the entire set of cities: if we know the values
of one variable, we can know the values of the other one.</p>
<p>A very common measure to quantify the association between variables is the
<strong>linear correlation coefficient</strong>. If we used this coefficient as a distance,
then the visualization of the variables becomes a visual display of the
matrix of correlations among variables.</p>
</div>
</div>
<div id="how-to-see-the-distances-between-points" class="section level2">
<h2><span class="header-section-number">1.3</span> How to see the distances between points</h2>
<p>Because both types of clouds—row-points and column-points—are located in
high dimensional spaces, we cannot observe them directly. The essence of
Principal Component Analysis involves searching for a plane on which we project
the cloud of points in such a way that the obtained configuration is as close
as possible to the original configuration of the cloud in the high-dimensional
space. We call this plane the <em>factorial plane</em>.</p>
<p>The way in which we obtain the desired plane, is by making the overall distances
between projected points as close as possible to the real distances between
points in the space of origin.</p>
<p>Let’s consider in first place the cloud of <span class="math inline">\(n\)</span> individual-points located in
the space where each axis corresponds to a variable. The following figure
depicts this idea when we have only three variables.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-5"></span>
<img src="images/figure-1-5.png" alt="Cloud of row-points in first factorial plane" width="90%" />
<p class="caption">
Figure 1.5: Cloud of row-points in first factorial plane
</p>
</div>
<p>The problem consists of finding the factorial plane such that the set of
of all pairs of distances <span class="math inline">\(d_F(i,i&#39;)\)</span> between points, is as close as possible
to the real distances <span class="math inline">\(d_X(i,i&#39;)\)</span> measured in the space of origin.</p>
<div id="how-to-find-the-projection-planes" class="section level3">
<h3><span class="header-section-number">1.3.1</span> How to find the projection planes</h3>
<p>Our goal has to do with finding a subspace of reduced dimension that conserves
tha maximum of information from the original configuration of the cloud. For
instance, let’s pretend that the original cloud has the shape of a mug, like
in the following figure:</p>
<p><img src="images/mug-shaped-cloud.png" width="30%" style="display: block; margin: auto;" /></p>
<p>Furthermore, let’s assume that we can only observe the projection of the mug
on a plane of reduced dimension. The question is: Which plane should we choose?</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-6"></span>
<img src="images/figure-1-6.png" alt="Three projections of a mug-shaped cloud points" width="685" />
<p class="caption">
Figure 1.6: Three projections of a mug-shaped cloud points
</p>
</div>
<p>We can consider of projecting this mug cloud over different planes.
As you can tell, the projection on the plane <span class="math inline">\(H_A\)</span> is much more informative
that the projection on the plane <span class="math inline">\(H_B\)</span>. At least we can see that the figure
on <span class="math inline">\(H_A\)</span> has to do with a lengthened object, and that one of its ends is wider
than the other end. In contrast, all the points of the projected cloud on
the plane <span class="math inline">\(H_B\)</span> are confounded, and it does not convey a clear idea the
original cloud, except for the shadow of the handle. However, the best
projection among the three planes is that of <span class="math inline">\(H_C\)</span>.</p>
<p>We obtain the plane <span class="math inline">\(H_C\)</span> by searching for the plane that makes the dispersion
of the projected points as large as possible:</p>
<p><span class="math display" id="eq:2">\[
Max_H \sum_i \sum_{i&#39;} d^{2}_{H} (i, i&#39;)
\tag{1.2}
\]</span></p>
<p>where <span class="math inline">\(H\)</span> represents the subspace of the projection.</p>
<p>Searching for the maximum can be written as:</p>
<p><span class="math display" id="eq:3">\[
Max_H \sum_i \sum_{i&#39;} d^{2}_{H} (i, i&#39;) = Max_H \left \{ 2n \sum_i d^{2}_{H} (i, G) \right \}
\tag{1.3}
\]</span></p>
<p>The problem of preserving the projected distances between all pairs of points
becomes a problem of preserving the distances between each point and the center of
gravity <span class="math inline">\(G\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-7"></span>
<img src="images/figure-1-7.png" alt="Decomposition of the distance between a row-point and the center of gravity" width="60%" />
<p class="caption">
Figure 1.7: Decomposition of the distance between a row-point and the center of gravity
</p>
</div>
<p>The formula in equation <a href="basic.html#eq:4">(1.4)</a> is actually an expression for the Pythagorean theorem. The spread of the cloud of points can be decomposed into two terms: the spread in the projection plane, and another (orthogonal) term given by the sum of the distances of the points to the projection plane:</p>
<p><span class="math display" id="eq:4">\[
\sum_{i} d^{2} (i, G) = \sum_i d^{2}_{H} (i, G) + \sum_i d^{2}_{\bar{H}} (i, G)
\tag{1.4}
\]</span></p>
<p>In this way, the projection plane that guarantees the maximum dispersion between the points, is also the plane that gets as close as possible to the original cloud (in the sense of least squares criterion). This is expressed in the following relation <a href="basic.html#eq:5">(1.5)</a></p>
<p><span class="math display" id="eq:5">\[
Max \hspace{2mm} \sum_{i} d^{2}_{H} (i, G) \quad \Longleftrightarrow \quad Min \hspace{2mm} d^{2}_{\bar{H}} (i, G)
\tag{1.5}
\]</span></p>
</div>
<div id="how-to-take-into-account-the-importance-of-individuals" class="section level3">
<h3><span class="header-section-number">1.3.2</span> How to take into account the importance of individuals</h3>
<p>Sometimes we may be interested in assigning weights to the individuals based on their relative importance or relevance. When all the individuals have the same importance, we can give a weight equal to <span class="math inline">\(1/n\)</span> to each of them. Thus, the fit criterion becomes:</p>
<p><span class="math display" id="eq:6">\[
Max \hspace{2mm} \sum_{i} \frac{1}{n} d^{2}_{H} (i, G) = Max \hspace{2mm} \frac{\sum_i (x_{iH} - \bar{x}_H)^2}{n}
\tag{1.6}
\]</span></p>
<p>In the general case where each individual has its own weight <span class="math inline">\(p_i\)</span> with <span class="math inline">\(\sum_i p_i = 1\)</span>, then the fit criterion is expressed as:</p>
<p><span class="math display" id="eq:7">\[
Max \hspace{2mm} \sum_{i} p_i \hspace{1mm} d^{2}_{H} (i, G)
\tag{1.7}
\]</span></p>
<p>The product of the weight of a point, <span class="math inline">\(p_i\)</span>, times the squared of its distance to the center of gravity, <span class="math inline">\(d^{2}_{H} (i, G)\)</span>, is known as <em>inertia of the point</em>. In this case, the problem involves looking for the projection plane that maximizes the projected inertia.</p>
</div>
<div id="inertia-decomposition" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Inertia Decomposition</h3>
<p>The total inertia is defined as:</p>
<p><span class="math display" id="eq:8">\[
I = \sum_{i=1}^{n} p_i \hspace{1mm} d^2(i,G)
\tag{1.8}
\]</span></p>
<p>The total inertia can be broken down into two additive terms:</p>
<ul>
<li>projected inertia on a subspace <span class="math inline">\(H\)</span></li>
<li>inertia orthogonally projected on a subspace <span class="math inline">\(\bar{H}\)</span></li>
</ul>
<p><span class="math display" id="eq:9">\[
I = I_H + I_{\bar{H}}
\tag{1.9}
\]</span></p>
<p>The problem of searching for the subspace that makes the dispersion of the projected points as large as possible can also be put in terms of inertias. Namely, we look for a plane <span class="math inline">\(H\)</span> that maximizes the projected inertia (see figure below).</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-8"></span>
<img src="images/figure-1-8.png" alt="Successive directions of maximum inertia" width="60%" />
<p class="caption">
Figure 1.8: Successive directions of maximum inertia
</p>
</div>
<p>In order to find the optimal subspace, we begin by looking for a one-dimensional space (i.e. a line) of maximum projected inertia. If all individuals have the same weight, then this first subspace coincides with the direction of maximum stretch of the cloud.</p>
<p>Having found the first one-dimensional subspace, the next step involves finding a two-dimensional subspace (i.e. a plane) with maximum projected inertia. Then, we look for a three-dimensional space, and so on and so forth. At each step, we look for a higher dimensional space such that the projected inertia is as large as possible.</p>
<p>It can be proved that the two-dimensional plane must contain the one-dimensional space (i.e. the line) of maximum projected inertia. Having found the first direction of maximum inertia, one needs to find another line, orthogonal to the first one, such that the plane formed by them has maximum inertia. Analogously, the subspace of three dimensions is formed with the two-dimensional space by adding an orthogonal direction to this plane (see figure <a href="basic.html#fig:fig-1-8">1.8</a>).</p>
<p>Because the inertia is additive in orthogonal directions, PCA involves the decomposition of the total inertia in <span class="math inline">\(p\)</span> additive components, as many as the number of dimensions in the original space.</p>
<p><span class="math display" id="eq:10">\[
I = I_1 + I_2 + \dots + I_p
\tag{1.10}
\]</span></p>
<p>Interestingly, the inertias (the dispersions) are decreasingly ordered, which means that subsequent orthogonal directions become smaller, that is, <span class="math inline">\(I_1 \geq I_2 \geq \dots \geq I_p\)</span>.</p>
<p>This implies that the configuration of the projected cloud on the first fatorial plane is as close as possible (in the least squares sense) to the original configuration.</p>
<p>For example, the figure below shows the plane of maximum projected inertia for in the data of the Cities.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-9"></span>
<img src="images/figure-1-9.png" alt="Cloud of variable-points in the standardized PCA" width="80%" />
<p class="caption">
Figure 1.9: Cloud of variable-points in the standardized PCA
</p>
</div>
<p>This is the representation of the cities, in a factorial plane, that best resembles the existing distances between the cities according to the salary level. For instance, we can tell that Tokyo, Zurich and Geneva are close to each other, indicating similar salaries among the analyzed professions. The same can be said about Manila, Mumbai, Lagos, and Prague; these cities have similar salaries among them, but they have a contrasting difference with the former group (Tokyo, Zurich, and Geneva).</p>
<p>We can also assume a certain similarity between northen cities such as Stockholm, and Copenhagen, whereas there seems to be a substantial salary difference between Paris and American cities (Los Angeles, Chicago, Toronto, Houston).</p>
<p>The center of the graph represents the average point of the cloud. This corresponds to cities with salaries closer to the mean values.</p>
<p>Notice the difference in scale utilized in the x-axis and the y-axis. Actually, the cloud of points is very extended along the x-axis <span class="math inline">\(F_1\)</span>.</p>
</div>
<div id="visualizing-association-between-variables." class="section level3">
<h3><span class="header-section-number">1.3.4</span> Visualizing association between variables.</h3>
<p>Let’s discuss what the analysis involves regarding the cloud of variable-points. Recall that the cloud of variable-points is defined by the columns of the data matrix <span class="math inline">\(\mathbf{X}\)</span>. Without loss of generality, we will assume that the variables are mean-centered and normalized by the standard deviation.</p>
<p>The first thing to do is to define a measure of distance between variables. One way to represent a certain notion of proximity among variables is given by the following formula:</p>
<p><span class="math display" id="eq:11">\[
d^2(j, j&#39;) = 2 (1 - cor(j, j&#39;))
\tag{1.11}
\]</span></p>
<p>If two variables <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span> measure the same thing (in the sense of having a linear correlation of 1) then their points will be overlapped.</p>
<p>In the case where two variables <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span> have a linear correlation equal to -1 (when one increases, the other decreases), the two variable-points will have a maximum distance in opposite directions.</p>
<p>When one variable does not provide any information about the other one, we have a situation when their correlation coefficient will be zero. This would correspond to an intermediate distance in which the variables form an orthogonal angle. These cases are illustrated in figure <a href="basic.html#fig:fig-1-10">1.10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-10"></span>
<img src="images/figure-1-10.png" alt="Cloud of variable-points in the standardized PCA" width="70%" />
<p class="caption">
Figure 1.10: Cloud of variable-points in the standardized PCA
</p>
</div>
<p>If the correlation is equal to 1, the squared distance is zero. Likewise, if the correlation is -1 the distance is 4, and if the correlation is zero then the distance is 2. This formulae define the so-called standardized Principal Components Analysis.</p>
<p>It can be shown that all the variable-points are located within a one-unit distance from the origin (inside a hypersphere of radius 1). The cloud of variable-points is defined by a set of vectors that start from the center of the sphere. The correlation coefficient between two variables coindices with the cosine of the angle formed by two corresponding vectors (associated to the variables).</p>
<p>Two variables that are close to each other will form a small angle, which corresponds to a large correlation coefficient. Two independent variables will have a zero correlation coefficient, thus forming a square angle between them. In turn, two opposed variables will have a correlation coefficient close to -1 and thus will appear on two opposite locations in the sphere.</p>
<p>Analogously to the cloud of row-points, we also seek to find a projection plane providing the largest amount of information about the associations between the variables. More precisely, we look for a plane that provides information about the angles between variables, that is, about their correlations.</p>
<p>In the case of the 12 variables observed on the 51 cities, we obtain the configuration on the first factorial plane, displayed in figure <a href="basic.html#fig:fig-1-11">1.11</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-11"></span>
<img src="images/figure-1-11.png" alt="Circle of correlations on the first factorial plane" width="70%" />
<p class="caption">
Figure 1.11: Circle of correlations on the first factorial plane
</p>
</div>
<p>Notice that in this case all the variables are positively correlated. Moreover, from the circle of correlations, it is possible to observe two groups of variables. One group is formed by <code>departmental_head</code>, <code>engineers</code>, <code>bank_clerk</code>, and <code>cook_chef</code>. The other group is formed by the rest of the professions.</p>
<p>In the matrix of correlations (see table <a href="basic.html#tab:table-1-3">1.1</a>) we can see the magnitude of the association between the variables. As you can tell, all the correlations are positive and with large value tanging from 0.59 to 0.96.</p>
<table>
<caption><span id="tab:table-1-3">Table 1.1: </span>Matrix of correlations.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">tea</th>
<th align="right">bus</th>
<th align="right">mec</th>
<th align="right">con</th>
<th align="right">met</th>
<th align="right">coo</th>
<th align="right">dep</th>
<th align="right">eng</th>
<th align="right">ban</th>
<th align="right">exe</th>
<th align="right">sal</th>
<th align="right">tex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tea</td>
<td align="right">1.00</td>
<td align="right">0.96</td>
<td align="right">0.84</td>
<td align="right">0.83</td>
<td align="right">0.91</td>
<td align="right">0.75</td>
<td align="right">0.78</td>
<td align="right">0.81</td>
<td align="right">0.82</td>
<td align="right">0.92</td>
<td align="right">0.88</td>
<td align="right">0.88</td>
</tr>
<tr class="even">
<td>bus</td>
<td align="right">0.96</td>
<td align="right">1.00</td>
<td align="right">0.89</td>
<td align="right">0.88</td>
<td align="right">0.94</td>
<td align="right">0.76</td>
<td align="right">0.74</td>
<td align="right">0.82</td>
<td align="right">0.80</td>
<td align="right">0.93</td>
<td align="right">0.89</td>
<td align="right">0.92</td>
</tr>
<tr class="odd">
<td>mec</td>
<td align="right">0.84</td>
<td align="right">0.89</td>
<td align="right">1.00</td>
<td align="right">0.95</td>
<td align="right">0.93</td>
<td align="right">0.80</td>
<td align="right">0.64</td>
<td align="right">0.74</td>
<td align="right">0.70</td>
<td align="right">0.88</td>
<td align="right">0.89</td>
<td align="right">0.89</td>
</tr>
<tr class="even">
<td>con</td>
<td align="right">0.83</td>
<td align="right">0.88</td>
<td align="right">0.95</td>
<td align="right">1.00</td>
<td align="right">0.93</td>
<td align="right">0.72</td>
<td align="right">0.59</td>
<td align="right">0.70</td>
<td align="right">0.64</td>
<td align="right">0.86</td>
<td align="right">0.86</td>
<td align="right">0.92</td>
</tr>
<tr class="odd">
<td>met</td>
<td align="right">0.91</td>
<td align="right">0.94</td>
<td align="right">0.93</td>
<td align="right">0.93</td>
<td align="right">1.00</td>
<td align="right">0.76</td>
<td align="right">0.69</td>
<td align="right">0.80</td>
<td align="right">0.72</td>
<td align="right">0.92</td>
<td align="right">0.88</td>
<td align="right">0.94</td>
</tr>
<tr class="even">
<td>coo</td>
<td align="right">0.75</td>
<td align="right">0.76</td>
<td align="right">0.80</td>
<td align="right">0.72</td>
<td align="right">0.76</td>
<td align="right">1.00</td>
<td align="right">0.82</td>
<td align="right">0.82</td>
<td align="right">0.79</td>
<td align="right">0.80</td>
<td align="right">0.85</td>
<td align="right">0.71</td>
</tr>
<tr class="odd">
<td>dep</td>
<td align="right">0.78</td>
<td align="right">0.74</td>
<td align="right">0.64</td>
<td align="right">0.59</td>
<td align="right">0.69</td>
<td align="right">0.82</td>
<td align="right">1.00</td>
<td align="right">0.87</td>
<td align="right">0.89</td>
<td align="right">0.80</td>
<td align="right">0.79</td>
<td align="right">0.65</td>
</tr>
<tr class="even">
<td>eng</td>
<td align="right">0.81</td>
<td align="right">0.82</td>
<td align="right">0.74</td>
<td align="right">0.70</td>
<td align="right">0.80</td>
<td align="right">0.82</td>
<td align="right">0.87</td>
<td align="right">1.00</td>
<td align="right">0.85</td>
<td align="right">0.87</td>
<td align="right">0.85</td>
<td align="right">0.81</td>
</tr>
<tr class="odd">
<td>ban</td>
<td align="right">0.82</td>
<td align="right">0.80</td>
<td align="right">0.70</td>
<td align="right">0.64</td>
<td align="right">0.72</td>
<td align="right">0.79</td>
<td align="right">0.89</td>
<td align="right">0.85</td>
<td align="right">1.00</td>
<td align="right">0.87</td>
<td align="right">0.85</td>
<td align="right">0.73</td>
</tr>
<tr class="even">
<td>exe</td>
<td align="right">0.92</td>
<td align="right">0.93</td>
<td align="right">0.88</td>
<td align="right">0.86</td>
<td align="right">0.92</td>
<td align="right">0.80</td>
<td align="right">0.80</td>
<td align="right">0.87</td>
<td align="right">0.87</td>
<td align="right">1.00</td>
<td align="right">0.94</td>
<td align="right">0.93</td>
</tr>
<tr class="odd">
<td>sal</td>
<td align="right">0.88</td>
<td align="right">0.89</td>
<td align="right">0.89</td>
<td align="right">0.86</td>
<td align="right">0.88</td>
<td align="right">0.85</td>
<td align="right">0.79</td>
<td align="right">0.85</td>
<td align="right">0.85</td>
<td align="right">0.94</td>
<td align="right">1.00</td>
<td align="right">0.89</td>
</tr>
<tr class="even">
<td>tex</td>
<td align="right">0.88</td>
<td align="right">0.92</td>
<td align="right">0.89</td>
<td align="right">0.92</td>
<td align="right">0.94</td>
<td align="right">0.71</td>
<td align="right">0.65</td>
<td align="right">0.81</td>
<td align="right">0.73</td>
<td align="right">0.93</td>
<td align="right">0.89</td>
<td align="right">1.00</td>
</tr>
</tbody>
</table>
<p>The fact that all the variables are positively correlated implies that if one type of salary in a city is high, then the other salaries in that city will also be high.</p>
<p>In later sections of the book we will emphasize the idea that Principal Component Analysis can be approached from the point of view of the variable-points having a distance based on the correlation between the variables. In other words, a PCA on the row-points is not an independent analysis from a PCA on the variable-points. Quite the opposite in fact, it is possible to obtain the results of a PCA on a set of points (e.g. variables) given the results of the other set (e.g. the rows). This, as we’ll see, provides a set of rules extremely valuable in the interpretation of results.</p>
</div>
<div id="normalized-pca-or-non-normalized-pca" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Normalized PCA or non-normalized PCA?</h3>
<p>As we’ve seen, Principal Component Analysis consists of decomposing the inertia of the original cloud, based on some orthogonal directions. Each of the obtained orthodonal directions accounts for a proportion of the original inertia.</p>
<p>But, what is the contribution of each variable to the original inertia?</p>
<p>The distance between variable-points defined in equation <a href="basic.html#eq:11">(1.11)</a> implies that the contribution, of each variable, to the total inertia is the same for all variables, equal to <span class="math inline">\(1/p\)</span>.</p>
<p>The inertia of the cloud of variable-points with respect to the origin coincides with the number of active variables.</p>
<p><span class="math display" id="eq:12">\[
I_T = \sum_{j=1}^{p} d^2(j, O) = p
\tag{1.12}
\]</span></p>
<p>In order for equation <a href="basic.html#eq:12">(1.12)</a> to make sense, we need to use standardized values (mean-centered and unit-variance):</p>
<p><span class="math display" id="eq:13">\[
z_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j}
\tag{1.13}
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the mean of variable <span class="math inline">\(j\)</span> and <span class="math inline">\(s_j\)</span> is the corresponding standard deviation. In this case we talk about <em>Normalized Principal Component Analysis</em>.</p>
<p>With standardized data, the distance of each variable and the origin is equal to 1:</p>
<p><span class="math display" id="eq:14">\[
d^2(j, O) = \sum_{j=1}^{n} \frac{1}{n} \left ( \frac{x_{ij} - \bar{x}_j}{s_j} \right )^2 = \frac{\frac{1}{n} \sum_j (x_{ij} - \bar{x}_j)^2}{s_{j}^{2}} = 1
\tag{1.14}
\]</span></p>
<p>We should mention that the use of Normalized PCA is not always justified. For example, if a PCA is performed by an analyst working for a bank, if may be more interesting to assign a larger weight to the products that contribute to the volume of the deposits. That is, the importance given to different variables should be done by taking into account the goal of the analysis.</p>
<p>If we use the <em>raw</em> data (i.e. mean-centered only, no scaling by the standard deviation), we could see what the contribution of each variable is to the total inertia. The squared distance of one variable to the origin is given by:</p>
<p><span class="math display" id="eq:15">\[
d^2(j,O) = \sum_{j=1}^{n} \frac{1}{n} (x_{ij} - \bar{x}_j)^2 = var(j)
\tag{1.15}
\]</span></p>
<p>With non-normalized data, the variables are no longer found inside a sphere of radious 1. Instead, the length of the segment (i.e. the vector) of each variable is equal to its standard deviation. We can then think of the cloud of variable-points as a set of vectors, each one of length equal to the standard deviation of the variable, and forming angles defined by the correlation coefficient between the variables.</p>
<p>This type of analysis is called <em>Non-Normalized Principal Component Analysis</em>.</p>
<p>With this type of PCA, the distance between two variables depends on the correlation (i.e. the angle between them), as well as on their variances.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-1-12"></span>
<img src="images/figure-1-12.png" alt="Cloud of variable-points in a non-normalized PCA" width="50%" />
<p class="caption">
Figure 1.12: Cloud of variable-points in a non-normalized PCA
</p>
</div>
<p>The total inertia of the cloud of variable-points is given by the sum of the variances of each variable:</p>
<p><span class="math display" id="eq:16">\[
I_T = \sum_{j=1}^{p} d^2(j,O) = \sum_{j=1}^{p} var(j)
\tag{1.16}
\]</span></p>
<p>The contribution of each variable to the total inertia is given by:</p>
<p><span class="math display" id="eq:17">\[
ctr_j = \frac{var(j)}{\sum_j var(j)}
\tag{1.17}
\]</span></p>
<p>The variance is a function of the unit of measurement in which a variable is measured. This provides a valuable degree of freedom to tune the importance of each variable in the analysis.</p>
<p>In practice, it is preferable to assigne the same importance to all the variables. This is a requirement for when the active variables have different units of measurement (e.g. euros, grams, meters, etc).</p>
<p>In our working example with the data about salaries in various cities, the summary statistics for the active variables are displayed in the following table <a href="basic.html#tab:table-1-4">1.2</a></p>
<table>
<caption><span id="tab:table-1-4">Table 1.2: </span>Summary statistics of active variables.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">weight</th>
<th align="right">mean</th>
<th align="right">stdev</th>
<th align="right">min</th>
<th align="right">max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>teacher</td>
<td align="right">51</td>
<td align="right">16801.96</td>
<td align="right">13375.19</td>
<td align="right">600</td>
<td align="right">56800</td>
</tr>
<tr class="even">
<td>bus_driver</td>
<td align="right">51</td>
<td align="right">14311.76</td>
<td align="right">10927.24</td>
<td align="right">400</td>
<td align="right">46100</td>
</tr>
<tr class="odd">
<td>mechanic</td>
<td align="right">51</td>
<td align="right">12384.31</td>
<td align="right">8605.61</td>
<td align="right">700</td>
<td align="right">30500</td>
</tr>
<tr class="even">
<td>construction_worker</td>
<td align="right">51</td>
<td align="right">10343.14</td>
<td align="right">8321.81</td>
<td align="right">200</td>
<td align="right">28000</td>
</tr>
<tr class="odd">
<td>metalworker</td>
<td align="right">51</td>
<td align="right">15145.10</td>
<td align="right">10346.23</td>
<td align="right">800</td>
<td align="right">38700</td>
</tr>
<tr class="even">
<td>cook_chef</td>
<td align="right">51</td>
<td align="right">15615.69</td>
<td align="right">8855.67</td>
<td align="right">500</td>
<td align="right">33900</td>
</tr>
<tr class="odd">
<td>factory_manager</td>
<td align="right">51</td>
<td align="right">30933.33</td>
<td align="right">21462.03</td>
<td align="right">1500</td>
<td align="right">95000</td>
</tr>
<tr class="even">
<td>engineer</td>
<td align="right">51</td>
<td align="right">24664.71</td>
<td align="right">14158.57</td>
<td align="right">1600</td>
<td align="right">59700</td>
</tr>
<tr class="odd">
<td>bank_clerk</td>
<td align="right">51</td>
<td align="right">18749.02</td>
<td align="right">13547.30</td>
<td align="right">1200</td>
<td align="right">58800</td>
</tr>
<tr class="even">
<td>executive_secretary</td>
<td align="right">51</td>
<td align="right">13311.76</td>
<td align="right">7645.12</td>
<td align="right">1400</td>
<td align="right">31500</td>
</tr>
<tr class="odd">
<td>salesperson</td>
<td align="right">51</td>
<td align="right">9658.82</td>
<td align="right">6124.87</td>
<td align="right">400</td>
<td align="right">24700</td>
</tr>
<tr class="even">
<td>textile_worker</td>
<td align="right">51</td>
<td align="right">9247.06</td>
<td align="right">6493.76</td>
<td align="right">300</td>
<td align="right">23800</td>
</tr>
</tbody>
</table>
<p>From the table above, we see that professions <code>factory manager</code> and <code>engineer</code> are the ones that have, on average, the higher salaries. Then we have <code>bank clerk</code>, <code>teacher</code>, <code>cook chef</code>, and <code>metalworker</code>. And in the last two places there is <code>salesperson</code> and <code>textile worker</code>, which are considered to be low-skills professions mostly performed by women.</p>
<p>Because all active variables are measured in the same unit (in dollars), we could carry out a non-normalized PCA. However, if we did this type of analysis, it would imply giving more importance to those professions with higher salary. Why? Because these are the variables that have a larger spread. If our goal is to compare the cities by giving the same importance to all professions, then it is more suitable to apply a normalized PCA. This, in turn, let us focus on the matrix of correlations among the active variables.</p>
<p>Interestingly, both cloud of points—individuals and variables—have the same inertia. On one hand, the inertia of the cloud of row-points is the sum of the squared distances between each point and the center of gravity, weighed by the importance of each individual. This inertia can be expressed with respect to each axis in the original space (in which each axis corresponds to a variable):</p>
<p><span class="math display" id="eq:18">\[
I_T = \sum_{i=1}^{n} \frac{1}{n} \sum_{j=1}^{p} (x_{ij} - \bar{x}_j)^2 =
\sum_{j=1}^{p} \sum_{i=1}^{n} \frac{1}{n} (x_{ij} - \bar{x}_j)^2 = \sum_{j=1}^{p} var(j)
\tag{1.18}
\]</span></p>
<p>The variance along a given axis is the spread of the projected coud on that axis. Because the axes are orthogonal, the total inertia is equal to the sum of the variances of the variables. Therefore, the inertias of both clouds are the same.</p>
</div>
<div id="distance-matrices" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Distance Matrices</h3>
<p>The rows (i.e. the cities) are located in a space where we measure distance in the classic sense. In a normalized PCA we have:</p>
<p><span class="math display" id="eq:19">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} \left ( \frac{x_{ij} - x_{i&#39;j}}{s_j} \right )^2
\tag{1.19}
\]</span></p>
<p>And in the case of a non-normalized PCA:</p>
<p><span class="math display" id="eq:20">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} (x_{ij} - x_{i&#39;j})^2
\tag{1.20}
\]</span></p>
<p>In the cloud of variable-points, the distance is defined by the formula <a href="basic.html#eq:11">(1.11)</a> when we are carrying out a normalized PCA. In the case of a non-normalized PCA then distance becomes:</p>
<p><span class="math display" id="eq:21">\[
d^2(j, j&#39;) = var(j) + var(j&#39;) - 2 cov(j,j&#39;)
\tag{1.21}
\]</span></p>
<p>All these distances can be organized in square matrices: an <span class="math inline">\(n \times n\)</span> distance matrix for the distances between individuals, and a <span class="math inline">\(p \times p\)</span> matrix for the distances between variables.</p>
<p>If we don’t defined how to measure the distances between points, then the clouds of points remain undefined. In our approximation, we assume that the rows (the cities) are located in a metric (Euclidean) space, which means that the distance is measured by the classic formula of the sum of squared differences:</p>
<p><span class="math display" id="eq:22">\[
d^2 (i, i&#39;) = \sum_{j=1}^{p} (x_{ij} - x_{i&#39;j})^2 = (x_i - x_{i&#39;}) \mathbf{I} (x_i - x_{i&#39;})
\tag{1.22}
\]</span></p>
<p>Notice that in this case, calculating the distance between two individuals takes the form of a scalar product, where the metric matrix is the identity matrix. This corresponds to a non-normalized PCA.</p>
<p>In contrast, when we carry out a normalized PCA, the distance between two individuals is measured by the formula:</p>
<p><span class="math display" id="eq:23">\[
d^2(i, i&#39;) = \sum_{j=1}^{p} \left ( \frac{x_{ij} - x_{i&#39;j}}{s_j} \right )^2 = 
(x_i - x_{i&#39;}) \mathbf{S}^{-2} (x_i - x_{i&#39;})
\tag{1.23}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{S}^{-2} = \left(\begin{array}{cccc}
\dots &amp; 0 &amp; 0 &amp; 0  \\
0 &amp; 1/s_{j}^{2} &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \dots  \\
\end{array}\right)
\]</span></p>
<p>In this case, the metric matrix is given by <span class="math inline">\(\mathbf{S}^{-2}\)</span> (diagonal matrix of inverses of variable variances). Notice that all euclidean metrics can be expressed in a canonical form by using a change of coordinates. When using <span class="math inline">\(\mathbf{S}^{-2}\)</span> such change of coordinates involves dividing the coordinates of the points by the standard deviation of each variable.</p>
<p>Regarding the cloud of variable-points, the distance between variables is defined based on the correlation between two variables. In general, this distance is given by the scalar product between two vectors:</p>
<p><span class="math display" id="eq:24">\[
cor(j,j&#39;) = \sum_{i=1}^{n} \frac{1}{n} \left ( \frac{x_{ij} - \bar{x}_j}{s_j} \right ) \left ( \frac{x_{ij&#39;} - \bar{x}_{j&#39;}}{s_{j&#39;}} \right ) = \mathbf{z_{j}^{\mathsf{T}} N z_j}
\tag{1.24}
\]</span></p>
<p>The natural metric matrix to be used in this case is the diagonal matrix <span class="math inline">\(\mathbf{N}\)</span> of weights for individuals (<span class="math inline">\(1/n\)</span> or <span class="math inline">\(p_i\)</span>). Thus, the distance between variables is defined as:</p>
<p><span class="math display" id="eq:25">\[
d^2(j,j&#39;) = &lt;z_j, z_{j&#39;}&gt; + &lt;z_j, z_{j&#39;}&gt; - 2&lt;z_j, z_{j&#39;}&gt;
\tag{1.25}
\]</span></p>
<p>which coincides with <a href="basic.html#eq:17">(1.17)</a> when using mean-centered data and standardized (normalized PCA), or with <a href="basic.html#eq:16">(1.16)</a> when using non-centenred data (non-normalized PCA).</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="terminology.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mechanics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pca4ds/pca4ds.github.io/edit/master/01-introduction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
